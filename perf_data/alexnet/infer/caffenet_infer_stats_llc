#           time             counts unit events
     0.100348038             24,601      LLC-load-misses          
     0.100348038            667,463      LLC-loads                
     0.200521662             44,459      LLC-load-misses          
     0.200521662          1,168,329      LLC-loads                
     0.300654908             21,365      LLC-load-misses          
     0.300654908          1,262,960      LLC-loads                
     0.400792310             87,846      LLC-load-misses          
     0.400792310          1,815,891      LLC-loads                
     0.500928042             10,639      LLC-load-misses          
     0.500928042          1,021,560      LLC-loads                
     0.601084634              3,937      LLC-load-misses          
     0.601084634            916,540      LLC-loads                
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0509 16:01:32.633147 118538 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0509 16:01:32.633169 118538 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0509 16:01:32.633172 118538 _caffe.cpp:142] Net('models/bvlc_reference_caffenet/deploy.prototxt', 1, weights='models/bvlc_reference_caffenet/caffenet_train_iter_27.caffemodel')
I0509 16:01:32.634542 118538 net.cpp:51] Initializing net from parameters: 
name: "CaffeNet"
state {
  phase: TEST
  level: 0
}
layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 10
      dim: 3
      dim: 227
      dim: 227
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  convolution_param {
    num_output: 96
    kernel_size: 11
    stride: 4
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0001
    beta: 0.75
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  inner_product_param {
    num_output: 4096
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  inner_product_param {
    num_output: 1000
  }
}
layer {
  name: "prob"
  type: "Softmax"
  bottom: "fc8"
  top: "prob"
}
I0509 16:01:32.634608 118538 layer_factory.hpp:77] Creating layer data
I0509 16:01:32.634618 118538 net.cpp:84] Creating Layer data
I0509 16:01:32.634624 118538 net.cpp:380] data -> data
I0509 16:01:32.634636 118538 net.cpp:122] Setting up data
I0509 16:01:32.634644 118538 net.cpp:129] Top shape: 10 3 227 227 (1545870)
I0509 16:01:32.634646 118538 net.cpp:137] Memory required for data: 6183480
I0509 16:01:32.634649 118538 layer_factory.hpp:77] Creating layer conv1
I0509 16:01:32.634656 118538 net.cpp:84] Creating Layer conv1
I0509 16:01:32.634660 118538 net.cpp:406] conv1 <- data
I0509 16:01:32.634665 118538 net.cpp:380] conv1 -> conv1
I0509 16:01:32.634672 118538 base_conv_layer.cpp:121] Group is 1channel is3number of output is 96
I0509 16:01:32.634747 118538 net.cpp:122] Setting up conv1
I0509 16:01:32.634752 118538 net.cpp:129] Top shape: 10 96 55 55 (2904000)
I0509 16:01:32.634754 118538 net.cpp:137] Memory required for data: 17799480
I0509 16:01:32.634763 118538 layer_factory.hpp:77] Creating layer relu1
I0509 16:01:32.634769 118538 net.cpp:84] Creating Layer relu1
I0509 16:01:32.634771 118538 net.cpp:406] relu1 <- conv1
I0509 16:01:32.634776 118538 net.cpp:367] relu1 -> conv1 (in-place)
I0509 16:01:32.634780 118538 net.cpp:122] Setting up relu1
I0509 16:01:32.634784 118538 net.cpp:129] Top shape: 10 96 55 55 (2904000)
I0509 16:01:32.634786 118538 net.cpp:137] Memory required for data: 29415480
I0509 16:01:32.634789 118538 layer_factory.hpp:77] Creating layer pool1
I0509 16:01:32.634793 118538 net.cpp:84] Creating Layer pool1
I0509 16:01:32.634795 118538 net.cpp:406] pool1 <- conv1
I0509 16:01:32.634799 118538 net.cpp:380] pool1 -> pool1
I0509 16:01:32.634807 118538 net.cpp:122] Setting up pool1
I0509 16:01:32.634811 118538 net.cpp:129] Top shape: 10 96 27 27 (699840)
I0509 16:01:32.634814 118538 net.cpp:137] Memory required for data: 32214840
I0509 16:01:32.634815 118538 layer_factory.hpp:77] Creating layer norm1
I0509 16:01:32.634820 118538 net.cpp:84] Creating Layer norm1
I0509 16:01:32.634824 118538 net.cpp:406] norm1 <- pool1
I0509 16:01:32.634826 118538 net.cpp:380] norm1 -> norm1
I0509 16:01:32.634832 118538 net.cpp:122] Setting up norm1
I0509 16:01:32.634836 118538 net.cpp:129] Top shape: 10 96 27 27 (699840)
I0509 16:01:32.634840 118538 net.cpp:137] Memory required for data: 35014200
I0509 16:01:32.634841 118538 layer_factory.hpp:77] Creating layer conv2
I0509 16:01:32.634845 118538 net.cpp:84] Creating Layer conv2
I0509 16:01:32.634848 118538 net.cpp:406] conv2 <- norm1
I0509 16:01:32.634852 118538 net.cpp:380] conv2 -> conv2
I0509 16:01:32.634860 118538 base_conv_layer.cpp:121] Group is 2channel is96number of output is 256
I0509 16:01:32.635315 118538 net.cpp:122] Setting up conv2
I0509 16:01:32.635321 118538 net.cpp:129] Top shape: 10 256 27 27 (1866240)
I0509 16:01:32.635324 118538 net.cpp:137] Memory required for data: 42479160
I0509 16:01:32.635329 118538 layer_factory.hpp:77] Creating layer relu2
I0509 16:01:32.635334 118538 net.cpp:84] Creating Layer relu2
I0509 16:01:32.635336 118538 net.cpp:406] relu2 <- conv2
I0509 16:01:32.635340 118538 net.cpp:367] relu2 -> conv2 (in-place)
I0509 16:01:32.635344 118538 net.cpp:122] Setting up relu2
I0509 16:01:32.635349 118538 net.cpp:129] Top shape: 10 256 27 27 (1866240)
I0509 16:01:32.635350 118538 net.cpp:137] Memory required for data: 49944120
I0509 16:01:32.635352 118538 layer_factory.hpp:77] Creating layer pool2
I0509 16:01:32.635359 118538 net.cpp:84] Creating Layer pool2
I0509 16:01:32.635360 118538 net.cpp:406] pool2 <- conv2
I0509 16:01:32.635365 118538 net.cpp:380] pool2 -> pool2
I0509 16:01:32.635370 118538 net.cpp:122] Setting up pool2
I0509 16:01:32.635373 118538 net.cpp:129] Top shape: 10 256 13 13 (432640)
I0509 16:01:32.635375 118538 net.cpp:137] Memory required for data: 51674680
I0509 16:01:32.635378 118538 layer_factory.hpp:77] Creating layer norm2
I0509 16:01:32.635386 118538 net.cpp:84] Creating Layer norm2
I0509 16:01:32.635390 118538 net.cpp:406] norm2 <- pool2
I0509 16:01:32.635393 118538 net.cpp:380] norm2 -> norm2
I0509 16:01:32.635398 118538 net.cpp:122] Setting up norm2
I0509 16:01:32.635402 118538 net.cpp:129] Top shape: 10 256 13 13 (432640)
I0509 16:01:32.635404 118538 net.cpp:137] Memory required for data: 53405240
I0509 16:01:32.635407 118538 layer_factory.hpp:77] Creating layer conv3
I0509 16:01:32.635413 118538 net.cpp:84] Creating Layer conv3
I0509 16:01:32.635416 118538 net.cpp:406] conv3 <- norm2
I0509 16:01:32.635419 118538 net.cpp:380] conv3 -> conv3
I0509 16:01:32.635426 118538 base_conv_layer.cpp:121] Group is 1channel is256number of output is 384
I0509 16:01:32.636448 118538 net.cpp:122] Setting up conv3
I0509 16:01:32.636456 118538 net.cpp:129] Top shape: 10 384 13 13 (648960)
I0509 16:01:32.636458 118538 net.cpp:137] Memory required for data: 56001080
I0509 16:01:32.636466 118538 layer_factory.hpp:77] Creating layer relu3
I0509 16:01:32.636471 118538 net.cpp:84] Creating Layer relu3
I0509 16:01:32.636473 118538 net.cpp:406] relu3 <- conv3
I0509 16:01:32.636477 118538 net.cpp:367] relu3 -> conv3 (in-place)
I0509 16:01:32.636482 118538 net.cpp:122] Setting up relu3
I0509 16:01:32.636485 118538 net.cpp:129] Top shape: 10 384 13 13 (648960)
I0509 16:01:32.636487 118538 net.cpp:137] Memory required for data: 58596920
I0509 16:01:32.636489 118538 layer_factory.hpp:77] Creating layer conv4
I0509 16:01:32.636493 118538 net.cpp:84] Creating Layer conv4
I0509 16:01:32.636497 118538 net.cpp:406] conv4 <- conv3
I0509 16:01:32.636500 118538 net.cpp:380] conv4 -> conv4
I0509 16:01:32.636508 118538 base_conv_layer.cpp:121] Group is 2channel is384number of output is 384
I0509 16:01:32.637470 118538 net.cpp:122] Setting up conv4
I0509 16:01:32.637475 118538 net.cpp:129] Top shape: 10 384 13 13 (648960)
I0509 16:01:32.637478 118538 net.cpp:137] Memory required for data: 61192760
I0509 16:01:32.637482 118538 layer_factory.hpp:77] Creating layer relu4
I0509 16:01:32.637486 118538 net.cpp:84] Creating Layer relu4
I0509 16:01:32.637490 118538 net.cpp:406] relu4 <- conv4
I0509 16:01:32.637495 118538 net.cpp:367] relu4 -> conv4 (in-place)
I0509 16:01:32.637498 118538 net.cpp:122] Setting up relu4
I0509 16:01:32.637501 118538 net.cpp:129] Top shape: 10 384 13 13 (648960)
I0509 16:01:32.637504 118538 net.cpp:137] Memory required for data: 63788600
I0509 16:01:32.637506 118538 layer_factory.hpp:77] Creating layer conv5
I0509 16:01:32.637511 118538 net.cpp:84] Creating Layer conv5
I0509 16:01:32.637513 118538 net.cpp:406] conv5 <- conv4
I0509 16:01:32.637518 118538 net.cpp:380] conv5 -> conv5
I0509 16:01:32.637524 118538 base_conv_layer.cpp:121] Group is 2channel is384number of output is 256
I0509 16:01:32.638166 118538 net.cpp:122] Setting up conv5
I0509 16:01:32.638171 118538 net.cpp:129] Top shape: 10 256 13 13 (432640)
I0509 16:01:32.638173 118538 net.cpp:137] Memory required for data: 65519160
I0509 16:01:32.638180 118538 layer_factory.hpp:77] Creating layer relu5
I0509 16:01:32.638185 118538 net.cpp:84] Creating Layer relu5
I0509 16:01:32.638187 118538 net.cpp:406] relu5 <- conv5
I0509 16:01:32.638191 118538 net.cpp:367] relu5 -> conv5 (in-place)
I0509 16:01:32.638195 118538 net.cpp:122] Setting up relu5
I0509 16:01:32.638198 118538 net.cpp:129] Top shape: 10 256 13 13 (432640)
I0509 16:01:32.638201 118538 net.cpp:137] Memory required for data: 67249720
I0509 16:01:32.638203 118538 layer_factory.hpp:77] Creating layer pool5
I0509 16:01:32.638211 118538 net.cpp:84] Creating Layer pool5
I0509 16:01:32.638213 118538 net.cpp:406] pool5 <- conv5
I0509 16:01:32.638217 118538 net.cpp:380] pool5 -> pool5
I0509 16:01:32.638223 118538 net.cpp:122] Setting up pool5
I0509 16:01:32.638227 118538 net.cpp:129] Top shape: 10 256 6 6 (92160)
I0509 16:01:32.638229 118538 net.cpp:137] Memory required for data: 67618360
I0509 16:01:32.638232 118538 layer_factory.hpp:77] Creating layer fc6
I0509 16:01:32.638238 118538 net.cpp:84] Creating Layer fc6
I0509 16:01:32.638242 118538 net.cpp:406] fc6 <- pool5
I0509 16:01:32.638248 118538 net.cpp:380] fc6 -> fc6
I0509 16:01:32.666746 118538 net.cpp:122] Setting up fc6
I0509 16:01:32.666770 118538 net.cpp:129] Top shape: 10 4096 (40960)
I0509 16:01:32.666774 118538 net.cpp:137] Memory required for data: 67782200
I0509 16:01:32.666780 118538 layer_factory.hpp:77] Creating layer relu6
I0509 16:01:32.666788 118538 net.cpp:84] Creating Layer relu6
I0509 16:01:32.666792 118538 net.cpp:406] relu6 <- fc6
I0509 16:01:32.666800 118538 net.cpp:367] relu6 -> fc6 (in-place)
I0509 16:01:32.666806 118538 net.cpp:122] Setting up relu6
I0509 16:01:32.666810 118538 net.cpp:129] Top shape: 10 4096 (40960)
I0509 16:01:32.666812 118538 net.cpp:137] Memory required for data: 67946040
I0509 16:01:32.666815 118538 layer_factory.hpp:77] Creating layer drop6
I0509 16:01:32.666821 118538 net.cpp:84] Creating Layer drop6
I0509 16:01:32.666822 118538 net.cpp:406] drop6 <- fc6
I0509 16:01:32.666826 118538 net.cpp:367] drop6 -> fc6 (in-place)
I0509 16:01:32.666831 118538 net.cpp:122] Setting up drop6
I0509 16:01:32.666836 118538 net.cpp:129] Top shape: 10 4096 (40960)
I0509 16:01:32.666837 118538 net.cpp:137] Memory required for data: 68109880
I0509 16:01:32.666839 118538 layer_factory.hpp:77] Creating layer fc7
I0509 16:01:32.666844 118538 net.cpp:84] Creating Layer fc7
I0509 16:01:32.666847 118538 net.cpp:406] fc7 <- fc6
I0509 16:01:32.666851 118538 net.cpp:380] fc7 -> fc7
I0509 16:01:32.679347 118538 net.cpp:122] Setting up fc7
I0509 16:01:32.679365 118538 net.cpp:129] Top shape: 10 4096 (40960)
I0509 16:01:32.679368 118538 net.cpp:137] Memory required for data: 68273720
I0509 16:01:32.679375 118538 layer_factory.hpp:77] Creating layer relu7
I0509 16:01:32.679381 118538 net.cpp:84] Creating Layer relu7
I0509 16:01:32.679385 118538 net.cpp:406] relu7 <- fc7
I0509 16:01:32.679391 118538 net.cpp:367] relu7 -> fc7 (in-place)
I0509 16:01:32.679396 118538 net.cpp:122] Setting up relu7
I0509 16:01:32.679400 118538 net.cpp:129] Top shape: 10 4096 (40960)
I0509 16:01:32.679402 118538 net.cpp:137] Memory required for data: 68437560
I0509 16:01:32.679405 118538 layer_factory.hpp:77] Creating layer drop7
I0509 16:01:32.679409 118538 net.cpp:84] Creating Layer drop7
I0509 16:01:32.679412 118538 net.cpp:406] drop7 <- fc7
I0509 16:01:32.679415 118538 net.cpp:367] drop7 -> fc7 (in-place)
I0509 16:01:32.679420 118538 net.cpp:122] Setting up drop7
I0509 16:01:32.679424 118538 net.cpp:129] Top shape: 10 4096 (40960)
I0509 16:01:32.679426 118538 net.cpp:137] Memory required for data: 68601400
I0509 16:01:32.679428 118538 layer_factory.hpp:77] Creating layer fc8
I0509 16:01:32.679435 118538 net.cpp:84] Creating Layer fc8
I0509 16:01:32.679436 118538 net.cpp:406] fc8 <- fc7
I0509 16:01:32.679440 118538 net.cpp:380] fc8 -> fc8
I0509 16:01:32.682659 118538 net.cpp:122] Setting up fc8
I0509 16:01:32.682665 118538 net.cpp:129] Top shape: 10 1000 (10000)
I0509 16:01:32.682668 118538 net.cpp:137] Memory required for data: 68641400
I0509 16:01:32.682672 118538 layer_factory.hpp:77] Creating layer prob
I0509 16:01:32.682678 118538 net.cpp:84] Creating Layer prob
I0509 16:01:32.682680 118538 net.cpp:406] prob <- fc8
I0509 16:01:32.682687 118538 net.cpp:380] prob -> prob
I0509 16:01:32.682696 118538 net.cpp:122] Setting up prob
I0509 16:01:32.682699 118538 net.cpp:129] Top shape: 10 1000 (10000)
I0509 16:01:32.682701 118538 net.cpp:137] Memory required for data: 68681400
I0509 16:01:32.682704 118538 net.cpp:200] prob does not need backward computation.
I0509 16:01:32.682708 118538 net.cpp:200] fc8 does not need backward computation.
I0509 16:01:32.682711 118538 net.cpp:200] drop7 does not need backward computation.
I0509 16:01:32.682713 118538 net.cpp:200] relu7 does not need backward computation.
I0509 16:01:32.682715 118538 net.cpp:200] fc7 does not need backward computation.
I0509 16:01:32.682718 118538 net.cpp:200] drop6 does not need backward computation.
I0509 16:01:32.682721 118538 net.cpp:200] relu6 does not need backward computation.
I0509 16:01:32.682724 118538 net.cpp:200] fc6 does not need backward computation.
I0509 16:01:32.682736 118538 net.cpp:200] pool5 does not need backward computation.
I0509 16:01:32.682739 118538 net.cpp:200] relu5 does not need backward computation.
I0509 16:01:32.682742 118538 net.cpp:200] conv5 does not need backward computation.
I0509 16:01:32.682745 118538 net.cpp:200] relu4 does not need backward computation.
I0509 16:01:32.682749 118538 net.cpp:200] conv4 does not need backward computation.
I0509 16:01:32.682751 118538 net.cpp:200] relu3 does not need backward computation.
I0509 16:01:32.682754 118538 net.cpp:200] conv3 does not need backward computation.
I0509 16:01:32.682757 118538 net.cpp:200] norm2 does not need backward computation.
I0509 16:01:32.682760 118538 net.cpp:200] pool2 does not need backward computation.
I0509 16:01:32.682763 118538 net.cpp:200] relu2 does not need backward computation.
I0509 16:01:32.682766 118538 net.cpp:200] conv2 does not need backward computation.
I0509 16:01:32.682770 118538 net.cpp:200] norm1 does not need backward computation.
I0509 16:01:32.682772 118538 net.cpp:200] pool1 does not need backward computation.
I0509 16:01:32.682777 118538 net.cpp:200] relu1 does not need backward computation.
I0509 16:01:32.682780 118538 net.cpp:200] conv1 does not need backward computation.
I0509 16:01:32.682782 118538 net.cpp:200] data does not need backward computation.
I0509 16:01:32.682785 118538 net.cpp:242] This network produces output prob
I0509 16:01:32.682796 118538 net.cpp:255] Network initialization done.
     0.701253947             21,257      LLC-load-misses          
     0.701253947            278,608      LLC-loads                
     0.801435083             54,668      LLC-load-misses          
     0.801435083            303,604      LLC-loads                
I0509 16:01:32.834342 118538 net.cpp:744] Ignoring source layer loss
Traceback (most recent call last):
  File "test_model.py", line 32, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 129, in _Net_forward
    self.blobs[in_].data[...] = blob
ValueError: could not broadcast input array from shape (10,1,28,28) into shape (10,3,227,227)
     0.873737062            360,435      LLC-load-misses          
     0.873737062          1,421,243      LLC-loads                
