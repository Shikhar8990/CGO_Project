WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:00.551960  1602 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:00.551991  1602 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:00.551995  1602 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:00.553488  1602 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:00.553637  1602 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:00.553706  1602 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:00.554627  1602 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:00.554831  1602 net.cpp:84] Creating Layer pair_data
I0429 17:11:00.554843  1602 net.cpp:380] pair_data -> pair_data
I0429 17:11:00.554855  1602 net.cpp:380] pair_data -> sim
I0429 17:11:00.554877  1602 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:00.555441  1602 net.cpp:122] Setting up pair_data
I0429 17:11:00.555451  1602 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:00.555456  1602 net.cpp:129] Top shape: 100 (100)
I0429 17:11:00.555459  1602 net.cpp:137] Memory required for data: 627600
I0429 17:11:00.555464  1602 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:00.555471  1602 net.cpp:84] Creating Layer slice_pair
I0429 17:11:00.555476  1602 net.cpp:406] slice_pair <- pair_data
I0429 17:11:00.555481  1602 net.cpp:380] slice_pair -> data
I0429 17:11:00.555488  1602 net.cpp:380] slice_pair -> data_p
I0429 17:11:00.555497  1602 net.cpp:122] Setting up slice_pair
I0429 17:11:00.555503  1602 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:00.555507  1602 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:00.555510  1602 net.cpp:137] Memory required for data: 1254800
I0429 17:11:00.555513  1602 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:00.555523  1602 net.cpp:84] Creating Layer conv1
I0429 17:11:00.555527  1602 net.cpp:406] conv1 <- data
I0429 17:11:00.555532  1602 net.cpp:380] conv1 -> conv1
I0429 17:11:00.555562  1602 net.cpp:122] Setting up conv1
I0429 17:11:00.555569  1602 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:00.555572  1602 net.cpp:137] Memory required for data: 5862800
I0429 17:11:00.555580  1602 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:00.555588  1602 net.cpp:84] Creating Layer pool1
I0429 17:11:00.555593  1602 net.cpp:406] pool1 <- conv1
I0429 17:11:00.555598  1602 net.cpp:380] pool1 -> pool1
I0429 17:11:00.555605  1602 net.cpp:122] Setting up pool1
I0429 17:11:00.555610  1602 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:00.555614  1602 net.cpp:137] Memory required for data: 7014800
I0429 17:11:00.555618  1602 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:00.555627  1602 net.cpp:84] Creating Layer conv2
I0429 17:11:00.555631  1602 net.cpp:406] conv2 <- pool1
I0429 17:11:00.555637  1602 net.cpp:380] conv2 -> conv2
I0429 17:11:00.555825  1602 net.cpp:122] Setting up conv2
I0429 17:11:00.555831  1602 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:00.555835  1602 net.cpp:137] Memory required for data: 8294800
I0429 17:11:00.555841  1602 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:00.555847  1602 net.cpp:84] Creating Layer pool2
I0429 17:11:00.555852  1602 net.cpp:406] pool2 <- conv2
I0429 17:11:00.555862  1602 net.cpp:380] pool2 -> pool2
I0429 17:11:00.555871  1602 net.cpp:122] Setting up pool2
I0429 17:11:00.555876  1602 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:00.555878  1602 net.cpp:137] Memory required for data: 8614800
I0429 17:11:00.555881  1602 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:00.555888  1602 net.cpp:84] Creating Layer ip1
I0429 17:11:00.555891  1602 net.cpp:406] ip1 <- pool2
I0429 17:11:00.555898  1602 net.cpp:380] ip1 -> ip1
I0429 17:11:00.558650  1602 net.cpp:122] Setting up ip1
I0429 17:11:00.558660  1602 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:00.558662  1602 net.cpp:137] Memory required for data: 8814800
I0429 17:11:00.558670  1602 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:00.558676  1602 net.cpp:84] Creating Layer relu1
I0429 17:11:00.558681  1602 net.cpp:406] relu1 <- ip1
I0429 17:11:00.558686  1602 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:00.558693  1602 net.cpp:122] Setting up relu1
I0429 17:11:00.558697  1602 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:00.558701  1602 net.cpp:137] Memory required for data: 9014800
I0429 17:11:00.558704  1602 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:00.558712  1602 net.cpp:84] Creating Layer ip2
I0429 17:11:00.558715  1602 net.cpp:406] ip2 <- ip1
I0429 17:11:00.558722  1602 net.cpp:380] ip2 -> ip2
I0429 17:11:00.558765  1602 net.cpp:122] Setting up ip2
I0429 17:11:00.558770  1602 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:00.558773  1602 net.cpp:137] Memory required for data: 9018800
I0429 17:11:00.558779  1602 layer_factory.hpp:77] Creating layer feat
I0429 17:11:00.558784  1602 net.cpp:84] Creating Layer feat
I0429 17:11:00.558787  1602 net.cpp:406] feat <- ip2
I0429 17:11:00.558794  1602 net.cpp:380] feat -> feat
I0429 17:11:00.558804  1602 net.cpp:122] Setting up feat
I0429 17:11:00.558807  1602 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:00.558810  1602 net.cpp:137] Memory required for data: 9019600
I0429 17:11:00.558817  1602 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:00.558826  1602 net.cpp:84] Creating Layer conv1_p
I0429 17:11:00.558830  1602 net.cpp:406] conv1_p <- data_p
I0429 17:11:00.558835  1602 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:00.558857  1602 net.cpp:122] Setting up conv1_p
I0429 17:11:00.558863  1602 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:00.558867  1602 net.cpp:137] Memory required for data: 13627600
I0429 17:11:00.558871  1602 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:00.558876  1602 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:00.558878  1602 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:00.558883  1602 net.cpp:84] Creating Layer pool1_p
I0429 17:11:00.558887  1602 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:00.558892  1602 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:00.558899  1602 net.cpp:122] Setting up pool1_p
I0429 17:11:00.558904  1602 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:00.558907  1602 net.cpp:137] Memory required for data: 14779600
I0429 17:11:00.558910  1602 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:00.558919  1602 net.cpp:84] Creating Layer conv2_p
I0429 17:11:00.558923  1602 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:00.558928  1602 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:00.559115  1602 net.cpp:122] Setting up conv2_p
I0429 17:11:00.559123  1602 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:00.559125  1602 net.cpp:137] Memory required for data: 16059600
I0429 17:11:00.559129  1602 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:00.559134  1602 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:00.559137  1602 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:00.559141  1602 net.cpp:84] Creating Layer pool2_p
I0429 17:11:00.559144  1602 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:00.559151  1602 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:00.559159  1602 net.cpp:122] Setting up pool2_p
I0429 17:11:00.559167  1602 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:00.559170  1602 net.cpp:137] Memory required for data: 16379600
I0429 17:11:00.559175  1602 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:00.559180  1602 net.cpp:84] Creating Layer ip1_p
I0429 17:11:00.559183  1602 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:00.559190  1602 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:00.561913  1602 net.cpp:122] Setting up ip1_p
I0429 17:11:00.561920  1602 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:00.561923  1602 net.cpp:137] Memory required for data: 16579600
I0429 17:11:00.561926  1602 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:00.561930  1602 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:00.561933  1602 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:00.561939  1602 net.cpp:84] Creating Layer relu1_p
I0429 17:11:00.561942  1602 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:00.561947  1602 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:00.561954  1602 net.cpp:122] Setting up relu1_p
I0429 17:11:00.561957  1602 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:00.561960  1602 net.cpp:137] Memory required for data: 16779600
I0429 17:11:00.561964  1602 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:00.561971  1602 net.cpp:84] Creating Layer ip2_p
I0429 17:11:00.561975  1602 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:00.561980  1602 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:00.562023  1602 net.cpp:122] Setting up ip2_p
I0429 17:11:00.562028  1602 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:00.562031  1602 net.cpp:137] Memory required for data: 16783600
I0429 17:11:00.562036  1602 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:00.562041  1602 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:00.562043  1602 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:00.562049  1602 net.cpp:84] Creating Layer feat_p
I0429 17:11:00.562053  1602 net.cpp:406] feat_p <- ip2_p
I0429 17:11:00.562057  1602 net.cpp:380] feat_p -> feat_p
I0429 17:11:00.562067  1602 net.cpp:122] Setting up feat_p
I0429 17:11:00.562072  1602 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:00.562073  1602 net.cpp:137] Memory required for data: 16784400
I0429 17:11:00.562077  1602 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:00.562081  1602 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:00.562084  1602 layer_factory.hpp:77] Creating layer loss
I0429 17:11:00.562090  1602 net.cpp:84] Creating Layer loss
I0429 17:11:00.562093  1602 net.cpp:406] loss <- feat
I0429 17:11:00.562098  1602 net.cpp:406] loss <- feat_p
I0429 17:11:00.562101  1602 net.cpp:406] loss <- sim
I0429 17:11:00.562105  1602 net.cpp:380] loss -> loss
I0429 17:11:00.562115  1602 net.cpp:122] Setting up loss
I0429 17:11:00.562119  1602 net.cpp:129] Top shape: (1)
I0429 17:11:00.562122  1602 net.cpp:132]     with loss weight 1
I0429 17:11:00.562129  1602 net.cpp:137] Memory required for data: 16784404
I0429 17:11:00.562132  1602 net.cpp:198] loss needs backward computation.
I0429 17:11:00.562137  1602 net.cpp:198] feat_p needs backward computation.
I0429 17:11:00.562140  1602 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:00.562144  1602 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:00.562146  1602 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:00.562150  1602 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:00.562152  1602 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:00.562155  1602 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:00.562158  1602 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:00.562162  1602 net.cpp:198] feat needs backward computation.
I0429 17:11:00.562165  1602 net.cpp:198] ip2 needs backward computation.
I0429 17:11:00.562168  1602 net.cpp:198] relu1 needs backward computation.
I0429 17:11:00.562175  1602 net.cpp:198] ip1 needs backward computation.
I0429 17:11:00.562180  1602 net.cpp:198] pool2 needs backward computation.
I0429 17:11:00.562182  1602 net.cpp:198] conv2 needs backward computation.
I0429 17:11:00.562186  1602 net.cpp:198] pool1 needs backward computation.
I0429 17:11:00.562188  1602 net.cpp:198] conv1 needs backward computation.
I0429 17:11:00.562193  1602 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:00.562197  1602 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:00.562201  1602 net.cpp:242] This network produces output loss
I0429 17:11:00.562288  1602 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:01.380362  1630 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:01.380420  1630 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:01.380424  1630 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:01.381831  1630 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:01.381981  1630 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:01.382052  1630 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:01.382843  1630 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:01.383021  1630 net.cpp:84] Creating Layer pair_data
I0429 17:11:01.383031  1630 net.cpp:380] pair_data -> pair_data
I0429 17:11:01.383044  1630 net.cpp:380] pair_data -> sim
I0429 17:11:01.383064  1630 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:01.383600  1630 net.cpp:122] Setting up pair_data
I0429 17:11:01.383627  1630 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:01.383632  1630 net.cpp:129] Top shape: 100 (100)
I0429 17:11:01.383635  1630 net.cpp:137] Memory required for data: 627600
I0429 17:11:01.383640  1630 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:01.383646  1630 net.cpp:84] Creating Layer slice_pair
I0429 17:11:01.383651  1630 net.cpp:406] slice_pair <- pair_data
I0429 17:11:01.383656  1630 net.cpp:380] slice_pair -> data
I0429 17:11:01.383662  1630 net.cpp:380] slice_pair -> data_p
I0429 17:11:01.383672  1630 net.cpp:122] Setting up slice_pair
I0429 17:11:01.383677  1630 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:01.383682  1630 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:01.383685  1630 net.cpp:137] Memory required for data: 1254800
I0429 17:11:01.383688  1630 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:01.383697  1630 net.cpp:84] Creating Layer conv1
I0429 17:11:01.383702  1630 net.cpp:406] conv1 <- data
I0429 17:11:01.383707  1630 net.cpp:380] conv1 -> conv1
I0429 17:11:01.383735  1630 net.cpp:122] Setting up conv1
I0429 17:11:01.383746  1630 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:01.383749  1630 net.cpp:137] Memory required for data: 5862800
I0429 17:11:01.383759  1630 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:01.383766  1630 net.cpp:84] Creating Layer pool1
I0429 17:11:01.383770  1630 net.cpp:406] pool1 <- conv1
I0429 17:11:01.383774  1630 net.cpp:380] pool1 -> pool1
I0429 17:11:01.383783  1630 net.cpp:122] Setting up pool1
I0429 17:11:01.383787  1630 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:01.383791  1630 net.cpp:137] Memory required for data: 7014800
I0429 17:11:01.383795  1630 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:01.383805  1630 net.cpp:84] Creating Layer conv2
I0429 17:11:01.383807  1630 net.cpp:406] conv2 <- pool1
I0429 17:11:01.383813  1630 net.cpp:380] conv2 -> conv2
I0429 17:11:01.384001  1630 net.cpp:122] Setting up conv2
I0429 17:11:01.384006  1630 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:01.384009  1630 net.cpp:137] Memory required for data: 8294800
I0429 17:11:01.384016  1630 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:01.384022  1630 net.cpp:84] Creating Layer pool2
I0429 17:11:01.384026  1630 net.cpp:406] pool2 <- conv2
I0429 17:11:01.384032  1630 net.cpp:380] pool2 -> pool2
I0429 17:11:01.384048  1630 net.cpp:122] Setting up pool2
I0429 17:11:01.384054  1630 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:01.384058  1630 net.cpp:137] Memory required for data: 8614800
I0429 17:11:01.384061  1630 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:01.384068  1630 net.cpp:84] Creating Layer ip1
I0429 17:11:01.384070  1630 net.cpp:406] ip1 <- pool2
I0429 17:11:01.384078  1630 net.cpp:380] ip1 -> ip1
I0429 17:11:01.386811  1630 net.cpp:122] Setting up ip1
I0429 17:11:01.386818  1630 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:01.386821  1630 net.cpp:137] Memory required for data: 8814800
I0429 17:11:01.386828  1630 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:01.386833  1630 net.cpp:84] Creating Layer relu1
I0429 17:11:01.386843  1630 net.cpp:406] relu1 <- ip1
I0429 17:11:01.386847  1630 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:01.386853  1630 net.cpp:122] Setting up relu1
I0429 17:11:01.386857  1630 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:01.386860  1630 net.cpp:137] Memory required for data: 9014800
I0429 17:11:01.386863  1630 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:01.386870  1630 net.cpp:84] Creating Layer ip2
I0429 17:11:01.386873  1630 net.cpp:406] ip2 <- ip1
I0429 17:11:01.386879  1630 net.cpp:380] ip2 -> ip2
I0429 17:11:01.386922  1630 net.cpp:122] Setting up ip2
I0429 17:11:01.386927  1630 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:01.386930  1630 net.cpp:137] Memory required for data: 9018800
I0429 17:11:01.386935  1630 layer_factory.hpp:77] Creating layer feat
I0429 17:11:01.386940  1630 net.cpp:84] Creating Layer feat
I0429 17:11:01.386945  1630 net.cpp:406] feat <- ip2
I0429 17:11:01.386950  1630 net.cpp:380] feat -> feat
I0429 17:11:01.386960  1630 net.cpp:122] Setting up feat
I0429 17:11:01.386963  1630 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:01.386966  1630 net.cpp:137] Memory required for data: 9019600
I0429 17:11:01.386972  1630 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:01.386981  1630 net.cpp:84] Creating Layer conv1_p
I0429 17:11:01.386983  1630 net.cpp:406] conv1_p <- data_p
I0429 17:11:01.386989  1630 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:01.387012  1630 net.cpp:122] Setting up conv1_p
I0429 17:11:01.387017  1630 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:01.387022  1630 net.cpp:137] Memory required for data: 13627600
I0429 17:11:01.387024  1630 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:01.387028  1630 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:01.387032  1630 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:01.387037  1630 net.cpp:84] Creating Layer pool1_p
I0429 17:11:01.387040  1630 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:01.387049  1630 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:01.387058  1630 net.cpp:122] Setting up pool1_p
I0429 17:11:01.387063  1630 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:01.387065  1630 net.cpp:137] Memory required for data: 14779600
I0429 17:11:01.387068  1630 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:01.387078  1630 net.cpp:84] Creating Layer conv2_p
I0429 17:11:01.387080  1630 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:01.387086  1630 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:01.387276  1630 net.cpp:122] Setting up conv2_p
I0429 17:11:01.387282  1630 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:01.387285  1630 net.cpp:137] Memory required for data: 16059600
I0429 17:11:01.387290  1630 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:01.387293  1630 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:01.387296  1630 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:01.387300  1630 net.cpp:84] Creating Layer pool2_p
I0429 17:11:01.387303  1630 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:01.387310  1630 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:01.387316  1630 net.cpp:122] Setting up pool2_p
I0429 17:11:01.387321  1630 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:01.387325  1630 net.cpp:137] Memory required for data: 16379600
I0429 17:11:01.387327  1630 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:01.387332  1630 net.cpp:84] Creating Layer ip1_p
I0429 17:11:01.387336  1630 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:01.387341  1630 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:01.390096  1630 net.cpp:122] Setting up ip1_p
I0429 17:11:01.390105  1630 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:01.390107  1630 net.cpp:137] Memory required for data: 16579600
I0429 17:11:01.390111  1630 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:01.390115  1630 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:01.390120  1630 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:01.390125  1630 net.cpp:84] Creating Layer relu1_p
I0429 17:11:01.390127  1630 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:01.390132  1630 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:01.390139  1630 net.cpp:122] Setting up relu1_p
I0429 17:11:01.390143  1630 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:01.390146  1630 net.cpp:137] Memory required for data: 16779600
I0429 17:11:01.390148  1630 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:01.390156  1630 net.cpp:84] Creating Layer ip2_p
I0429 17:11:01.390161  1630 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:01.390166  1630 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:01.390210  1630 net.cpp:122] Setting up ip2_p
I0429 17:11:01.390216  1630 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:01.390219  1630 net.cpp:137] Memory required for data: 16783600
I0429 17:11:01.390224  1630 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:01.390228  1630 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:01.390233  1630 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:01.390239  1630 net.cpp:84] Creating Layer feat_p
I0429 17:11:01.390241  1630 net.cpp:406] feat_p <- ip2_p
I0429 17:11:01.390246  1630 net.cpp:380] feat_p -> feat_p
I0429 17:11:01.390255  1630 net.cpp:122] Setting up feat_p
I0429 17:11:01.390259  1630 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:01.390262  1630 net.cpp:137] Memory required for data: 16784400
I0429 17:11:01.390265  1630 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:01.390269  1630 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:01.390272  1630 layer_factory.hpp:77] Creating layer loss
I0429 17:11:01.390280  1630 net.cpp:84] Creating Layer loss
I0429 17:11:01.390282  1630 net.cpp:406] loss <- feat
I0429 17:11:01.390287  1630 net.cpp:406] loss <- feat_p
I0429 17:11:01.390295  1630 net.cpp:406] loss <- sim
I0429 17:11:01.390300  1630 net.cpp:380] loss -> loss
I0429 17:11:01.390310  1630 net.cpp:122] Setting up loss
I0429 17:11:01.390314  1630 net.cpp:129] Top shape: (1)
I0429 17:11:01.390317  1630 net.cpp:132]     with loss weight 1
I0429 17:11:01.390324  1630 net.cpp:137] Memory required for data: 16784404
I0429 17:11:01.390328  1630 net.cpp:198] loss needs backward computation.
I0429 17:11:01.390333  1630 net.cpp:198] feat_p needs backward computation.
I0429 17:11:01.390336  1630 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:01.390339  1630 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:01.390342  1630 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:01.390346  1630 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:01.390348  1630 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:01.390352  1630 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:01.390355  1630 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:01.390359  1630 net.cpp:198] feat needs backward computation.
I0429 17:11:01.390362  1630 net.cpp:198] ip2 needs backward computation.
I0429 17:11:01.390365  1630 net.cpp:198] relu1 needs backward computation.
I0429 17:11:01.390368  1630 net.cpp:198] ip1 needs backward computation.
I0429 17:11:01.390372  1630 net.cpp:198] pool2 needs backward computation.
I0429 17:11:01.390375  1630 net.cpp:198] conv2 needs backward computation.
I0429 17:11:01.390378  1630 net.cpp:198] pool1 needs backward computation.
I0429 17:11:01.390382  1630 net.cpp:198] conv1 needs backward computation.
I0429 17:11:01.390388  1630 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:01.390391  1630 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:01.390394  1630 net.cpp:242] This network produces output loss
I0429 17:11:01.390475  1630 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:02.137811  1658 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:02.137837  1658 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:02.137840  1658 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:02.139245  1658 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:02.139396  1658 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:02.139464  1658 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:02.140230  1658 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:02.140444  1658 net.cpp:84] Creating Layer pair_data
I0429 17:11:02.140453  1658 net.cpp:380] pair_data -> pair_data
I0429 17:11:02.140465  1658 net.cpp:380] pair_data -> sim
I0429 17:11:02.140491  1658 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:02.141031  1658 net.cpp:122] Setting up pair_data
I0429 17:11:02.141058  1658 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:02.141063  1658 net.cpp:129] Top shape: 100 (100)
I0429 17:11:02.141067  1658 net.cpp:137] Memory required for data: 627600
I0429 17:11:02.141070  1658 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:02.141077  1658 net.cpp:84] Creating Layer slice_pair
I0429 17:11:02.141082  1658 net.cpp:406] slice_pair <- pair_data
I0429 17:11:02.141088  1658 net.cpp:380] slice_pair -> data
I0429 17:11:02.141094  1658 net.cpp:380] slice_pair -> data_p
I0429 17:11:02.141103  1658 net.cpp:122] Setting up slice_pair
I0429 17:11:02.141108  1658 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:02.141113  1658 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:02.141115  1658 net.cpp:137] Memory required for data: 1254800
I0429 17:11:02.141119  1658 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:02.141129  1658 net.cpp:84] Creating Layer conv1
I0429 17:11:02.141131  1658 net.cpp:406] conv1 <- data
I0429 17:11:02.141136  1658 net.cpp:380] conv1 -> conv1
I0429 17:11:02.141173  1658 net.cpp:122] Setting up conv1
I0429 17:11:02.141180  1658 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:02.141183  1658 net.cpp:137] Memory required for data: 5862800
I0429 17:11:02.141191  1658 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:02.141198  1658 net.cpp:84] Creating Layer pool1
I0429 17:11:02.141202  1658 net.cpp:406] pool1 <- conv1
I0429 17:11:02.141207  1658 net.cpp:380] pool1 -> pool1
I0429 17:11:02.141216  1658 net.cpp:122] Setting up pool1
I0429 17:11:02.141221  1658 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:02.141223  1658 net.cpp:137] Memory required for data: 7014800
I0429 17:11:02.141227  1658 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:02.141235  1658 net.cpp:84] Creating Layer conv2
I0429 17:11:02.141239  1658 net.cpp:406] conv2 <- pool1
I0429 17:11:02.141247  1658 net.cpp:380] conv2 -> conv2
I0429 17:11:02.141430  1658 net.cpp:122] Setting up conv2
I0429 17:11:02.141436  1658 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:02.141439  1658 net.cpp:137] Memory required for data: 8294800
I0429 17:11:02.141445  1658 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:02.141451  1658 net.cpp:84] Creating Layer pool2
I0429 17:11:02.141455  1658 net.cpp:406] pool2 <- conv2
I0429 17:11:02.141463  1658 net.cpp:380] pool2 -> pool2
I0429 17:11:02.141469  1658 net.cpp:122] Setting up pool2
I0429 17:11:02.141474  1658 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:02.141476  1658 net.cpp:137] Memory required for data: 8614800
I0429 17:11:02.141479  1658 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:02.141485  1658 net.cpp:84] Creating Layer ip1
I0429 17:11:02.141489  1658 net.cpp:406] ip1 <- pool2
I0429 17:11:02.141495  1658 net.cpp:380] ip1 -> ip1
I0429 17:11:02.144284  1658 net.cpp:122] Setting up ip1
I0429 17:11:02.144294  1658 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:02.144296  1658 net.cpp:137] Memory required for data: 8814800
I0429 17:11:02.144304  1658 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:02.144309  1658 net.cpp:84] Creating Layer relu1
I0429 17:11:02.144314  1658 net.cpp:406] relu1 <- ip1
I0429 17:11:02.144320  1658 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:02.144325  1658 net.cpp:122] Setting up relu1
I0429 17:11:02.144330  1658 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:02.144332  1658 net.cpp:137] Memory required for data: 9014800
I0429 17:11:02.144335  1658 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:02.144341  1658 net.cpp:84] Creating Layer ip2
I0429 17:11:02.144343  1658 net.cpp:406] ip2 <- ip1
I0429 17:11:02.144351  1658 net.cpp:380] ip2 -> ip2
I0429 17:11:02.144395  1658 net.cpp:122] Setting up ip2
I0429 17:11:02.144399  1658 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:02.144402  1658 net.cpp:137] Memory required for data: 9018800
I0429 17:11:02.144407  1658 layer_factory.hpp:77] Creating layer feat
I0429 17:11:02.144417  1658 net.cpp:84] Creating Layer feat
I0429 17:11:02.144420  1658 net.cpp:406] feat <- ip2
I0429 17:11:02.144426  1658 net.cpp:380] feat -> feat
I0429 17:11:02.144436  1658 net.cpp:122] Setting up feat
I0429 17:11:02.144440  1658 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:02.144443  1658 net.cpp:137] Memory required for data: 9019600
I0429 17:11:02.144449  1658 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:02.144459  1658 net.cpp:84] Creating Layer conv1_p
I0429 17:11:02.144464  1658 net.cpp:406] conv1_p <- data_p
I0429 17:11:02.144469  1658 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:02.144490  1658 net.cpp:122] Setting up conv1_p
I0429 17:11:02.144496  1658 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:02.144500  1658 net.cpp:137] Memory required for data: 13627600
I0429 17:11:02.144503  1658 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:02.144507  1658 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:02.144511  1658 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:02.144516  1658 net.cpp:84] Creating Layer pool1_p
I0429 17:11:02.144520  1658 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:02.144523  1658 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:02.144531  1658 net.cpp:122] Setting up pool1_p
I0429 17:11:02.144536  1658 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:02.144539  1658 net.cpp:137] Memory required for data: 14779600
I0429 17:11:02.144542  1658 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:02.144552  1658 net.cpp:84] Creating Layer conv2_p
I0429 17:11:02.144556  1658 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:02.144562  1658 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:02.144745  1658 net.cpp:122] Setting up conv2_p
I0429 17:11:02.144752  1658 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:02.144755  1658 net.cpp:137] Memory required for data: 16059600
I0429 17:11:02.144758  1658 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:02.144762  1658 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:02.144767  1658 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:02.144771  1658 net.cpp:84] Creating Layer pool2_p
I0429 17:11:02.144774  1658 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:02.144779  1658 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:02.144788  1658 net.cpp:122] Setting up pool2_p
I0429 17:11:02.144791  1658 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:02.144794  1658 net.cpp:137] Memory required for data: 16379600
I0429 17:11:02.144798  1658 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:02.144803  1658 net.cpp:84] Creating Layer ip1_p
I0429 17:11:02.144805  1658 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:02.144811  1658 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:02.147498  1658 net.cpp:122] Setting up ip1_p
I0429 17:11:02.147505  1658 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:02.147506  1658 net.cpp:137] Memory required for data: 16579600
I0429 17:11:02.147511  1658 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:02.147513  1658 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:02.147517  1658 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:02.147522  1658 net.cpp:84] Creating Layer relu1_p
I0429 17:11:02.147526  1658 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:02.147529  1658 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:02.147536  1658 net.cpp:122] Setting up relu1_p
I0429 17:11:02.147539  1658 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:02.147542  1658 net.cpp:137] Memory required for data: 16779600
I0429 17:11:02.147544  1658 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:02.147552  1658 net.cpp:84] Creating Layer ip2_p
I0429 17:11:02.147555  1658 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:02.147559  1658 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:02.147603  1658 net.cpp:122] Setting up ip2_p
I0429 17:11:02.147610  1658 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:02.147614  1658 net.cpp:137] Memory required for data: 16783600
I0429 17:11:02.147619  1658 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:02.147622  1658 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:02.147625  1658 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:02.147631  1658 net.cpp:84] Creating Layer feat_p
I0429 17:11:02.147634  1658 net.cpp:406] feat_p <- ip2_p
I0429 17:11:02.147639  1658 net.cpp:380] feat_p -> feat_p
I0429 17:11:02.147647  1658 net.cpp:122] Setting up feat_p
I0429 17:11:02.147651  1658 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:02.147655  1658 net.cpp:137] Memory required for data: 16784400
I0429 17:11:02.147657  1658 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:02.147661  1658 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:02.147663  1658 layer_factory.hpp:77] Creating layer loss
I0429 17:11:02.147670  1658 net.cpp:84] Creating Layer loss
I0429 17:11:02.147673  1658 net.cpp:406] loss <- feat
I0429 17:11:02.147676  1658 net.cpp:406] loss <- feat_p
I0429 17:11:02.147680  1658 net.cpp:406] loss <- sim
I0429 17:11:02.147685  1658 net.cpp:380] loss -> loss
I0429 17:11:02.147697  1658 net.cpp:122] Setting up loss
I0429 17:11:02.147701  1658 net.cpp:129] Top shape: (1)
I0429 17:11:02.147703  1658 net.cpp:132]     with loss weight 1
I0429 17:11:02.147711  1658 net.cpp:137] Memory required for data: 16784404
I0429 17:11:02.147714  1658 net.cpp:198] loss needs backward computation.
I0429 17:11:02.147719  1658 net.cpp:198] feat_p needs backward computation.
I0429 17:11:02.147722  1658 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:02.147725  1658 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:02.147728  1658 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:02.147732  1658 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:02.147734  1658 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:02.147737  1658 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:02.147740  1658 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:02.147744  1658 net.cpp:198] feat needs backward computation.
I0429 17:11:02.147747  1658 net.cpp:198] ip2 needs backward computation.
I0429 17:11:02.147750  1658 net.cpp:198] relu1 needs backward computation.
I0429 17:11:02.147753  1658 net.cpp:198] ip1 needs backward computation.
I0429 17:11:02.147756  1658 net.cpp:198] pool2 needs backward computation.
I0429 17:11:02.147759  1658 net.cpp:198] conv2 needs backward computation.
I0429 17:11:02.147763  1658 net.cpp:198] pool1 needs backward computation.
I0429 17:11:02.147765  1658 net.cpp:198] conv1 needs backward computation.
I0429 17:11:02.147771  1658 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:02.147775  1658 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:02.147778  1658 net.cpp:242] This network produces output loss
I0429 17:11:02.147852  1658 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:02.844933  1691 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:02.844959  1691 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:02.844961  1691 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:02.846364  1691 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:02.846514  1691 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:02.846585  1691 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:02.847676  1691 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:02.848028  1691 net.cpp:84] Creating Layer pair_data
I0429 17:11:02.848062  1691 net.cpp:380] pair_data -> pair_data
I0429 17:11:02.848084  1691 net.cpp:380] pair_data -> sim
I0429 17:11:02.848119  1691 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:02.849066  1691 net.cpp:122] Setting up pair_data
I0429 17:11:02.849086  1691 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:02.849095  1691 net.cpp:129] Top shape: 100 (100)
I0429 17:11:02.849100  1691 net.cpp:137] Memory required for data: 627600
I0429 17:11:02.849107  1691 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:02.849120  1691 net.cpp:84] Creating Layer slice_pair
I0429 17:11:02.849128  1691 net.cpp:406] slice_pair <- pair_data
I0429 17:11:02.849138  1691 net.cpp:380] slice_pair -> data
I0429 17:11:02.849151  1691 net.cpp:380] slice_pair -> data_p
I0429 17:11:02.849170  1691 net.cpp:122] Setting up slice_pair
I0429 17:11:02.849180  1691 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:02.849189  1691 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:02.849195  1691 net.cpp:137] Memory required for data: 1254800
I0429 17:11:02.849200  1691 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:02.849217  1691 net.cpp:84] Creating Layer conv1
I0429 17:11:02.849223  1691 net.cpp:406] conv1 <- data
I0429 17:11:02.849234  1691 net.cpp:380] conv1 -> conv1
I0429 17:11:02.849288  1691 net.cpp:122] Setting up conv1
I0429 17:11:02.849300  1691 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:02.849306  1691 net.cpp:137] Memory required for data: 5862800
I0429 17:11:02.849323  1691 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:02.849335  1691 net.cpp:84] Creating Layer pool1
I0429 17:11:02.849342  1691 net.cpp:406] pool1 <- conv1
I0429 17:11:02.849351  1691 net.cpp:380] pool1 -> pool1
I0429 17:11:02.849366  1691 net.cpp:122] Setting up pool1
I0429 17:11:02.849375  1691 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:02.849381  1691 net.cpp:137] Memory required for data: 7014800
I0429 17:11:02.849386  1691 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:02.849406  1691 net.cpp:84] Creating Layer conv2
I0429 17:11:02.849411  1691 net.cpp:406] conv2 <- pool1
I0429 17:11:02.849423  1691 net.cpp:380] conv2 -> conv2
I0429 17:11:02.849773  1691 net.cpp:122] Setting up conv2
I0429 17:11:02.849786  1691 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:02.849792  1691 net.cpp:137] Memory required for data: 8294800
I0429 17:11:02.849805  1691 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:02.849817  1691 net.cpp:84] Creating Layer pool2
I0429 17:11:02.849824  1691 net.cpp:406] pool2 <- conv2
I0429 17:11:02.849835  1691 net.cpp:380] pool2 -> pool2
I0429 17:11:02.849848  1691 net.cpp:122] Setting up pool2
I0429 17:11:02.849858  1691 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:02.849862  1691 net.cpp:137] Memory required for data: 8614800
I0429 17:11:02.849869  1691 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:02.849880  1691 net.cpp:84] Creating Layer ip1
I0429 17:11:02.849895  1691 net.cpp:406] ip1 <- pool2
I0429 17:11:02.849906  1691 net.cpp:380] ip1 -> ip1
I0429 17:11:02.855033  1691 net.cpp:122] Setting up ip1
I0429 17:11:02.855049  1691 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:02.855056  1691 net.cpp:137] Memory required for data: 8814800
I0429 17:11:02.855072  1691 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:02.855083  1691 net.cpp:84] Creating Layer relu1
I0429 17:11:02.855090  1691 net.cpp:406] relu1 <- ip1
I0429 17:11:02.855100  1691 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:02.855113  1691 net.cpp:122] Setting up relu1
I0429 17:11:02.855120  1691 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:02.855126  1691 net.cpp:137] Memory required for data: 9014800
I0429 17:11:02.855131  1691 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:02.855144  1691 net.cpp:84] Creating Layer ip2
I0429 17:11:02.855149  1691 net.cpp:406] ip2 <- ip1
I0429 17:11:02.855164  1691 net.cpp:380] ip2 -> ip2
I0429 17:11:02.855245  1691 net.cpp:122] Setting up ip2
I0429 17:11:02.855254  1691 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:02.855259  1691 net.cpp:137] Memory required for data: 9018800
I0429 17:11:02.855269  1691 layer_factory.hpp:77] Creating layer feat
I0429 17:11:02.855278  1691 net.cpp:84] Creating Layer feat
I0429 17:11:02.855285  1691 net.cpp:406] feat <- ip2
I0429 17:11:02.855296  1691 net.cpp:380] feat -> feat
I0429 17:11:02.855314  1691 net.cpp:122] Setting up feat
I0429 17:11:02.855322  1691 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:02.855327  1691 net.cpp:137] Memory required for data: 9019600
I0429 17:11:02.855340  1691 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:02.855360  1691 net.cpp:84] Creating Layer conv1_p
I0429 17:11:02.855366  1691 net.cpp:406] conv1_p <- data_p
I0429 17:11:02.855376  1691 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:02.855417  1691 net.cpp:122] Setting up conv1_p
I0429 17:11:02.855432  1691 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:02.855437  1691 net.cpp:137] Memory required for data: 13627600
I0429 17:11:02.855444  1691 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:02.855453  1691 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:02.855459  1691 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:02.855468  1691 net.cpp:84] Creating Layer pool1_p
I0429 17:11:02.855474  1691 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:02.855484  1691 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:02.855499  1691 net.cpp:122] Setting up pool1_p
I0429 17:11:02.855506  1691 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:02.855514  1691 net.cpp:137] Memory required for data: 14779600
I0429 17:11:02.855518  1691 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:02.855535  1691 net.cpp:84] Creating Layer conv2_p
I0429 17:11:02.855541  1691 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:02.855551  1691 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:02.855901  1691 net.cpp:122] Setting up conv2_p
I0429 17:11:02.855914  1691 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:02.855921  1691 net.cpp:137] Memory required for data: 16059600
I0429 17:11:02.855927  1691 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:02.855936  1691 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:02.855942  1691 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:02.855950  1691 net.cpp:84] Creating Layer pool2_p
I0429 17:11:02.855957  1691 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:02.855967  1691 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:02.855981  1691 net.cpp:122] Setting up pool2_p
I0429 17:11:02.855989  1691 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:02.855995  1691 net.cpp:137] Memory required for data: 16379600
I0429 17:11:02.856000  1691 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:02.856010  1691 net.cpp:84] Creating Layer ip1_p
I0429 17:11:02.856016  1691 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:02.856053  1691 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:02.860829  1691 net.cpp:122] Setting up ip1_p
I0429 17:11:02.860843  1691 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:02.860848  1691 net.cpp:137] Memory required for data: 16579600
I0429 17:11:02.860855  1691 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:02.860862  1691 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:02.860868  1691 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:02.860878  1691 net.cpp:84] Creating Layer relu1_p
I0429 17:11:02.860884  1691 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:02.860894  1691 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:02.860904  1691 net.cpp:122] Setting up relu1_p
I0429 17:11:02.860911  1691 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:02.860916  1691 net.cpp:137] Memory required for data: 16779600
I0429 17:11:02.860921  1691 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:02.860934  1691 net.cpp:84] Creating Layer ip2_p
I0429 17:11:02.860940  1691 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:02.860949  1691 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:02.861027  1691 net.cpp:122] Setting up ip2_p
I0429 17:11:02.861037  1691 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:02.861042  1691 net.cpp:137] Memory required for data: 16783600
I0429 17:11:02.861052  1691 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:02.861058  1691 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:02.861064  1691 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:02.861075  1691 net.cpp:84] Creating Layer feat_p
I0429 17:11:02.861080  1691 net.cpp:406] feat_p <- ip2_p
I0429 17:11:02.861089  1691 net.cpp:380] feat_p -> feat_p
I0429 17:11:02.861105  1691 net.cpp:122] Setting up feat_p
I0429 17:11:02.861114  1691 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:02.861117  1691 net.cpp:137] Memory required for data: 16784400
I0429 17:11:02.861124  1691 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:02.861130  1691 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:02.861136  1691 layer_factory.hpp:77] Creating layer loss
I0429 17:11:02.861147  1691 net.cpp:84] Creating Layer loss
I0429 17:11:02.861152  1691 net.cpp:406] loss <- feat
I0429 17:11:02.861160  1691 net.cpp:406] loss <- feat_p
I0429 17:11:02.861166  1691 net.cpp:406] loss <- sim
I0429 17:11:02.861176  1691 net.cpp:380] loss -> loss
I0429 17:11:02.861191  1691 net.cpp:122] Setting up loss
I0429 17:11:02.861199  1691 net.cpp:129] Top shape: (1)
I0429 17:11:02.861203  1691 net.cpp:132]     with loss weight 1
I0429 17:11:02.861217  1691 net.cpp:137] Memory required for data: 16784404
I0429 17:11:02.861222  1691 net.cpp:198] loss needs backward computation.
I0429 17:11:02.861230  1691 net.cpp:198] feat_p needs backward computation.
I0429 17:11:02.861235  1691 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:02.861241  1691 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:02.861246  1691 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:02.861253  1691 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:02.861258  1691 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:02.861264  1691 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:02.861269  1691 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:02.861276  1691 net.cpp:198] feat needs backward computation.
I0429 17:11:02.861282  1691 net.cpp:198] ip2 needs backward computation.
I0429 17:11:02.861287  1691 net.cpp:198] relu1 needs backward computation.
I0429 17:11:02.861294  1691 net.cpp:198] ip1 needs backward computation.
I0429 17:11:02.861299  1691 net.cpp:198] pool2 needs backward computation.
I0429 17:11:02.861305  1691 net.cpp:198] conv2 needs backward computation.
I0429 17:11:02.861310  1691 net.cpp:198] pool1 needs backward computation.
I0429 17:11:02.861322  1691 net.cpp:198] conv1 needs backward computation.
I0429 17:11:02.861335  1691 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:02.861342  1691 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:02.861347  1691 net.cpp:242] This network produces output loss
I0429 17:11:02.861485  1691 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:03.548765  1719 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:03.548792  1719 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:03.548796  1719 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:03.550276  1719 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:03.550434  1719 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:03.550506  1719 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:03.551431  1719 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:03.551635  1719 net.cpp:84] Creating Layer pair_data
I0429 17:11:03.551646  1719 net.cpp:380] pair_data -> pair_data
I0429 17:11:03.551658  1719 net.cpp:380] pair_data -> sim
I0429 17:11:03.551682  1719 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:03.552254  1719 net.cpp:122] Setting up pair_data
I0429 17:11:03.552265  1719 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:03.552270  1719 net.cpp:129] Top shape: 100 (100)
I0429 17:11:03.552273  1719 net.cpp:137] Memory required for data: 627600
I0429 17:11:03.552278  1719 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:03.552285  1719 net.cpp:84] Creating Layer slice_pair
I0429 17:11:03.552289  1719 net.cpp:406] slice_pair <- pair_data
I0429 17:11:03.552294  1719 net.cpp:380] slice_pair -> data
I0429 17:11:03.552301  1719 net.cpp:380] slice_pair -> data_p
I0429 17:11:03.552310  1719 net.cpp:122] Setting up slice_pair
I0429 17:11:03.552315  1719 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:03.552320  1719 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:03.552323  1719 net.cpp:137] Memory required for data: 1254800
I0429 17:11:03.552326  1719 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:03.552335  1719 net.cpp:84] Creating Layer conv1
I0429 17:11:03.552340  1719 net.cpp:406] conv1 <- data
I0429 17:11:03.552345  1719 net.cpp:380] conv1 -> conv1
I0429 17:11:03.552377  1719 net.cpp:122] Setting up conv1
I0429 17:11:03.552383  1719 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:03.552386  1719 net.cpp:137] Memory required for data: 5862800
I0429 17:11:03.552394  1719 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:03.552402  1719 net.cpp:84] Creating Layer pool1
I0429 17:11:03.552407  1719 net.cpp:406] pool1 <- conv1
I0429 17:11:03.552417  1719 net.cpp:380] pool1 -> pool1
I0429 17:11:03.552424  1719 net.cpp:122] Setting up pool1
I0429 17:11:03.552429  1719 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:03.552433  1719 net.cpp:137] Memory required for data: 7014800
I0429 17:11:03.552436  1719 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:03.552445  1719 net.cpp:84] Creating Layer conv2
I0429 17:11:03.552449  1719 net.cpp:406] conv2 <- pool1
I0429 17:11:03.552455  1719 net.cpp:380] conv2 -> conv2
I0429 17:11:03.552640  1719 net.cpp:122] Setting up conv2
I0429 17:11:03.552647  1719 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:03.552650  1719 net.cpp:137] Memory required for data: 8294800
I0429 17:11:03.552657  1719 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:03.552664  1719 net.cpp:84] Creating Layer pool2
I0429 17:11:03.552666  1719 net.cpp:406] pool2 <- conv2
I0429 17:11:03.552672  1719 net.cpp:380] pool2 -> pool2
I0429 17:11:03.552680  1719 net.cpp:122] Setting up pool2
I0429 17:11:03.552683  1719 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:03.552687  1719 net.cpp:137] Memory required for data: 8614800
I0429 17:11:03.552690  1719 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:03.552697  1719 net.cpp:84] Creating Layer ip1
I0429 17:11:03.552700  1719 net.cpp:406] ip1 <- pool2
I0429 17:11:03.552707  1719 net.cpp:380] ip1 -> ip1
I0429 17:11:03.555423  1719 net.cpp:122] Setting up ip1
I0429 17:11:03.555428  1719 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:03.555431  1719 net.cpp:137] Memory required for data: 8814800
I0429 17:11:03.555438  1719 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:03.555444  1719 net.cpp:84] Creating Layer relu1
I0429 17:11:03.555446  1719 net.cpp:406] relu1 <- ip1
I0429 17:11:03.555452  1719 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:03.555459  1719 net.cpp:122] Setting up relu1
I0429 17:11:03.555462  1719 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:03.555465  1719 net.cpp:137] Memory required for data: 9014800
I0429 17:11:03.555469  1719 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:03.555476  1719 net.cpp:84] Creating Layer ip2
I0429 17:11:03.555480  1719 net.cpp:406] ip2 <- ip1
I0429 17:11:03.555485  1719 net.cpp:380] ip2 -> ip2
I0429 17:11:03.555529  1719 net.cpp:122] Setting up ip2
I0429 17:11:03.555533  1719 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:03.555537  1719 net.cpp:137] Memory required for data: 9018800
I0429 17:11:03.555542  1719 layer_factory.hpp:77] Creating layer feat
I0429 17:11:03.555547  1719 net.cpp:84] Creating Layer feat
I0429 17:11:03.555550  1719 net.cpp:406] feat <- ip2
I0429 17:11:03.555557  1719 net.cpp:380] feat -> feat
I0429 17:11:03.555565  1719 net.cpp:122] Setting up feat
I0429 17:11:03.555569  1719 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:03.555572  1719 net.cpp:137] Memory required for data: 9019600
I0429 17:11:03.555579  1719 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:03.555588  1719 net.cpp:84] Creating Layer conv1_p
I0429 17:11:03.555591  1719 net.cpp:406] conv1_p <- data_p
I0429 17:11:03.555596  1719 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:03.555618  1719 net.cpp:122] Setting up conv1_p
I0429 17:11:03.555624  1719 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:03.555627  1719 net.cpp:137] Memory required for data: 13627600
I0429 17:11:03.555631  1719 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:03.555635  1719 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:03.555639  1719 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:03.555644  1719 net.cpp:84] Creating Layer pool1_p
I0429 17:11:03.555646  1719 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:03.555650  1719 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:03.555660  1719 net.cpp:122] Setting up pool1_p
I0429 17:11:03.555663  1719 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:03.555667  1719 net.cpp:137] Memory required for data: 14779600
I0429 17:11:03.555670  1719 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:03.555681  1719 net.cpp:84] Creating Layer conv2_p
I0429 17:11:03.555685  1719 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:03.555691  1719 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:03.555876  1719 net.cpp:122] Setting up conv2_p
I0429 17:11:03.555882  1719 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:03.555886  1719 net.cpp:137] Memory required for data: 16059600
I0429 17:11:03.555888  1719 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:03.555893  1719 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:03.555896  1719 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:03.555902  1719 net.cpp:84] Creating Layer pool2_p
I0429 17:11:03.555904  1719 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:03.555910  1719 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:03.555917  1719 net.cpp:122] Setting up pool2_p
I0429 17:11:03.555922  1719 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:03.555924  1719 net.cpp:137] Memory required for data: 16379600
I0429 17:11:03.555928  1719 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:03.555934  1719 net.cpp:84] Creating Layer ip1_p
I0429 17:11:03.555938  1719 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:03.555943  1719 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:03.558699  1719 net.cpp:122] Setting up ip1_p
I0429 17:11:03.558708  1719 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:03.558712  1719 net.cpp:137] Memory required for data: 16579600
I0429 17:11:03.558715  1719 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:03.558718  1719 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:03.558722  1719 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:03.558727  1719 net.cpp:84] Creating Layer relu1_p
I0429 17:11:03.558730  1719 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:03.558737  1719 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:03.558743  1719 net.cpp:122] Setting up relu1_p
I0429 17:11:03.558748  1719 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:03.558749  1719 net.cpp:137] Memory required for data: 16779600
I0429 17:11:03.558753  1719 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:03.558761  1719 net.cpp:84] Creating Layer ip2_p
I0429 17:11:03.558764  1719 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:03.558769  1719 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:03.558815  1719 net.cpp:122] Setting up ip2_p
I0429 17:11:03.558820  1719 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:03.558823  1719 net.cpp:137] Memory required for data: 16783600
I0429 17:11:03.558828  1719 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:03.558832  1719 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:03.558835  1719 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:03.558841  1719 net.cpp:84] Creating Layer feat_p
I0429 17:11:03.558845  1719 net.cpp:406] feat_p <- ip2_p
I0429 17:11:03.558850  1719 net.cpp:380] feat_p -> feat_p
I0429 17:11:03.558859  1719 net.cpp:122] Setting up feat_p
I0429 17:11:03.558863  1719 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:03.558866  1719 net.cpp:137] Memory required for data: 16784400
I0429 17:11:03.558869  1719 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:03.558873  1719 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:03.558876  1719 layer_factory.hpp:77] Creating layer loss
I0429 17:11:03.558882  1719 net.cpp:84] Creating Layer loss
I0429 17:11:03.558887  1719 net.cpp:406] loss <- feat
I0429 17:11:03.558890  1719 net.cpp:406] loss <- feat_p
I0429 17:11:03.558894  1719 net.cpp:406] loss <- sim
I0429 17:11:03.558898  1719 net.cpp:380] loss -> loss
I0429 17:11:03.558907  1719 net.cpp:122] Setting up loss
I0429 17:11:03.558912  1719 net.cpp:129] Top shape: (1)
I0429 17:11:03.558914  1719 net.cpp:132]     with loss weight 1
I0429 17:11:03.558923  1719 net.cpp:137] Memory required for data: 16784404
I0429 17:11:03.558929  1719 net.cpp:198] loss needs backward computation.
I0429 17:11:03.558934  1719 net.cpp:198] feat_p needs backward computation.
I0429 17:11:03.558938  1719 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:03.558941  1719 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:03.558944  1719 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:03.558948  1719 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:03.558950  1719 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:03.558954  1719 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:03.558957  1719 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:03.558961  1719 net.cpp:198] feat needs backward computation.
I0429 17:11:03.558964  1719 net.cpp:198] ip2 needs backward computation.
I0429 17:11:03.558967  1719 net.cpp:198] relu1 needs backward computation.
I0429 17:11:03.558970  1719 net.cpp:198] ip1 needs backward computation.
I0429 17:11:03.558974  1719 net.cpp:198] pool2 needs backward computation.
I0429 17:11:03.558977  1719 net.cpp:198] conv2 needs backward computation.
I0429 17:11:03.558980  1719 net.cpp:198] pool1 needs backward computation.
I0429 17:11:03.558984  1719 net.cpp:198] conv1 needs backward computation.
I0429 17:11:03.558990  1719 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:03.558993  1719 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:03.558996  1719 net.cpp:242] This network produces output loss
I0429 17:11:03.559079  1719 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.

 Performance counter stats for 'python test_model.py examples/images/cat_gray.jpg' (5 runs):

     1,882,090,406      cycles:u                                                      ( +-  1.05% )
     2,421,157,562      instructions:u            #    1.29  insns per cycle          ( +-  0.18% )

       0.740846410 seconds time elapsed                                          ( +-  3.39% )

WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:04.333374  1750 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:04.333400  1750 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:04.333402  1750 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:04.334918  1750 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:04.335067  1750 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:04.335137  1750 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:04.336084  1750 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:04.336380  1750 net.cpp:84] Creating Layer pair_data
I0429 17:11:04.336393  1750 net.cpp:380] pair_data -> pair_data
I0429 17:11:04.336410  1750 net.cpp:380] pair_data -> sim
I0429 17:11:04.336439  1750 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:04.337224  1750 net.cpp:122] Setting up pair_data
I0429 17:11:04.337237  1750 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:04.337245  1750 net.cpp:129] Top shape: 100 (100)
I0429 17:11:04.337249  1750 net.cpp:137] Memory required for data: 627600
I0429 17:11:04.337255  1750 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:04.337266  1750 net.cpp:84] Creating Layer slice_pair
I0429 17:11:04.337272  1750 net.cpp:406] slice_pair <- pair_data
I0429 17:11:04.337280  1750 net.cpp:380] slice_pair -> data
I0429 17:11:04.337291  1750 net.cpp:380] slice_pair -> data_p
I0429 17:11:04.337304  1750 net.cpp:122] Setting up slice_pair
I0429 17:11:04.337312  1750 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:04.337319  1750 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:04.337324  1750 net.cpp:137] Memory required for data: 1254800
I0429 17:11:04.337329  1750 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:04.337343  1750 net.cpp:84] Creating Layer conv1
I0429 17:11:04.337349  1750 net.cpp:406] conv1 <- data
I0429 17:11:04.337357  1750 net.cpp:380] conv1 -> conv1
I0429 17:11:04.337401  1750 net.cpp:122] Setting up conv1
I0429 17:11:04.337410  1750 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:04.337415  1750 net.cpp:137] Memory required for data: 5862800
I0429 17:11:04.337426  1750 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:04.337437  1750 net.cpp:84] Creating Layer pool1
I0429 17:11:04.337443  1750 net.cpp:406] pool1 <- conv1
I0429 17:11:04.337450  1750 net.cpp:380] pool1 -> pool1
I0429 17:11:04.337463  1750 net.cpp:122] Setting up pool1
I0429 17:11:04.337471  1750 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:04.337476  1750 net.cpp:137] Memory required for data: 7014800
I0429 17:11:04.337479  1750 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:04.337496  1750 net.cpp:84] Creating Layer conv2
I0429 17:11:04.337502  1750 net.cpp:406] conv2 <- pool1
I0429 17:11:04.337510  1750 net.cpp:380] conv2 -> conv2
I0429 17:11:04.337801  1750 net.cpp:122] Setting up conv2
I0429 17:11:04.337810  1750 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:04.337815  1750 net.cpp:137] Memory required for data: 8294800
I0429 17:11:04.337824  1750 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:04.337833  1750 net.cpp:84] Creating Layer pool2
I0429 17:11:04.337838  1750 net.cpp:406] pool2 <- conv2
I0429 17:11:04.337846  1750 net.cpp:380] pool2 -> pool2
I0429 17:11:04.337857  1750 net.cpp:122] Setting up pool2
I0429 17:11:04.337863  1750 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:04.337868  1750 net.cpp:137] Memory required for data: 8614800
I0429 17:11:04.337872  1750 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:04.337882  1750 net.cpp:84] Creating Layer ip1
I0429 17:11:04.337888  1750 net.cpp:406] ip1 <- pool2
I0429 17:11:04.337896  1750 net.cpp:380] ip1 -> ip1
I0429 17:11:04.342134  1750 net.cpp:122] Setting up ip1
I0429 17:11:04.342147  1750 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:04.342151  1750 net.cpp:137] Memory required for data: 8814800
I0429 17:11:04.342165  1750 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:04.342173  1750 net.cpp:84] Creating Layer relu1
I0429 17:11:04.342180  1750 net.cpp:406] relu1 <- ip1
I0429 17:11:04.342187  1750 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:04.342196  1750 net.cpp:122] Setting up relu1
I0429 17:11:04.342202  1750 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:04.342207  1750 net.cpp:137] Memory required for data: 9014800
I0429 17:11:04.342212  1750 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:04.342221  1750 net.cpp:84] Creating Layer ip2
I0429 17:11:04.342226  1750 net.cpp:406] ip2 <- ip1
I0429 17:11:04.342236  1750 net.cpp:380] ip2 -> ip2
I0429 17:11:04.342305  1750 net.cpp:122] Setting up ip2
I0429 17:11:04.342313  1750 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:04.342317  1750 net.cpp:137] Memory required for data: 9018800
I0429 17:11:04.342330  1750 layer_factory.hpp:77] Creating layer feat
I0429 17:11:04.342339  1750 net.cpp:84] Creating Layer feat
I0429 17:11:04.342344  1750 net.cpp:406] feat <- ip2
I0429 17:11:04.342356  1750 net.cpp:380] feat -> feat
I0429 17:11:04.342371  1750 net.cpp:122] Setting up feat
I0429 17:11:04.342377  1750 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:04.342381  1750 net.cpp:137] Memory required for data: 9019600
I0429 17:11:04.342391  1750 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:04.342403  1750 net.cpp:84] Creating Layer conv1_p
I0429 17:11:04.342408  1750 net.cpp:406] conv1_p <- data_p
I0429 17:11:04.342417  1750 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:04.342449  1750 net.cpp:122] Setting up conv1_p
I0429 17:11:04.342458  1750 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:04.342463  1750 net.cpp:137] Memory required for data: 13627600
I0429 17:11:04.342468  1750 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:04.342473  1750 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:04.342479  1750 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:04.342488  1750 net.cpp:84] Creating Layer pool1_p
I0429 17:11:04.342492  1750 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:04.342499  1750 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:04.342511  1750 net.cpp:122] Setting up pool1_p
I0429 17:11:04.342517  1750 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:04.342522  1750 net.cpp:137] Memory required for data: 14779600
I0429 17:11:04.342526  1750 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:04.342540  1750 net.cpp:84] Creating Layer conv2_p
I0429 17:11:04.342545  1750 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:04.342553  1750 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:04.342862  1750 net.cpp:122] Setting up conv2_p
I0429 17:11:04.342872  1750 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:04.342877  1750 net.cpp:137] Memory required for data: 16059600
I0429 17:11:04.342882  1750 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:04.342901  1750 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:04.342907  1750 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:04.342916  1750 net.cpp:84] Creating Layer pool2_p
I0429 17:11:04.342921  1750 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:04.342929  1750 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:04.342941  1750 net.cpp:122] Setting up pool2_p
I0429 17:11:04.342947  1750 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:04.342952  1750 net.cpp:137] Memory required for data: 16379600
I0429 17:11:04.342955  1750 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:04.342964  1750 net.cpp:84] Creating Layer ip1_p
I0429 17:11:04.342968  1750 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:04.342978  1750 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:04.347113  1750 net.cpp:122] Setting up ip1_p
I0429 17:11:04.347123  1750 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:04.347128  1750 net.cpp:137] Memory required for data: 16579600
I0429 17:11:04.347133  1750 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:04.347139  1750 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:04.347144  1750 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:04.347153  1750 net.cpp:84] Creating Layer relu1_p
I0429 17:11:04.347158  1750 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:04.347164  1750 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:04.347174  1750 net.cpp:122] Setting up relu1_p
I0429 17:11:04.347180  1750 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:04.347184  1750 net.cpp:137] Memory required for data: 16779600
I0429 17:11:04.347188  1750 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:04.347200  1750 net.cpp:84] Creating Layer ip2_p
I0429 17:11:04.347205  1750 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:04.347213  1750 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:04.347290  1750 net.cpp:122] Setting up ip2_p
I0429 17:11:04.347297  1750 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:04.347301  1750 net.cpp:137] Memory required for data: 16783600
I0429 17:11:04.347309  1750 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:04.347316  1750 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:04.347321  1750 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:04.347329  1750 net.cpp:84] Creating Layer feat_p
I0429 17:11:04.347334  1750 net.cpp:406] feat_p <- ip2_p
I0429 17:11:04.347342  1750 net.cpp:380] feat_p -> feat_p
I0429 17:11:04.347357  1750 net.cpp:122] Setting up feat_p
I0429 17:11:04.347362  1750 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:04.347367  1750 net.cpp:137] Memory required for data: 16784400
I0429 17:11:04.347371  1750 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:04.347378  1750 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:04.347383  1750 layer_factory.hpp:77] Creating layer loss
I0429 17:11:04.347391  1750 net.cpp:84] Creating Layer loss
I0429 17:11:04.347395  1750 net.cpp:406] loss <- feat
I0429 17:11:04.347403  1750 net.cpp:406] loss <- feat_p
I0429 17:11:04.347407  1750 net.cpp:406] loss <- sim
I0429 17:11:04.347415  1750 net.cpp:380] loss -> loss
I0429 17:11:04.347429  1750 net.cpp:122] Setting up loss
I0429 17:11:04.347435  1750 net.cpp:129] Top shape: (1)
I0429 17:11:04.347440  1750 net.cpp:132]     with loss weight 1
I0429 17:11:04.347450  1750 net.cpp:137] Memory required for data: 16784404
I0429 17:11:04.347455  1750 net.cpp:198] loss needs backward computation.
I0429 17:11:04.347461  1750 net.cpp:198] feat_p needs backward computation.
I0429 17:11:04.347466  1750 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:04.347471  1750 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:04.347476  1750 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:04.347481  1750 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:04.347486  1750 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:04.347491  1750 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:04.347496  1750 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:04.347502  1750 net.cpp:198] feat needs backward computation.
I0429 17:11:04.347507  1750 net.cpp:198] ip2 needs backward computation.
I0429 17:11:04.347512  1750 net.cpp:198] relu1 needs backward computation.
I0429 17:11:04.347517  1750 net.cpp:198] ip1 needs backward computation.
I0429 17:11:04.347523  1750 net.cpp:198] pool2 needs backward computation.
I0429 17:11:04.347527  1750 net.cpp:198] conv2 needs backward computation.
I0429 17:11:04.347532  1750 net.cpp:198] pool1 needs backward computation.
I0429 17:11:04.347537  1750 net.cpp:198] conv1 needs backward computation.
I0429 17:11:04.347545  1750 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:04.347553  1750 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:04.347556  1750 net.cpp:242] This network produces output loss
I0429 17:11:04.347676  1750 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:05.068182  1778 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:05.068208  1778 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:05.068212  1778 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:05.069645  1778 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:05.069797  1778 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:05.069861  1778 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:05.070878  1778 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:05.071215  1778 net.cpp:84] Creating Layer pair_data
I0429 17:11:05.071233  1778 net.cpp:380] pair_data -> pair_data
I0429 17:11:05.071254  1778 net.cpp:380] pair_data -> sim
I0429 17:11:05.071293  1778 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:05.072259  1778 net.cpp:122] Setting up pair_data
I0429 17:11:05.072278  1778 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:05.072288  1778 net.cpp:129] Top shape: 100 (100)
I0429 17:11:05.072293  1778 net.cpp:137] Memory required for data: 627600
I0429 17:11:05.072300  1778 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:05.072314  1778 net.cpp:84] Creating Layer slice_pair
I0429 17:11:05.072324  1778 net.cpp:406] slice_pair <- pair_data
I0429 17:11:05.072335  1778 net.cpp:380] slice_pair -> data
I0429 17:11:05.072347  1778 net.cpp:380] slice_pair -> data_p
I0429 17:11:05.072365  1778 net.cpp:122] Setting up slice_pair
I0429 17:11:05.072374  1778 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:05.072381  1778 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:05.072387  1778 net.cpp:137] Memory required for data: 1254800
I0429 17:11:05.072393  1778 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:05.072412  1778 net.cpp:84] Creating Layer conv1
I0429 17:11:05.072417  1778 net.cpp:406] conv1 <- data
I0429 17:11:05.072427  1778 net.cpp:380] conv1 -> conv1
I0429 17:11:05.072486  1778 net.cpp:122] Setting up conv1
I0429 17:11:05.072499  1778 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:05.072518  1778 net.cpp:137] Memory required for data: 5862800
I0429 17:11:05.072531  1778 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:05.072542  1778 net.cpp:84] Creating Layer pool1
I0429 17:11:05.072549  1778 net.cpp:406] pool1 <- conv1
I0429 17:11:05.072557  1778 net.cpp:380] pool1 -> pool1
I0429 17:11:05.072571  1778 net.cpp:122] Setting up pool1
I0429 17:11:05.072578  1778 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:05.072583  1778 net.cpp:137] Memory required for data: 7014800
I0429 17:11:05.072589  1778 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:05.072607  1778 net.cpp:84] Creating Layer conv2
I0429 17:11:05.072613  1778 net.cpp:406] conv2 <- pool1
I0429 17:11:05.072623  1778 net.cpp:380] conv2 -> conv2
I0429 17:11:05.072945  1778 net.cpp:122] Setting up conv2
I0429 17:11:05.072955  1778 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:05.072960  1778 net.cpp:137] Memory required for data: 8294800
I0429 17:11:05.072971  1778 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:05.072981  1778 net.cpp:84] Creating Layer pool2
I0429 17:11:05.072988  1778 net.cpp:406] pool2 <- conv2
I0429 17:11:05.072996  1778 net.cpp:380] pool2 -> pool2
I0429 17:11:05.073009  1778 net.cpp:122] Setting up pool2
I0429 17:11:05.073016  1778 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:05.073021  1778 net.cpp:137] Memory required for data: 8614800
I0429 17:11:05.073026  1778 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:05.073045  1778 net.cpp:84] Creating Layer ip1
I0429 17:11:05.073050  1778 net.cpp:406] ip1 <- pool2
I0429 17:11:05.073061  1778 net.cpp:380] ip1 -> ip1
I0429 17:11:05.077818  1778 net.cpp:122] Setting up ip1
I0429 17:11:05.077831  1778 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:05.077836  1778 net.cpp:137] Memory required for data: 8814800
I0429 17:11:05.077852  1778 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:05.077862  1778 net.cpp:84] Creating Layer relu1
I0429 17:11:05.077867  1778 net.cpp:406] relu1 <- ip1
I0429 17:11:05.077877  1778 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:05.077886  1778 net.cpp:122] Setting up relu1
I0429 17:11:05.077893  1778 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:05.077898  1778 net.cpp:137] Memory required for data: 9014800
I0429 17:11:05.077905  1778 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:05.077915  1778 net.cpp:84] Creating Layer ip2
I0429 17:11:05.077920  1778 net.cpp:406] ip2 <- ip1
I0429 17:11:05.077931  1778 net.cpp:380] ip2 -> ip2
I0429 17:11:05.078011  1778 net.cpp:122] Setting up ip2
I0429 17:11:05.078017  1778 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:05.078022  1778 net.cpp:137] Memory required for data: 9018800
I0429 17:11:05.078032  1778 layer_factory.hpp:77] Creating layer feat
I0429 17:11:05.078039  1778 net.cpp:84] Creating Layer feat
I0429 17:11:05.078044  1778 net.cpp:406] feat <- ip2
I0429 17:11:05.078054  1778 net.cpp:380] feat -> feat
I0429 17:11:05.078071  1778 net.cpp:122] Setting up feat
I0429 17:11:05.078078  1778 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:05.078083  1778 net.cpp:137] Memory required for data: 9019600
I0429 17:11:05.078094  1778 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:05.078109  1778 net.cpp:84] Creating Layer conv1_p
I0429 17:11:05.078114  1778 net.cpp:406] conv1_p <- data_p
I0429 17:11:05.078124  1778 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:05.078161  1778 net.cpp:122] Setting up conv1_p
I0429 17:11:05.078169  1778 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:05.078174  1778 net.cpp:137] Memory required for data: 13627600
I0429 17:11:05.078181  1778 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:05.078187  1778 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:05.078193  1778 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:05.078203  1778 net.cpp:84] Creating Layer pool1_p
I0429 17:11:05.078209  1778 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:05.078217  1778 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:05.078230  1778 net.cpp:122] Setting up pool1_p
I0429 17:11:05.078238  1778 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:05.078243  1778 net.cpp:137] Memory required for data: 14779600
I0429 17:11:05.078248  1778 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:05.078263  1778 net.cpp:84] Creating Layer conv2_p
I0429 17:11:05.078269  1778 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:05.078277  1778 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:05.078599  1778 net.cpp:122] Setting up conv2_p
I0429 17:11:05.078609  1778 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:05.078615  1778 net.cpp:137] Memory required for data: 16059600
I0429 17:11:05.078621  1778 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:05.078627  1778 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:05.078634  1778 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:05.078646  1778 net.cpp:84] Creating Layer pool2_p
I0429 17:11:05.078651  1778 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:05.078661  1778 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:05.078675  1778 net.cpp:122] Setting up pool2_p
I0429 17:11:05.078681  1778 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:05.078686  1778 net.cpp:137] Memory required for data: 16379600
I0429 17:11:05.078691  1778 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:05.078701  1778 net.cpp:84] Creating Layer ip1_p
I0429 17:11:05.078711  1778 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:05.078722  1778 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:05.083431  1778 net.cpp:122] Setting up ip1_p
I0429 17:11:05.083442  1778 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:05.083446  1778 net.cpp:137] Memory required for data: 16579600
I0429 17:11:05.083452  1778 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:05.083459  1778 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:05.083465  1778 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:05.083474  1778 net.cpp:84] Creating Layer relu1_p
I0429 17:11:05.083480  1778 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:05.083487  1778 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:05.083498  1778 net.cpp:122] Setting up relu1_p
I0429 17:11:05.083505  1778 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:05.083509  1778 net.cpp:137] Memory required for data: 16779600
I0429 17:11:05.083514  1778 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:05.083528  1778 net.cpp:84] Creating Layer ip2_p
I0429 17:11:05.083533  1778 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:05.083541  1778 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:05.083618  1778 net.cpp:122] Setting up ip2_p
I0429 17:11:05.083627  1778 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:05.083632  1778 net.cpp:137] Memory required for data: 16783600
I0429 17:11:05.083640  1778 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:05.083647  1778 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:05.083653  1778 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:05.083665  1778 net.cpp:84] Creating Layer feat_p
I0429 17:11:05.083670  1778 net.cpp:406] feat_p <- ip2_p
I0429 17:11:05.083679  1778 net.cpp:380] feat_p -> feat_p
I0429 17:11:05.083694  1778 net.cpp:122] Setting up feat_p
I0429 17:11:05.083701  1778 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:05.083706  1778 net.cpp:137] Memory required for data: 16784400
I0429 17:11:05.083711  1778 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:05.083719  1778 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:05.083724  1778 layer_factory.hpp:77] Creating layer loss
I0429 17:11:05.083734  1778 net.cpp:84] Creating Layer loss
I0429 17:11:05.083739  1778 net.cpp:406] loss <- feat
I0429 17:11:05.083746  1778 net.cpp:406] loss <- feat_p
I0429 17:11:05.083752  1778 net.cpp:406] loss <- sim
I0429 17:11:05.083760  1778 net.cpp:380] loss -> loss
I0429 17:11:05.083781  1778 net.cpp:122] Setting up loss
I0429 17:11:05.083789  1778 net.cpp:129] Top shape: (1)
I0429 17:11:05.083794  1778 net.cpp:132]     with loss weight 1
I0429 17:11:05.083806  1778 net.cpp:137] Memory required for data: 16784404
I0429 17:11:05.083811  1778 net.cpp:198] loss needs backward computation.
I0429 17:11:05.083818  1778 net.cpp:198] feat_p needs backward computation.
I0429 17:11:05.083824  1778 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:05.083830  1778 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:05.083835  1778 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:05.083842  1778 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:05.083847  1778 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:05.083853  1778 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:05.083858  1778 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:05.083864  1778 net.cpp:198] feat needs backward computation.
I0429 17:11:05.083869  1778 net.cpp:198] ip2 needs backward computation.
I0429 17:11:05.083875  1778 net.cpp:198] relu1 needs backward computation.
I0429 17:11:05.083880  1778 net.cpp:198] ip1 needs backward computation.
I0429 17:11:05.083886  1778 net.cpp:198] pool2 needs backward computation.
I0429 17:11:05.083892  1778 net.cpp:198] conv2 needs backward computation.
I0429 17:11:05.083897  1778 net.cpp:198] pool1 needs backward computation.
I0429 17:11:05.083907  1778 net.cpp:198] conv1 needs backward computation.
I0429 17:11:05.083919  1778 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:05.083925  1778 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:05.083930  1778 net.cpp:242] This network produces output loss
I0429 17:11:05.084081  1778 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:06.103932  1806 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:06.103963  1806 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:06.103968  1806 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:06.106178  1806 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:06.106416  1806 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:06.106513  1806 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:06.107679  1806 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:06.107997  1806 net.cpp:84] Creating Layer pair_data
I0429 17:11:06.108014  1806 net.cpp:380] pair_data -> pair_data
I0429 17:11:06.108032  1806 net.cpp:380] pair_data -> sim
I0429 17:11:06.108105  1806 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:06.108958  1806 net.cpp:122] Setting up pair_data
I0429 17:11:06.108983  1806 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:06.108991  1806 net.cpp:129] Top shape: 100 (100)
I0429 17:11:06.108996  1806 net.cpp:137] Memory required for data: 627600
I0429 17:11:06.109002  1806 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:06.109014  1806 net.cpp:84] Creating Layer slice_pair
I0429 17:11:06.109020  1806 net.cpp:406] slice_pair <- pair_data
I0429 17:11:06.109030  1806 net.cpp:380] slice_pair -> data
I0429 17:11:06.109041  1806 net.cpp:380] slice_pair -> data_p
I0429 17:11:06.109055  1806 net.cpp:122] Setting up slice_pair
I0429 17:11:06.109063  1806 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:06.109071  1806 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:06.109076  1806 net.cpp:137] Memory required for data: 1254800
I0429 17:11:06.109081  1806 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:06.109097  1806 net.cpp:84] Creating Layer conv1
I0429 17:11:06.109102  1806 net.cpp:406] conv1 <- data
I0429 17:11:06.109110  1806 net.cpp:380] conv1 -> conv1
I0429 17:11:06.109163  1806 net.cpp:122] Setting up conv1
I0429 17:11:06.109174  1806 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:06.109177  1806 net.cpp:137] Memory required for data: 5862800
I0429 17:11:06.109190  1806 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:06.109200  1806 net.cpp:84] Creating Layer pool1
I0429 17:11:06.109206  1806 net.cpp:406] pool1 <- conv1
I0429 17:11:06.109222  1806 net.cpp:380] pool1 -> pool1
I0429 17:11:06.109237  1806 net.cpp:122] Setting up pool1
I0429 17:11:06.109246  1806 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:06.109251  1806 net.cpp:137] Memory required for data: 7014800
I0429 17:11:06.109256  1806 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:06.109271  1806 net.cpp:84] Creating Layer conv2
I0429 17:11:06.109277  1806 net.cpp:406] conv2 <- pool1
I0429 17:11:06.109287  1806 net.cpp:380] conv2 -> conv2
I0429 17:11:06.109593  1806 net.cpp:122] Setting up conv2
I0429 17:11:06.109603  1806 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:06.109607  1806 net.cpp:137] Memory required for data: 8294800
I0429 17:11:06.109618  1806 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:06.109628  1806 net.cpp:84] Creating Layer pool2
I0429 17:11:06.109634  1806 net.cpp:406] pool2 <- conv2
I0429 17:11:06.109644  1806 net.cpp:380] pool2 -> pool2
I0429 17:11:06.109655  1806 net.cpp:122] Setting up pool2
I0429 17:11:06.109663  1806 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:06.109668  1806 net.cpp:137] Memory required for data: 8614800
I0429 17:11:06.109673  1806 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:06.109683  1806 net.cpp:84] Creating Layer ip1
I0429 17:11:06.109688  1806 net.cpp:406] ip1 <- pool2
I0429 17:11:06.109707  1806 net.cpp:380] ip1 -> ip1
I0429 17:11:06.114231  1806 net.cpp:122] Setting up ip1
I0429 17:11:06.114246  1806 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:06.114250  1806 net.cpp:137] Memory required for data: 8814800
I0429 17:11:06.114265  1806 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:06.114274  1806 net.cpp:84] Creating Layer relu1
I0429 17:11:06.114280  1806 net.cpp:406] relu1 <- ip1
I0429 17:11:06.114289  1806 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:06.114300  1806 net.cpp:122] Setting up relu1
I0429 17:11:06.114305  1806 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:06.114310  1806 net.cpp:137] Memory required for data: 9014800
I0429 17:11:06.114315  1806 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:06.114325  1806 net.cpp:84] Creating Layer ip2
I0429 17:11:06.114329  1806 net.cpp:406] ip2 <- ip1
I0429 17:11:06.114341  1806 net.cpp:380] ip2 -> ip2
I0429 17:11:06.114413  1806 net.cpp:122] Setting up ip2
I0429 17:11:06.114419  1806 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:06.114424  1806 net.cpp:137] Memory required for data: 9018800
I0429 17:11:06.114434  1806 layer_factory.hpp:77] Creating layer feat
I0429 17:11:06.114440  1806 net.cpp:84] Creating Layer feat
I0429 17:11:06.114445  1806 net.cpp:406] feat <- ip2
I0429 17:11:06.114455  1806 net.cpp:380] feat -> feat
I0429 17:11:06.114470  1806 net.cpp:122] Setting up feat
I0429 17:11:06.114477  1806 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:06.114481  1806 net.cpp:137] Memory required for data: 9019600
I0429 17:11:06.114491  1806 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:06.114506  1806 net.cpp:84] Creating Layer conv1_p
I0429 17:11:06.114511  1806 net.cpp:406] conv1_p <- data_p
I0429 17:11:06.114519  1806 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:06.114554  1806 net.cpp:122] Setting up conv1_p
I0429 17:11:06.114564  1806 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:06.114569  1806 net.cpp:137] Memory required for data: 13627600
I0429 17:11:06.114575  1806 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:06.114583  1806 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:06.114589  1806 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:06.114598  1806 net.cpp:84] Creating Layer pool1_p
I0429 17:11:06.114603  1806 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:06.114610  1806 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:06.114622  1806 net.cpp:122] Setting up pool1_p
I0429 17:11:06.114630  1806 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:06.114635  1806 net.cpp:137] Memory required for data: 14779600
I0429 17:11:06.114645  1806 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:06.114661  1806 net.cpp:84] Creating Layer conv2_p
I0429 17:11:06.114667  1806 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:06.114676  1806 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:06.114975  1806 net.cpp:122] Setting up conv2_p
I0429 17:11:06.114985  1806 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:06.114990  1806 net.cpp:137] Memory required for data: 16059600
I0429 17:11:06.114996  1806 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:06.115002  1806 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:06.115008  1806 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:06.115017  1806 net.cpp:84] Creating Layer pool2_p
I0429 17:11:06.115022  1806 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:06.115032  1806 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:06.115043  1806 net.cpp:122] Setting up pool2_p
I0429 17:11:06.115051  1806 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:06.115056  1806 net.cpp:137] Memory required for data: 16379600
I0429 17:11:06.115061  1806 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:06.115068  1806 net.cpp:84] Creating Layer ip1_p
I0429 17:11:06.115073  1806 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:06.115083  1806 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:06.119539  1806 net.cpp:122] Setting up ip1_p
I0429 17:11:06.119550  1806 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:06.119555  1806 net.cpp:137] Memory required for data: 16579600
I0429 17:11:06.119560  1806 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:06.119566  1806 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:06.119572  1806 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:06.119582  1806 net.cpp:84] Creating Layer relu1_p
I0429 17:11:06.119587  1806 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:06.119595  1806 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:06.119606  1806 net.cpp:122] Setting up relu1_p
I0429 17:11:06.119611  1806 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:06.119616  1806 net.cpp:137] Memory required for data: 16779600
I0429 17:11:06.119621  1806 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:06.119633  1806 net.cpp:84] Creating Layer ip2_p
I0429 17:11:06.119638  1806 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:06.119647  1806 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:06.119719  1806 net.cpp:122] Setting up ip2_p
I0429 17:11:06.119727  1806 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:06.119732  1806 net.cpp:137] Memory required for data: 16783600
I0429 17:11:06.119740  1806 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:06.119746  1806 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:06.119752  1806 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:06.119761  1806 net.cpp:84] Creating Layer feat_p
I0429 17:11:06.119766  1806 net.cpp:406] feat_p <- ip2_p
I0429 17:11:06.119774  1806 net.cpp:380] feat_p -> feat_p
I0429 17:11:06.119789  1806 net.cpp:122] Setting up feat_p
I0429 17:11:06.119796  1806 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:06.119801  1806 net.cpp:137] Memory required for data: 16784400
I0429 17:11:06.119806  1806 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:06.119812  1806 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:06.119817  1806 layer_factory.hpp:77] Creating layer loss
I0429 17:11:06.119827  1806 net.cpp:84] Creating Layer loss
I0429 17:11:06.119832  1806 net.cpp:406] loss <- feat
I0429 17:11:06.119839  1806 net.cpp:406] loss <- feat_p
I0429 17:11:06.119844  1806 net.cpp:406] loss <- sim
I0429 17:11:06.119853  1806 net.cpp:380] loss -> loss
I0429 17:11:06.119873  1806 net.cpp:122] Setting up loss
I0429 17:11:06.119880  1806 net.cpp:129] Top shape: (1)
I0429 17:11:06.119884  1806 net.cpp:132]     with loss weight 1
I0429 17:11:06.119901  1806 net.cpp:137] Memory required for data: 16784404
I0429 17:11:06.119906  1806 net.cpp:198] loss needs backward computation.
I0429 17:11:06.119913  1806 net.cpp:198] feat_p needs backward computation.
I0429 17:11:06.119920  1806 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:06.119925  1806 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:06.119930  1806 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:06.119935  1806 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:06.119940  1806 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:06.119946  1806 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:06.119951  1806 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:06.119956  1806 net.cpp:198] feat needs backward computation.
I0429 17:11:06.119962  1806 net.cpp:198] ip2 needs backward computation.
I0429 17:11:06.119967  1806 net.cpp:198] relu1 needs backward computation.
I0429 17:11:06.119971  1806 net.cpp:198] ip1 needs backward computation.
I0429 17:11:06.119977  1806 net.cpp:198] pool2 needs backward computation.
I0429 17:11:06.119983  1806 net.cpp:198] conv2 needs backward computation.
I0429 17:11:06.119988  1806 net.cpp:198] pool1 needs backward computation.
I0429 17:11:06.119993  1806 net.cpp:198] conv1 needs backward computation.
I0429 17:11:06.120002  1806 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:06.120009  1806 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:06.120014  1806 net.cpp:242] This network produces output loss
I0429 17:11:06.120168  1806 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:06.821136  1834 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:06.821158  1834 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:06.821161  1834 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:06.822561  1834 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:06.822711  1834 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:06.822777  1834 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:06.823799  1834 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:06.824167  1834 net.cpp:84] Creating Layer pair_data
I0429 17:11:06.824187  1834 net.cpp:380] pair_data -> pair_data
I0429 17:11:06.824208  1834 net.cpp:380] pair_data -> sim
I0429 17:11:06.824236  1834 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:06.825161  1834 net.cpp:122] Setting up pair_data
I0429 17:11:06.825187  1834 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:06.825196  1834 net.cpp:129] Top shape: 100 (100)
I0429 17:11:06.825201  1834 net.cpp:137] Memory required for data: 627600
I0429 17:11:06.825215  1834 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:06.825228  1834 net.cpp:84] Creating Layer slice_pair
I0429 17:11:06.825235  1834 net.cpp:406] slice_pair <- pair_data
I0429 17:11:06.825245  1834 net.cpp:380] slice_pair -> data
I0429 17:11:06.825258  1834 net.cpp:380] slice_pair -> data_p
I0429 17:11:06.825273  1834 net.cpp:122] Setting up slice_pair
I0429 17:11:06.825281  1834 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:06.825289  1834 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:06.825295  1834 net.cpp:137] Memory required for data: 1254800
I0429 17:11:06.825301  1834 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:06.825317  1834 net.cpp:84] Creating Layer conv1
I0429 17:11:06.825325  1834 net.cpp:406] conv1 <- data
I0429 17:11:06.825333  1834 net.cpp:380] conv1 -> conv1
I0429 17:11:06.825384  1834 net.cpp:122] Setting up conv1
I0429 17:11:06.825395  1834 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:06.825400  1834 net.cpp:137] Memory required for data: 5862800
I0429 17:11:06.825413  1834 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:06.825426  1834 net.cpp:84] Creating Layer pool1
I0429 17:11:06.825433  1834 net.cpp:406] pool1 <- conv1
I0429 17:11:06.825440  1834 net.cpp:380] pool1 -> pool1
I0429 17:11:06.825454  1834 net.cpp:122] Setting up pool1
I0429 17:11:06.825461  1834 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:06.825469  1834 net.cpp:137] Memory required for data: 7014800
I0429 17:11:06.825474  1834 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:06.825489  1834 net.cpp:84] Creating Layer conv2
I0429 17:11:06.825495  1834 net.cpp:406] conv2 <- pool1
I0429 17:11:06.825505  1834 net.cpp:380] conv2 -> conv2
I0429 17:11:06.825830  1834 net.cpp:122] Setting up conv2
I0429 17:11:06.825840  1834 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:06.825846  1834 net.cpp:137] Memory required for data: 8294800
I0429 17:11:06.825857  1834 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:06.825867  1834 net.cpp:84] Creating Layer pool2
I0429 17:11:06.825873  1834 net.cpp:406] pool2 <- conv2
I0429 17:11:06.825882  1834 net.cpp:380] pool2 -> pool2
I0429 17:11:06.825896  1834 net.cpp:122] Setting up pool2
I0429 17:11:06.825902  1834 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:06.825907  1834 net.cpp:137] Memory required for data: 8614800
I0429 17:11:06.825913  1834 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:06.825923  1834 net.cpp:84] Creating Layer ip1
I0429 17:11:06.825929  1834 net.cpp:406] ip1 <- pool2
I0429 17:11:06.825939  1834 net.cpp:380] ip1 -> ip1
I0429 17:11:06.830716  1834 net.cpp:122] Setting up ip1
I0429 17:11:06.830731  1834 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:06.830736  1834 net.cpp:137] Memory required for data: 8814800
I0429 17:11:06.830752  1834 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:06.830761  1834 net.cpp:84] Creating Layer relu1
I0429 17:11:06.830768  1834 net.cpp:406] relu1 <- ip1
I0429 17:11:06.830780  1834 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:06.830790  1834 net.cpp:122] Setting up relu1
I0429 17:11:06.830796  1834 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:06.830801  1834 net.cpp:137] Memory required for data: 9014800
I0429 17:11:06.830806  1834 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:06.830817  1834 net.cpp:84] Creating Layer ip2
I0429 17:11:06.830822  1834 net.cpp:406] ip2 <- ip1
I0429 17:11:06.830834  1834 net.cpp:380] ip2 -> ip2
I0429 17:11:06.830914  1834 net.cpp:122] Setting up ip2
I0429 17:11:06.830922  1834 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:06.830927  1834 net.cpp:137] Memory required for data: 9018800
I0429 17:11:06.830936  1834 layer_factory.hpp:77] Creating layer feat
I0429 17:11:06.830945  1834 net.cpp:84] Creating Layer feat
I0429 17:11:06.830950  1834 net.cpp:406] feat <- ip2
I0429 17:11:06.830961  1834 net.cpp:380] feat -> feat
I0429 17:11:06.830978  1834 net.cpp:122] Setting up feat
I0429 17:11:06.830986  1834 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:06.830996  1834 net.cpp:137] Memory required for data: 9019600
I0429 17:11:06.831007  1834 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:06.831022  1834 net.cpp:84] Creating Layer conv1_p
I0429 17:11:06.831028  1834 net.cpp:406] conv1_p <- data_p
I0429 17:11:06.831038  1834 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:06.831075  1834 net.cpp:122] Setting up conv1_p
I0429 17:11:06.831087  1834 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:06.831092  1834 net.cpp:137] Memory required for data: 13627600
I0429 17:11:06.831099  1834 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:06.831106  1834 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:06.831112  1834 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:06.831120  1834 net.cpp:84] Creating Layer pool1_p
I0429 17:11:06.831126  1834 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:06.831135  1834 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:06.831146  1834 net.cpp:122] Setting up pool1_p
I0429 17:11:06.831154  1834 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:06.831159  1834 net.cpp:137] Memory required for data: 14779600
I0429 17:11:06.831164  1834 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:06.831182  1834 net.cpp:84] Creating Layer conv2_p
I0429 17:11:06.831187  1834 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:06.831197  1834 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:06.831516  1834 net.cpp:122] Setting up conv2_p
I0429 17:11:06.831526  1834 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:06.831532  1834 net.cpp:137] Memory required for data: 16059600
I0429 17:11:06.831537  1834 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:06.831544  1834 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:06.831550  1834 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:06.831558  1834 net.cpp:84] Creating Layer pool2_p
I0429 17:11:06.831564  1834 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:06.831576  1834 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:06.831588  1834 net.cpp:122] Setting up pool2_p
I0429 17:11:06.831595  1834 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:06.831600  1834 net.cpp:137] Memory required for data: 16379600
I0429 17:11:06.831605  1834 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:06.831615  1834 net.cpp:84] Creating Layer ip1_p
I0429 17:11:06.831620  1834 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:06.831631  1834 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:06.836364  1834 net.cpp:122] Setting up ip1_p
I0429 17:11:06.836375  1834 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:06.836380  1834 net.cpp:137] Memory required for data: 16579600
I0429 17:11:06.836386  1834 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:06.836392  1834 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:06.836398  1834 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:06.836407  1834 net.cpp:84] Creating Layer relu1_p
I0429 17:11:06.836412  1834 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:06.836421  1834 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:06.836431  1834 net.cpp:122] Setting up relu1_p
I0429 17:11:06.836437  1834 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:06.836441  1834 net.cpp:137] Memory required for data: 16779600
I0429 17:11:06.836447  1834 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:06.836458  1834 net.cpp:84] Creating Layer ip2_p
I0429 17:11:06.836463  1834 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:06.836472  1834 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:06.836546  1834 net.cpp:122] Setting up ip2_p
I0429 17:11:06.836555  1834 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:06.836560  1834 net.cpp:137] Memory required for data: 16783600
I0429 17:11:06.836568  1834 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:06.836575  1834 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:06.836585  1834 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:06.836594  1834 net.cpp:84] Creating Layer feat_p
I0429 17:11:06.836601  1834 net.cpp:406] feat_p <- ip2_p
I0429 17:11:06.836607  1834 net.cpp:380] feat_p -> feat_p
I0429 17:11:06.836623  1834 net.cpp:122] Setting up feat_p
I0429 17:11:06.836630  1834 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:06.836634  1834 net.cpp:137] Memory required for data: 16784400
I0429 17:11:06.836640  1834 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:06.836647  1834 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:06.836652  1834 layer_factory.hpp:77] Creating layer loss
I0429 17:11:06.836661  1834 net.cpp:84] Creating Layer loss
I0429 17:11:06.836666  1834 net.cpp:406] loss <- feat
I0429 17:11:06.836673  1834 net.cpp:406] loss <- feat_p
I0429 17:11:06.836679  1834 net.cpp:406] loss <- sim
I0429 17:11:06.836688  1834 net.cpp:380] loss -> loss
I0429 17:11:06.836702  1834 net.cpp:122] Setting up loss
I0429 17:11:06.836709  1834 net.cpp:129] Top shape: (1)
I0429 17:11:06.836714  1834 net.cpp:132]     with loss weight 1
I0429 17:11:06.836724  1834 net.cpp:137] Memory required for data: 16784404
I0429 17:11:06.836730  1834 net.cpp:198] loss needs backward computation.
I0429 17:11:06.836737  1834 net.cpp:198] feat_p needs backward computation.
I0429 17:11:06.836742  1834 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:06.836748  1834 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:06.836753  1834 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:06.836758  1834 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:06.836765  1834 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:06.836769  1834 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:06.836774  1834 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:06.836781  1834 net.cpp:198] feat needs backward computation.
I0429 17:11:06.836786  1834 net.cpp:198] ip2 needs backward computation.
I0429 17:11:06.836791  1834 net.cpp:198] relu1 needs backward computation.
I0429 17:11:06.836796  1834 net.cpp:198] ip1 needs backward computation.
I0429 17:11:06.836802  1834 net.cpp:198] pool2 needs backward computation.
I0429 17:11:06.836807  1834 net.cpp:198] conv2 needs backward computation.
I0429 17:11:06.836812  1834 net.cpp:198] pool1 needs backward computation.
I0429 17:11:06.836818  1834 net.cpp:198] conv1 needs backward computation.
I0429 17:11:06.836827  1834 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:06.836834  1834 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:06.836839  1834 net.cpp:242] This network produces output loss
I0429 17:11:06.836969  1834 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:07.530382  1862 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:07.530407  1862 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:07.530411  1862 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:07.531849  1862 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:07.531998  1862 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:07.532095  1862 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:07.533094  1862 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:07.533444  1862 net.cpp:84] Creating Layer pair_data
I0429 17:11:07.533464  1862 net.cpp:380] pair_data -> pair_data
I0429 17:11:07.533485  1862 net.cpp:380] pair_data -> sim
I0429 17:11:07.533514  1862 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:07.534454  1862 net.cpp:122] Setting up pair_data
I0429 17:11:07.534472  1862 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:07.534482  1862 net.cpp:129] Top shape: 100 (100)
I0429 17:11:07.534487  1862 net.cpp:137] Memory required for data: 627600
I0429 17:11:07.534493  1862 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:07.534507  1862 net.cpp:84] Creating Layer slice_pair
I0429 17:11:07.534514  1862 net.cpp:406] slice_pair <- pair_data
I0429 17:11:07.534525  1862 net.cpp:380] slice_pair -> data
I0429 17:11:07.534538  1862 net.cpp:380] slice_pair -> data_p
I0429 17:11:07.534554  1862 net.cpp:122] Setting up slice_pair
I0429 17:11:07.534564  1862 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:07.534574  1862 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:07.534579  1862 net.cpp:137] Memory required for data: 1254800
I0429 17:11:07.534584  1862 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:07.534602  1862 net.cpp:84] Creating Layer conv1
I0429 17:11:07.534610  1862 net.cpp:406] conv1 <- data
I0429 17:11:07.534620  1862 net.cpp:380] conv1 -> conv1
I0429 17:11:07.534674  1862 net.cpp:122] Setting up conv1
I0429 17:11:07.534687  1862 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:07.534693  1862 net.cpp:137] Memory required for data: 5862800
I0429 17:11:07.534706  1862 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:07.534719  1862 net.cpp:84] Creating Layer pool1
I0429 17:11:07.534725  1862 net.cpp:406] pool1 <- conv1
I0429 17:11:07.534734  1862 net.cpp:380] pool1 -> pool1
I0429 17:11:07.534749  1862 net.cpp:122] Setting up pool1
I0429 17:11:07.534757  1862 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:07.534765  1862 net.cpp:137] Memory required for data: 7014800
I0429 17:11:07.534770  1862 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:07.534791  1862 net.cpp:84] Creating Layer conv2
I0429 17:11:07.534798  1862 net.cpp:406] conv2 <- pool1
I0429 17:11:07.534809  1862 net.cpp:380] conv2 -> conv2
I0429 17:11:07.535156  1862 net.cpp:122] Setting up conv2
I0429 17:11:07.535166  1862 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:07.535172  1862 net.cpp:137] Memory required for data: 8294800
I0429 17:11:07.535184  1862 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:07.535194  1862 net.cpp:84] Creating Layer pool2
I0429 17:11:07.535202  1862 net.cpp:406] pool2 <- conv2
I0429 17:11:07.535212  1862 net.cpp:380] pool2 -> pool2
I0429 17:11:07.535224  1862 net.cpp:122] Setting up pool2
I0429 17:11:07.535233  1862 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:07.535238  1862 net.cpp:137] Memory required for data: 8614800
I0429 17:11:07.535243  1862 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:07.535255  1862 net.cpp:84] Creating Layer ip1
I0429 17:11:07.535261  1862 net.cpp:406] ip1 <- pool2
I0429 17:11:07.535274  1862 net.cpp:380] ip1 -> ip1
I0429 17:11:07.540143  1862 net.cpp:122] Setting up ip1
I0429 17:11:07.540159  1862 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:07.540165  1862 net.cpp:137] Memory required for data: 8814800
I0429 17:11:07.540189  1862 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:07.540200  1862 net.cpp:84] Creating Layer relu1
I0429 17:11:07.540206  1862 net.cpp:406] relu1 <- ip1
I0429 17:11:07.540218  1862 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:07.540230  1862 net.cpp:122] Setting up relu1
I0429 17:11:07.540236  1862 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:07.540242  1862 net.cpp:137] Memory required for data: 9014800
I0429 17:11:07.540247  1862 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:07.540258  1862 net.cpp:84] Creating Layer ip2
I0429 17:11:07.540264  1862 net.cpp:406] ip2 <- ip1
I0429 17:11:07.540277  1862 net.cpp:380] ip2 -> ip2
I0429 17:11:07.540354  1862 net.cpp:122] Setting up ip2
I0429 17:11:07.540360  1862 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:07.540365  1862 net.cpp:137] Memory required for data: 9018800
I0429 17:11:07.540374  1862 layer_factory.hpp:77] Creating layer feat
I0429 17:11:07.540385  1862 net.cpp:84] Creating Layer feat
I0429 17:11:07.540390  1862 net.cpp:406] feat <- ip2
I0429 17:11:07.540401  1862 net.cpp:380] feat -> feat
I0429 17:11:07.540417  1862 net.cpp:122] Setting up feat
I0429 17:11:07.540424  1862 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:07.540429  1862 net.cpp:137] Memory required for data: 9019600
I0429 17:11:07.540441  1862 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:07.540454  1862 net.cpp:84] Creating Layer conv1_p
I0429 17:11:07.540460  1862 net.cpp:406] conv1_p <- data_p
I0429 17:11:07.540470  1862 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:07.540508  1862 net.cpp:122] Setting up conv1_p
I0429 17:11:07.540518  1862 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:07.540525  1862 net.cpp:137] Memory required for data: 13627600
I0429 17:11:07.540531  1862 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:07.540539  1862 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:07.540545  1862 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:07.540554  1862 net.cpp:84] Creating Layer pool1_p
I0429 17:11:07.540560  1862 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:07.540566  1862 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:07.540580  1862 net.cpp:122] Setting up pool1_p
I0429 17:11:07.540586  1862 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:07.540592  1862 net.cpp:137] Memory required for data: 14779600
I0429 17:11:07.540597  1862 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:07.540611  1862 net.cpp:84] Creating Layer conv2_p
I0429 17:11:07.540616  1862 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:07.540627  1862 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:07.540976  1862 net.cpp:122] Setting up conv2_p
I0429 17:11:07.540988  1862 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:07.540994  1862 net.cpp:137] Memory required for data: 16059600
I0429 17:11:07.541000  1862 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:07.541007  1862 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:07.541013  1862 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:07.541024  1862 net.cpp:84] Creating Layer pool2_p
I0429 17:11:07.541029  1862 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:07.541040  1862 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:07.541052  1862 net.cpp:122] Setting up pool2_p
I0429 17:11:07.541060  1862 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:07.541065  1862 net.cpp:137] Memory required for data: 16379600
I0429 17:11:07.541070  1862 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:07.541080  1862 net.cpp:84] Creating Layer ip1_p
I0429 17:11:07.541086  1862 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:07.541097  1862 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:07.545853  1862 net.cpp:122] Setting up ip1_p
I0429 17:11:07.545866  1862 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:07.545871  1862 net.cpp:137] Memory required for data: 16579600
I0429 17:11:07.545882  1862 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:07.545889  1862 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:07.545895  1862 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:07.545904  1862 net.cpp:84] Creating Layer relu1_p
I0429 17:11:07.545910  1862 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:07.545917  1862 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:07.545929  1862 net.cpp:122] Setting up relu1_p
I0429 17:11:07.545936  1862 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:07.545940  1862 net.cpp:137] Memory required for data: 16779600
I0429 17:11:07.545946  1862 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:07.545959  1862 net.cpp:84] Creating Layer ip2_p
I0429 17:11:07.545965  1862 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:07.545974  1862 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:07.546052  1862 net.cpp:122] Setting up ip2_p
I0429 17:11:07.546061  1862 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:07.546066  1862 net.cpp:137] Memory required for data: 16783600
I0429 17:11:07.546074  1862 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:07.546082  1862 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:07.546087  1862 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:07.546098  1862 net.cpp:84] Creating Layer feat_p
I0429 17:11:07.546103  1862 net.cpp:406] feat_p <- ip2_p
I0429 17:11:07.546111  1862 net.cpp:380] feat_p -> feat_p
I0429 17:11:07.546128  1862 net.cpp:122] Setting up feat_p
I0429 17:11:07.546135  1862 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:07.546139  1862 net.cpp:137] Memory required for data: 16784400
I0429 17:11:07.546145  1862 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:07.546152  1862 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:07.546159  1862 layer_factory.hpp:77] Creating layer loss
I0429 17:11:07.546169  1862 net.cpp:84] Creating Layer loss
I0429 17:11:07.546175  1862 net.cpp:406] loss <- feat
I0429 17:11:07.546181  1862 net.cpp:406] loss <- feat_p
I0429 17:11:07.546187  1862 net.cpp:406] loss <- sim
I0429 17:11:07.546195  1862 net.cpp:380] loss -> loss
I0429 17:11:07.546211  1862 net.cpp:122] Setting up loss
I0429 17:11:07.546218  1862 net.cpp:129] Top shape: (1)
I0429 17:11:07.546223  1862 net.cpp:132]     with loss weight 1
I0429 17:11:07.546234  1862 net.cpp:137] Memory required for data: 16784404
I0429 17:11:07.546241  1862 net.cpp:198] loss needs backward computation.
I0429 17:11:07.546248  1862 net.cpp:198] feat_p needs backward computation.
I0429 17:11:07.546253  1862 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:07.546259  1862 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:07.546264  1862 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:07.546270  1862 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:07.546277  1862 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:07.546281  1862 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:07.546288  1862 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:07.546293  1862 net.cpp:198] feat needs backward computation.
I0429 17:11:07.546299  1862 net.cpp:198] ip2 needs backward computation.
I0429 17:11:07.546305  1862 net.cpp:198] relu1 needs backward computation.
I0429 17:11:07.546310  1862 net.cpp:198] ip1 needs backward computation.
I0429 17:11:07.546316  1862 net.cpp:198] pool2 needs backward computation.
I0429 17:11:07.546322  1862 net.cpp:198] conv2 needs backward computation.
I0429 17:11:07.546327  1862 net.cpp:198] pool1 needs backward computation.
I0429 17:11:07.546334  1862 net.cpp:198] conv1 needs backward computation.
I0429 17:11:07.546342  1862 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:07.546350  1862 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:07.546355  1862 net.cpp:242] This network produces output loss
I0429 17:11:07.546494  1862 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.

 Performance counter stats for 'python test_model.py examples/images/cat_gray.jpg' (5 runs):

     2,000,517,839      cycles:u                                                      ( +-  9.06% )
     2,411,472,964      instructions:u            #    1.21  insns per cycle          ( +-  0.31% )

       0.798755097 seconds time elapsed                                          ( +-  7.52% )

WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:08.265012  1895 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:08.265036  1895 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:08.265039  1895 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:08.266477  1895 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:08.266646  1895 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:08.266717  1895 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:08.267709  1895 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:08.267995  1895 net.cpp:84] Creating Layer pair_data
I0429 17:11:08.268012  1895 net.cpp:380] pair_data -> pair_data
I0429 17:11:08.268028  1895 net.cpp:380] pair_data -> sim
I0429 17:11:08.268070  1895 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:08.268879  1895 net.cpp:122] Setting up pair_data
I0429 17:11:08.268900  1895 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:08.268908  1895 net.cpp:129] Top shape: 100 (100)
I0429 17:11:08.268913  1895 net.cpp:137] Memory required for data: 627600
I0429 17:11:08.268918  1895 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:08.268929  1895 net.cpp:84] Creating Layer slice_pair
I0429 17:11:08.268934  1895 net.cpp:406] slice_pair <- pair_data
I0429 17:11:08.268942  1895 net.cpp:380] slice_pair -> data
I0429 17:11:08.268954  1895 net.cpp:380] slice_pair -> data_p
I0429 17:11:08.268966  1895 net.cpp:122] Setting up slice_pair
I0429 17:11:08.268975  1895 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:08.268982  1895 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:08.268986  1895 net.cpp:137] Memory required for data: 1254800
I0429 17:11:08.268991  1895 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:08.269006  1895 net.cpp:84] Creating Layer conv1
I0429 17:11:08.269011  1895 net.cpp:406] conv1 <- data
I0429 17:11:08.269019  1895 net.cpp:380] conv1 -> conv1
I0429 17:11:08.269067  1895 net.cpp:122] Setting up conv1
I0429 17:11:08.269075  1895 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:08.269080  1895 net.cpp:137] Memory required for data: 5862800
I0429 17:11:08.269090  1895 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:08.269100  1895 net.cpp:84] Creating Layer pool1
I0429 17:11:08.269114  1895 net.cpp:406] pool1 <- conv1
I0429 17:11:08.269120  1895 net.cpp:380] pool1 -> pool1
I0429 17:11:08.269132  1895 net.cpp:122] Setting up pool1
I0429 17:11:08.269140  1895 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:08.269145  1895 net.cpp:137] Memory required for data: 7014800
I0429 17:11:08.269150  1895 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:08.269163  1895 net.cpp:84] Creating Layer conv2
I0429 17:11:08.269168  1895 net.cpp:406] conv2 <- pool1
I0429 17:11:08.269177  1895 net.cpp:380] conv2 -> conv2
I0429 17:11:08.269472  1895 net.cpp:122] Setting up conv2
I0429 17:11:08.269480  1895 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:08.269486  1895 net.cpp:137] Memory required for data: 8294800
I0429 17:11:08.269496  1895 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:08.269505  1895 net.cpp:84] Creating Layer pool2
I0429 17:11:08.269510  1895 net.cpp:406] pool2 <- conv2
I0429 17:11:08.269518  1895 net.cpp:380] pool2 -> pool2
I0429 17:11:08.269528  1895 net.cpp:122] Setting up pool2
I0429 17:11:08.269536  1895 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:08.269539  1895 net.cpp:137] Memory required for data: 8614800
I0429 17:11:08.269544  1895 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:08.269553  1895 net.cpp:84] Creating Layer ip1
I0429 17:11:08.269558  1895 net.cpp:406] ip1 <- pool2
I0429 17:11:08.269567  1895 net.cpp:380] ip1 -> ip1
I0429 17:11:08.273761  1895 net.cpp:122] Setting up ip1
I0429 17:11:08.273773  1895 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:08.273778  1895 net.cpp:137] Memory required for data: 8814800
I0429 17:11:08.273788  1895 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:08.273797  1895 net.cpp:84] Creating Layer relu1
I0429 17:11:08.273802  1895 net.cpp:406] relu1 <- ip1
I0429 17:11:08.273813  1895 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:08.273821  1895 net.cpp:122] Setting up relu1
I0429 17:11:08.273826  1895 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:08.273831  1895 net.cpp:137] Memory required for data: 9014800
I0429 17:11:08.273836  1895 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:08.273845  1895 net.cpp:84] Creating Layer ip2
I0429 17:11:08.273850  1895 net.cpp:406] ip2 <- ip1
I0429 17:11:08.273860  1895 net.cpp:380] ip2 -> ip2
I0429 17:11:08.273929  1895 net.cpp:122] Setting up ip2
I0429 17:11:08.273936  1895 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:08.273941  1895 net.cpp:137] Memory required for data: 9018800
I0429 17:11:08.273948  1895 layer_factory.hpp:77] Creating layer feat
I0429 17:11:08.273955  1895 net.cpp:84] Creating Layer feat
I0429 17:11:08.273960  1895 net.cpp:406] feat <- ip2
I0429 17:11:08.273983  1895 net.cpp:380] feat -> feat
I0429 17:11:08.274000  1895 net.cpp:122] Setting up feat
I0429 17:11:08.274006  1895 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:08.274011  1895 net.cpp:137] Memory required for data: 9019600
I0429 17:11:08.274020  1895 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:08.274034  1895 net.cpp:84] Creating Layer conv1_p
I0429 17:11:08.274039  1895 net.cpp:406] conv1_p <- data_p
I0429 17:11:08.274047  1895 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:08.274080  1895 net.cpp:122] Setting up conv1_p
I0429 17:11:08.274089  1895 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:08.274094  1895 net.cpp:137] Memory required for data: 13627600
I0429 17:11:08.274099  1895 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:08.274106  1895 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:08.274111  1895 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:08.274117  1895 net.cpp:84] Creating Layer pool1_p
I0429 17:11:08.274122  1895 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:08.274129  1895 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:08.274140  1895 net.cpp:122] Setting up pool1_p
I0429 17:11:08.274147  1895 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:08.274152  1895 net.cpp:137] Memory required for data: 14779600
I0429 17:11:08.274160  1895 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:08.274174  1895 net.cpp:84] Creating Layer conv2_p
I0429 17:11:08.274180  1895 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:08.274188  1895 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:08.274487  1895 net.cpp:122] Setting up conv2_p
I0429 17:11:08.274498  1895 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:08.274502  1895 net.cpp:137] Memory required for data: 16059600
I0429 17:11:08.274508  1895 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:08.274513  1895 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:08.274519  1895 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:08.274526  1895 net.cpp:84] Creating Layer pool2_p
I0429 17:11:08.274531  1895 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:08.274540  1895 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:08.274550  1895 net.cpp:122] Setting up pool2_p
I0429 17:11:08.274557  1895 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:08.274561  1895 net.cpp:137] Memory required for data: 16379600
I0429 17:11:08.274566  1895 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:08.274574  1895 net.cpp:84] Creating Layer ip1_p
I0429 17:11:08.274580  1895 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:08.274588  1895 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:08.278764  1895 net.cpp:122] Setting up ip1_p
I0429 17:11:08.278776  1895 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:08.278780  1895 net.cpp:137] Memory required for data: 16579600
I0429 17:11:08.278786  1895 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:08.278792  1895 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:08.278797  1895 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:08.278805  1895 net.cpp:84] Creating Layer relu1_p
I0429 17:11:08.278810  1895 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:08.278817  1895 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:08.278825  1895 net.cpp:122] Setting up relu1_p
I0429 17:11:08.278833  1895 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:08.278837  1895 net.cpp:137] Memory required for data: 16779600
I0429 17:11:08.278842  1895 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:08.278854  1895 net.cpp:84] Creating Layer ip2_p
I0429 17:11:08.278858  1895 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:08.278867  1895 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:08.278934  1895 net.cpp:122] Setting up ip2_p
I0429 17:11:08.278942  1895 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:08.278946  1895 net.cpp:137] Memory required for data: 16783600
I0429 17:11:08.278954  1895 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:08.278960  1895 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:08.278965  1895 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:08.278976  1895 net.cpp:84] Creating Layer feat_p
I0429 17:11:08.278980  1895 net.cpp:406] feat_p <- ip2_p
I0429 17:11:08.278988  1895 net.cpp:380] feat_p -> feat_p
I0429 17:11:08.279002  1895 net.cpp:122] Setting up feat_p
I0429 17:11:08.279008  1895 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:08.279012  1895 net.cpp:137] Memory required for data: 16784400
I0429 17:11:08.279017  1895 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:08.279023  1895 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:08.279027  1895 layer_factory.hpp:77] Creating layer loss
I0429 17:11:08.279037  1895 net.cpp:84] Creating Layer loss
I0429 17:11:08.279042  1895 net.cpp:406] loss <- feat
I0429 17:11:08.279048  1895 net.cpp:406] loss <- feat_p
I0429 17:11:08.279053  1895 net.cpp:406] loss <- sim
I0429 17:11:08.279060  1895 net.cpp:380] loss -> loss
I0429 17:11:08.279073  1895 net.cpp:122] Setting up loss
I0429 17:11:08.279080  1895 net.cpp:129] Top shape: (1)
I0429 17:11:08.279084  1895 net.cpp:132]     with loss weight 1
I0429 17:11:08.279099  1895 net.cpp:137] Memory required for data: 16784404
I0429 17:11:08.279104  1895 net.cpp:198] loss needs backward computation.
I0429 17:11:08.279111  1895 net.cpp:198] feat_p needs backward computation.
I0429 17:11:08.279116  1895 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:08.279121  1895 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:08.279125  1895 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:08.279131  1895 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:08.279135  1895 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:08.279140  1895 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:08.279145  1895 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:08.279151  1895 net.cpp:198] feat needs backward computation.
I0429 17:11:08.279156  1895 net.cpp:198] ip2 needs backward computation.
I0429 17:11:08.279160  1895 net.cpp:198] relu1 needs backward computation.
I0429 17:11:08.279165  1895 net.cpp:198] ip1 needs backward computation.
I0429 17:11:08.279170  1895 net.cpp:198] pool2 needs backward computation.
I0429 17:11:08.279175  1895 net.cpp:198] conv2 needs backward computation.
I0429 17:11:08.279181  1895 net.cpp:198] pool1 needs backward computation.
I0429 17:11:08.279186  1895 net.cpp:198] conv1 needs backward computation.
I0429 17:11:08.279191  1895 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:08.279199  1895 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:08.279204  1895 net.cpp:242] This network produces output loss
I0429 17:11:08.279327  1895 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:08.966570  1923 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:08.966594  1923 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:08.966598  1923 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:08.968024  1923 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:08.968209  1923 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:08.968286  1923 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:08.969328  1923 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:08.969616  1923 net.cpp:84] Creating Layer pair_data
I0429 17:11:08.969631  1923 net.cpp:380] pair_data -> pair_data
I0429 17:11:08.969648  1923 net.cpp:380] pair_data -> sim
I0429 17:11:08.969671  1923 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:08.970474  1923 net.cpp:122] Setting up pair_data
I0429 17:11:08.970489  1923 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:08.970496  1923 net.cpp:129] Top shape: 100 (100)
I0429 17:11:08.970506  1923 net.cpp:137] Memory required for data: 627600
I0429 17:11:08.970512  1923 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:08.970525  1923 net.cpp:84] Creating Layer slice_pair
I0429 17:11:08.970530  1923 net.cpp:406] slice_pair <- pair_data
I0429 17:11:08.970538  1923 net.cpp:380] slice_pair -> data
I0429 17:11:08.970549  1923 net.cpp:380] slice_pair -> data_p
I0429 17:11:08.970562  1923 net.cpp:122] Setting up slice_pair
I0429 17:11:08.970571  1923 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:08.970577  1923 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:08.970582  1923 net.cpp:137] Memory required for data: 1254800
I0429 17:11:08.970587  1923 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:08.970602  1923 net.cpp:84] Creating Layer conv1
I0429 17:11:08.970607  1923 net.cpp:406] conv1 <- data
I0429 17:11:08.970615  1923 net.cpp:380] conv1 -> conv1
I0429 17:11:08.970660  1923 net.cpp:122] Setting up conv1
I0429 17:11:08.970669  1923 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:08.970675  1923 net.cpp:137] Memory required for data: 5862800
I0429 17:11:08.970686  1923 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:08.970697  1923 net.cpp:84] Creating Layer pool1
I0429 17:11:08.970702  1923 net.cpp:406] pool1 <- conv1
I0429 17:11:08.970710  1923 net.cpp:380] pool1 -> pool1
I0429 17:11:08.970722  1923 net.cpp:122] Setting up pool1
I0429 17:11:08.970729  1923 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:08.970736  1923 net.cpp:137] Memory required for data: 7014800
I0429 17:11:08.970739  1923 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:08.970753  1923 net.cpp:84] Creating Layer conv2
I0429 17:11:08.970758  1923 net.cpp:406] conv2 <- pool1
I0429 17:11:08.970767  1923 net.cpp:380] conv2 -> conv2
I0429 17:11:08.971055  1923 net.cpp:122] Setting up conv2
I0429 17:11:08.971063  1923 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:08.971068  1923 net.cpp:137] Memory required for data: 8294800
I0429 17:11:08.971078  1923 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:08.971086  1923 net.cpp:84] Creating Layer pool2
I0429 17:11:08.971091  1923 net.cpp:406] pool2 <- conv2
I0429 17:11:08.971101  1923 net.cpp:380] pool2 -> pool2
I0429 17:11:08.971112  1923 net.cpp:122] Setting up pool2
I0429 17:11:08.971118  1923 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:08.971123  1923 net.cpp:137] Memory required for data: 8614800
I0429 17:11:08.971127  1923 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:08.971138  1923 net.cpp:84] Creating Layer ip1
I0429 17:11:08.971143  1923 net.cpp:406] ip1 <- pool2
I0429 17:11:08.971153  1923 net.cpp:380] ip1 -> ip1
I0429 17:11:08.975198  1923 net.cpp:122] Setting up ip1
I0429 17:11:08.975211  1923 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:08.975215  1923 net.cpp:137] Memory required for data: 8814800
I0429 17:11:08.975229  1923 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:08.975237  1923 net.cpp:84] Creating Layer relu1
I0429 17:11:08.975242  1923 net.cpp:406] relu1 <- ip1
I0429 17:11:08.975251  1923 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:08.975260  1923 net.cpp:122] Setting up relu1
I0429 17:11:08.975265  1923 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:08.975270  1923 net.cpp:137] Memory required for data: 9014800
I0429 17:11:08.975275  1923 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:08.975284  1923 net.cpp:84] Creating Layer ip2
I0429 17:11:08.975288  1923 net.cpp:406] ip2 <- ip1
I0429 17:11:08.975298  1923 net.cpp:380] ip2 -> ip2
I0429 17:11:08.975363  1923 net.cpp:122] Setting up ip2
I0429 17:11:08.975369  1923 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:08.975373  1923 net.cpp:137] Memory required for data: 9018800
I0429 17:11:08.975381  1923 layer_factory.hpp:77] Creating layer feat
I0429 17:11:08.975389  1923 net.cpp:84] Creating Layer feat
I0429 17:11:08.975392  1923 net.cpp:406] feat <- ip2
I0429 17:11:08.975401  1923 net.cpp:380] feat -> feat
I0429 17:11:08.975415  1923 net.cpp:122] Setting up feat
I0429 17:11:08.975426  1923 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:08.975431  1923 net.cpp:137] Memory required for data: 9019600
I0429 17:11:08.975441  1923 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:08.975453  1923 net.cpp:84] Creating Layer conv1_p
I0429 17:11:08.975458  1923 net.cpp:406] conv1_p <- data_p
I0429 17:11:08.975466  1923 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:08.975497  1923 net.cpp:122] Setting up conv1_p
I0429 17:11:08.975507  1923 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:08.975512  1923 net.cpp:137] Memory required for data: 13627600
I0429 17:11:08.975517  1923 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:08.975522  1923 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:08.975528  1923 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:08.975534  1923 net.cpp:84] Creating Layer pool1_p
I0429 17:11:08.975538  1923 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:08.975545  1923 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:08.975555  1923 net.cpp:122] Setting up pool1_p
I0429 17:11:08.975563  1923 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:08.975566  1923 net.cpp:137] Memory required for data: 14779600
I0429 17:11:08.975570  1923 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:08.975584  1923 net.cpp:84] Creating Layer conv2_p
I0429 17:11:08.975589  1923 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:08.975596  1923 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:08.975867  1923 net.cpp:122] Setting up conv2_p
I0429 17:11:08.975877  1923 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:08.975880  1923 net.cpp:137] Memory required for data: 16059600
I0429 17:11:08.975886  1923 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:08.975891  1923 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:08.975896  1923 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:08.975904  1923 net.cpp:84] Creating Layer pool2_p
I0429 17:11:08.975908  1923 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:08.975916  1923 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:08.975926  1923 net.cpp:122] Setting up pool2_p
I0429 17:11:08.975932  1923 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:08.975937  1923 net.cpp:137] Memory required for data: 16379600
I0429 17:11:08.975941  1923 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:08.975950  1923 net.cpp:84] Creating Layer ip1_p
I0429 17:11:08.975953  1923 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:08.975961  1923 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:08.979882  1923 net.cpp:122] Setting up ip1_p
I0429 17:11:08.979892  1923 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:08.979897  1923 net.cpp:137] Memory required for data: 16579600
I0429 17:11:08.979902  1923 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:08.979907  1923 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:08.979912  1923 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:08.979919  1923 net.cpp:84] Creating Layer relu1_p
I0429 17:11:08.979924  1923 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:08.979930  1923 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:08.979939  1923 net.cpp:122] Setting up relu1_p
I0429 17:11:08.979945  1923 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:08.979949  1923 net.cpp:137] Memory required for data: 16779600
I0429 17:11:08.979954  1923 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:08.979964  1923 net.cpp:84] Creating Layer ip2_p
I0429 17:11:08.979969  1923 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:08.979975  1923 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:08.980046  1923 net.cpp:122] Setting up ip2_p
I0429 17:11:08.980056  1923 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:08.980059  1923 net.cpp:137] Memory required for data: 16783600
I0429 17:11:08.980067  1923 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:08.980077  1923 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:08.980082  1923 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:08.980090  1923 net.cpp:84] Creating Layer feat_p
I0429 17:11:08.980095  1923 net.cpp:406] feat_p <- ip2_p
I0429 17:11:08.980103  1923 net.cpp:380] feat_p -> feat_p
I0429 17:11:08.980116  1923 net.cpp:122] Setting up feat_p
I0429 17:11:08.980121  1923 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:08.980125  1923 net.cpp:137] Memory required for data: 16784400
I0429 17:11:08.980130  1923 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:08.980136  1923 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:08.980140  1923 layer_factory.hpp:77] Creating layer loss
I0429 17:11:08.980149  1923 net.cpp:84] Creating Layer loss
I0429 17:11:08.980154  1923 net.cpp:406] loss <- feat
I0429 17:11:08.980159  1923 net.cpp:406] loss <- feat_p
I0429 17:11:08.980165  1923 net.cpp:406] loss <- sim
I0429 17:11:08.980171  1923 net.cpp:380] loss -> loss
I0429 17:11:08.980185  1923 net.cpp:122] Setting up loss
I0429 17:11:08.980191  1923 net.cpp:129] Top shape: (1)
I0429 17:11:08.980195  1923 net.cpp:132]     with loss weight 1
I0429 17:11:08.980204  1923 net.cpp:137] Memory required for data: 16784404
I0429 17:11:08.980209  1923 net.cpp:198] loss needs backward computation.
I0429 17:11:08.980216  1923 net.cpp:198] feat_p needs backward computation.
I0429 17:11:08.980221  1923 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:08.980226  1923 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:08.980229  1923 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:08.980234  1923 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:08.980239  1923 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:08.980243  1923 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:08.980248  1923 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:08.980253  1923 net.cpp:198] feat needs backward computation.
I0429 17:11:08.980258  1923 net.cpp:198] ip2 needs backward computation.
I0429 17:11:08.980263  1923 net.cpp:198] relu1 needs backward computation.
I0429 17:11:08.980268  1923 net.cpp:198] ip1 needs backward computation.
I0429 17:11:08.980273  1923 net.cpp:198] pool2 needs backward computation.
I0429 17:11:08.980278  1923 net.cpp:198] conv2 needs backward computation.
I0429 17:11:08.980281  1923 net.cpp:198] pool1 needs backward computation.
I0429 17:11:08.980286  1923 net.cpp:198] conv1 needs backward computation.
I0429 17:11:08.980294  1923 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:08.980300  1923 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:08.980304  1923 net.cpp:242] This network produces output loss
I0429 17:11:08.980422  1923 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:10.026942  1952 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:10.026974  1952 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:10.026980  1952 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:10.029224  1952 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:10.029467  1952 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:10.029569  1952 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:10.030724  1952 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:10.031041  1952 net.cpp:84] Creating Layer pair_data
I0429 17:11:10.031059  1952 net.cpp:380] pair_data -> pair_data
I0429 17:11:10.031080  1952 net.cpp:380] pair_data -> sim
I0429 17:11:10.031117  1952 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:10.032011  1952 net.cpp:122] Setting up pair_data
I0429 17:11:10.032063  1952 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:10.032078  1952 net.cpp:129] Top shape: 100 (100)
I0429 17:11:10.032083  1952 net.cpp:137] Memory required for data: 627600
I0429 17:11:10.032089  1952 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:10.032104  1952 net.cpp:84] Creating Layer slice_pair
I0429 17:11:10.032110  1952 net.cpp:406] slice_pair <- pair_data
I0429 17:11:10.032120  1952 net.cpp:380] slice_pair -> data
I0429 17:11:10.032133  1952 net.cpp:380] slice_pair -> data_p
I0429 17:11:10.032148  1952 net.cpp:122] Setting up slice_pair
I0429 17:11:10.032157  1952 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:10.032166  1952 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:10.032171  1952 net.cpp:137] Memory required for data: 1254800
I0429 17:11:10.032178  1952 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:10.032196  1952 net.cpp:84] Creating Layer conv1
I0429 17:11:10.032203  1952 net.cpp:406] conv1 <- data
I0429 17:11:10.032213  1952 net.cpp:380] conv1 -> conv1
I0429 17:11:10.032263  1952 net.cpp:122] Setting up conv1
I0429 17:11:10.032275  1952 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:10.032280  1952 net.cpp:137] Memory required for data: 5862800
I0429 17:11:10.032294  1952 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:10.032305  1952 net.cpp:84] Creating Layer pool1
I0429 17:11:10.032312  1952 net.cpp:406] pool1 <- conv1
I0429 17:11:10.032320  1952 net.cpp:380] pool1 -> pool1
I0429 17:11:10.032335  1952 net.cpp:122] Setting up pool1
I0429 17:11:10.032343  1952 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:10.032349  1952 net.cpp:137] Memory required for data: 7014800
I0429 17:11:10.032356  1952 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:10.032383  1952 net.cpp:84] Creating Layer conv2
I0429 17:11:10.032390  1952 net.cpp:406] conv2 <- pool1
I0429 17:11:10.032402  1952 net.cpp:380] conv2 -> conv2
I0429 17:11:10.032748  1952 net.cpp:122] Setting up conv2
I0429 17:11:10.032762  1952 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:10.032766  1952 net.cpp:137] Memory required for data: 8294800
I0429 17:11:10.032778  1952 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:10.032791  1952 net.cpp:84] Creating Layer pool2
I0429 17:11:10.032797  1952 net.cpp:406] pool2 <- conv2
I0429 17:11:10.032806  1952 net.cpp:380] pool2 -> pool2
I0429 17:11:10.032819  1952 net.cpp:122] Setting up pool2
I0429 17:11:10.032826  1952 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:10.032831  1952 net.cpp:137] Memory required for data: 8614800
I0429 17:11:10.032837  1952 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:10.032861  1952 net.cpp:84] Creating Layer ip1
I0429 17:11:10.032866  1952 net.cpp:406] ip1 <- pool2
I0429 17:11:10.032877  1952 net.cpp:380] ip1 -> ip1
I0429 17:11:10.037421  1952 net.cpp:122] Setting up ip1
I0429 17:11:10.037436  1952 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:10.037448  1952 net.cpp:137] Memory required for data: 8814800
I0429 17:11:10.037463  1952 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:10.037473  1952 net.cpp:84] Creating Layer relu1
I0429 17:11:10.037479  1952 net.cpp:406] relu1 <- ip1
I0429 17:11:10.037489  1952 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:10.037499  1952 net.cpp:122] Setting up relu1
I0429 17:11:10.037506  1952 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:10.037511  1952 net.cpp:137] Memory required for data: 9014800
I0429 17:11:10.037516  1952 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:10.037528  1952 net.cpp:84] Creating Layer ip2
I0429 17:11:10.037533  1952 net.cpp:406] ip2 <- ip1
I0429 17:11:10.037544  1952 net.cpp:380] ip2 -> ip2
I0429 17:11:10.037617  1952 net.cpp:122] Setting up ip2
I0429 17:11:10.037624  1952 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:10.037629  1952 net.cpp:137] Memory required for data: 9018800
I0429 17:11:10.037638  1952 layer_factory.hpp:77] Creating layer feat
I0429 17:11:10.037647  1952 net.cpp:84] Creating Layer feat
I0429 17:11:10.037652  1952 net.cpp:406] feat <- ip2
I0429 17:11:10.037664  1952 net.cpp:380] feat -> feat
I0429 17:11:10.037680  1952 net.cpp:122] Setting up feat
I0429 17:11:10.037688  1952 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:10.037693  1952 net.cpp:137] Memory required for data: 9019600
I0429 17:11:10.037703  1952 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:10.037716  1952 net.cpp:84] Creating Layer conv1_p
I0429 17:11:10.037722  1952 net.cpp:406] conv1_p <- data_p
I0429 17:11:10.037734  1952 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:10.037767  1952 net.cpp:122] Setting up conv1_p
I0429 17:11:10.037778  1952 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:10.037784  1952 net.cpp:137] Memory required for data: 13627600
I0429 17:11:10.037791  1952 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:10.037797  1952 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:10.037804  1952 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:10.037812  1952 net.cpp:84] Creating Layer pool1_p
I0429 17:11:10.037819  1952 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:10.037827  1952 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:10.037839  1952 net.cpp:122] Setting up pool1_p
I0429 17:11:10.037847  1952 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:10.037853  1952 net.cpp:137] Memory required for data: 14779600
I0429 17:11:10.037858  1952 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:10.037871  1952 net.cpp:84] Creating Layer conv2_p
I0429 17:11:10.037878  1952 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:10.037888  1952 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:10.038190  1952 net.cpp:122] Setting up conv2_p
I0429 17:11:10.038200  1952 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:10.038205  1952 net.cpp:137] Memory required for data: 16059600
I0429 17:11:10.038213  1952 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:10.038218  1952 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:10.038225  1952 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:10.038233  1952 net.cpp:84] Creating Layer pool2_p
I0429 17:11:10.038239  1952 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:10.038249  1952 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:10.038260  1952 net.cpp:122] Setting up pool2_p
I0429 17:11:10.038267  1952 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:10.038272  1952 net.cpp:137] Memory required for data: 16379600
I0429 17:11:10.038277  1952 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:10.038287  1952 net.cpp:84] Creating Layer ip1_p
I0429 17:11:10.038292  1952 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:10.038303  1952 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:10.042806  1952 net.cpp:122] Setting up ip1_p
I0429 17:11:10.042819  1952 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:10.042824  1952 net.cpp:137] Memory required for data: 16579600
I0429 17:11:10.042834  1952 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:10.042841  1952 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:10.042848  1952 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:10.042858  1952 net.cpp:84] Creating Layer relu1_p
I0429 17:11:10.042863  1952 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:10.042871  1952 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:10.042883  1952 net.cpp:122] Setting up relu1_p
I0429 17:11:10.042889  1952 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:10.042894  1952 net.cpp:137] Memory required for data: 16779600
I0429 17:11:10.042899  1952 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:10.042912  1952 net.cpp:84] Creating Layer ip2_p
I0429 17:11:10.042918  1952 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:10.042927  1952 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:10.043001  1952 net.cpp:122] Setting up ip2_p
I0429 17:11:10.043011  1952 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:10.043015  1952 net.cpp:137] Memory required for data: 16783600
I0429 17:11:10.043025  1952 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:10.043031  1952 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:10.043037  1952 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:10.043047  1952 net.cpp:84] Creating Layer feat_p
I0429 17:11:10.043053  1952 net.cpp:406] feat_p <- ip2_p
I0429 17:11:10.043061  1952 net.cpp:380] feat_p -> feat_p
I0429 17:11:10.043077  1952 net.cpp:122] Setting up feat_p
I0429 17:11:10.043084  1952 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:10.043089  1952 net.cpp:137] Memory required for data: 16784400
I0429 17:11:10.043095  1952 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:10.043102  1952 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:10.043107  1952 layer_factory.hpp:77] Creating layer loss
I0429 17:11:10.043118  1952 net.cpp:84] Creating Layer loss
I0429 17:11:10.043123  1952 net.cpp:406] loss <- feat
I0429 17:11:10.043130  1952 net.cpp:406] loss <- feat_p
I0429 17:11:10.043136  1952 net.cpp:406] loss <- sim
I0429 17:11:10.043144  1952 net.cpp:380] loss -> loss
I0429 17:11:10.043159  1952 net.cpp:122] Setting up loss
I0429 17:11:10.043167  1952 net.cpp:129] Top shape: (1)
I0429 17:11:10.043172  1952 net.cpp:132]     with loss weight 1
I0429 17:11:10.043184  1952 net.cpp:137] Memory required for data: 16784404
I0429 17:11:10.043190  1952 net.cpp:198] loss needs backward computation.
I0429 17:11:10.043196  1952 net.cpp:198] feat_p needs backward computation.
I0429 17:11:10.043202  1952 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:10.043208  1952 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:10.043213  1952 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:10.043220  1952 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:10.043226  1952 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:10.043231  1952 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:10.043236  1952 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:10.043243  1952 net.cpp:198] feat needs backward computation.
I0429 17:11:10.043248  1952 net.cpp:198] ip2 needs backward computation.
I0429 17:11:10.043254  1952 net.cpp:198] relu1 needs backward computation.
I0429 17:11:10.043259  1952 net.cpp:198] ip1 needs backward computation.
I0429 17:11:10.043267  1952 net.cpp:198] pool2 needs backward computation.
I0429 17:11:10.043272  1952 net.cpp:198] conv2 needs backward computation.
I0429 17:11:10.043277  1952 net.cpp:198] pool1 needs backward computation.
I0429 17:11:10.043282  1952 net.cpp:198] conv1 needs backward computation.
I0429 17:11:10.043292  1952 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:10.043299  1952 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:10.043304  1952 net.cpp:242] This network produces output loss
I0429 17:11:10.043443  1952 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:10.837592  1988 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:10.837616  1988 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:10.837620  1988 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:10.839035  1988 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:10.839184  1988 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:10.839259  1988 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:10.840062  1988 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:10.840252  1988 net.cpp:84] Creating Layer pair_data
I0429 17:11:10.840261  1988 net.cpp:380] pair_data -> pair_data
I0429 17:11:10.840272  1988 net.cpp:380] pair_data -> sim
I0429 17:11:10.840291  1988 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:10.840855  1988 net.cpp:122] Setting up pair_data
I0429 17:11:10.840883  1988 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:10.840886  1988 net.cpp:129] Top shape: 100 (100)
I0429 17:11:10.840889  1988 net.cpp:137] Memory required for data: 627600
I0429 17:11:10.840893  1988 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:10.840900  1988 net.cpp:84] Creating Layer slice_pair
I0429 17:11:10.840904  1988 net.cpp:406] slice_pair <- pair_data
I0429 17:11:10.840909  1988 net.cpp:380] slice_pair -> data
I0429 17:11:10.840916  1988 net.cpp:380] slice_pair -> data_p
I0429 17:11:10.840924  1988 net.cpp:122] Setting up slice_pair
I0429 17:11:10.840929  1988 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:10.840934  1988 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:10.840936  1988 net.cpp:137] Memory required for data: 1254800
I0429 17:11:10.840939  1988 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:10.840950  1988 net.cpp:84] Creating Layer conv1
I0429 17:11:10.840952  1988 net.cpp:406] conv1 <- data
I0429 17:11:10.840957  1988 net.cpp:380] conv1 -> conv1
I0429 17:11:10.840987  1988 net.cpp:122] Setting up conv1
I0429 17:11:10.840993  1988 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:10.840996  1988 net.cpp:137] Memory required for data: 5862800
I0429 17:11:10.841003  1988 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:10.841011  1988 net.cpp:84] Creating Layer pool1
I0429 17:11:10.841013  1988 net.cpp:406] pool1 <- conv1
I0429 17:11:10.841019  1988 net.cpp:380] pool1 -> pool1
I0429 17:11:10.841027  1988 net.cpp:122] Setting up pool1
I0429 17:11:10.841032  1988 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:10.841035  1988 net.cpp:137] Memory required for data: 7014800
I0429 17:11:10.841042  1988 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:10.841053  1988 net.cpp:84] Creating Layer conv2
I0429 17:11:10.841056  1988 net.cpp:406] conv2 <- pool1
I0429 17:11:10.841063  1988 net.cpp:380] conv2 -> conv2
I0429 17:11:10.841248  1988 net.cpp:122] Setting up conv2
I0429 17:11:10.841253  1988 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:10.841256  1988 net.cpp:137] Memory required for data: 8294800
I0429 17:11:10.841264  1988 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:10.841269  1988 net.cpp:84] Creating Layer pool2
I0429 17:11:10.841272  1988 net.cpp:406] pool2 <- conv2
I0429 17:11:10.841279  1988 net.cpp:380] pool2 -> pool2
I0429 17:11:10.841287  1988 net.cpp:122] Setting up pool2
I0429 17:11:10.841291  1988 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:10.841295  1988 net.cpp:137] Memory required for data: 8614800
I0429 17:11:10.841297  1988 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:10.841303  1988 net.cpp:84] Creating Layer ip1
I0429 17:11:10.841306  1988 net.cpp:406] ip1 <- pool2
I0429 17:11:10.841312  1988 net.cpp:380] ip1 -> ip1
I0429 17:11:10.844074  1988 net.cpp:122] Setting up ip1
I0429 17:11:10.844084  1988 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:10.844087  1988 net.cpp:137] Memory required for data: 8814800
I0429 17:11:10.844094  1988 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:10.844099  1988 net.cpp:84] Creating Layer relu1
I0429 17:11:10.844105  1988 net.cpp:406] relu1 <- ip1
I0429 17:11:10.844110  1988 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:10.844116  1988 net.cpp:122] Setting up relu1
I0429 17:11:10.844120  1988 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:10.844123  1988 net.cpp:137] Memory required for data: 9014800
I0429 17:11:10.844126  1988 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:10.844133  1988 net.cpp:84] Creating Layer ip2
I0429 17:11:10.844136  1988 net.cpp:406] ip2 <- ip1
I0429 17:11:10.844142  1988 net.cpp:380] ip2 -> ip2
I0429 17:11:10.844203  1988 net.cpp:122] Setting up ip2
I0429 17:11:10.844209  1988 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:10.844213  1988 net.cpp:137] Memory required for data: 9018800
I0429 17:11:10.844218  1988 layer_factory.hpp:77] Creating layer feat
I0429 17:11:10.844223  1988 net.cpp:84] Creating Layer feat
I0429 17:11:10.844225  1988 net.cpp:406] feat <- ip2
I0429 17:11:10.844231  1988 net.cpp:380] feat -> feat
I0429 17:11:10.844241  1988 net.cpp:122] Setting up feat
I0429 17:11:10.844245  1988 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:10.844249  1988 net.cpp:137] Memory required for data: 9019600
I0429 17:11:10.844254  1988 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:10.844264  1988 net.cpp:84] Creating Layer conv1_p
I0429 17:11:10.844267  1988 net.cpp:406] conv1_p <- data_p
I0429 17:11:10.844272  1988 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:10.844296  1988 net.cpp:122] Setting up conv1_p
I0429 17:11:10.844302  1988 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:10.844305  1988 net.cpp:137] Memory required for data: 13627600
I0429 17:11:10.844308  1988 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:10.844312  1988 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:10.844316  1988 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:10.844321  1988 net.cpp:84] Creating Layer pool1_p
I0429 17:11:10.844323  1988 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:10.844328  1988 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:10.844336  1988 net.cpp:122] Setting up pool1_p
I0429 17:11:10.844341  1988 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:10.844343  1988 net.cpp:137] Memory required for data: 14779600
I0429 17:11:10.844347  1988 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:10.844355  1988 net.cpp:84] Creating Layer conv2_p
I0429 17:11:10.844358  1988 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:10.844363  1988 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:10.844547  1988 net.cpp:122] Setting up conv2_p
I0429 17:11:10.844558  1988 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:10.844560  1988 net.cpp:137] Memory required for data: 16059600
I0429 17:11:10.844564  1988 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:10.844568  1988 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:10.844571  1988 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:10.844576  1988 net.cpp:84] Creating Layer pool2_p
I0429 17:11:10.844579  1988 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:10.844590  1988 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:10.844597  1988 net.cpp:122] Setting up pool2_p
I0429 17:11:10.844601  1988 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:10.844604  1988 net.cpp:137] Memory required for data: 16379600
I0429 17:11:10.844606  1988 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:10.844614  1988 net.cpp:84] Creating Layer ip1_p
I0429 17:11:10.844616  1988 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:10.844621  1988 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:10.847338  1988 net.cpp:122] Setting up ip1_p
I0429 17:11:10.847345  1988 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:10.847348  1988 net.cpp:137] Memory required for data: 16579600
I0429 17:11:10.847352  1988 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:10.847355  1988 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:10.847359  1988 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:10.847363  1988 net.cpp:84] Creating Layer relu1_p
I0429 17:11:10.847367  1988 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:10.847373  1988 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:10.847378  1988 net.cpp:122] Setting up relu1_p
I0429 17:11:10.847381  1988 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:10.847384  1988 net.cpp:137] Memory required for data: 16779600
I0429 17:11:10.847388  1988 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:10.847394  1988 net.cpp:84] Creating Layer ip2_p
I0429 17:11:10.847398  1988 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:10.847403  1988 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:10.847448  1988 net.cpp:122] Setting up ip2_p
I0429 17:11:10.847453  1988 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:10.847456  1988 net.cpp:137] Memory required for data: 16783600
I0429 17:11:10.847461  1988 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:10.847465  1988 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:10.847468  1988 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:10.847474  1988 net.cpp:84] Creating Layer feat_p
I0429 17:11:10.847477  1988 net.cpp:406] feat_p <- ip2_p
I0429 17:11:10.847482  1988 net.cpp:380] feat_p -> feat_p
I0429 17:11:10.847491  1988 net.cpp:122] Setting up feat_p
I0429 17:11:10.847496  1988 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:10.847497  1988 net.cpp:137] Memory required for data: 16784400
I0429 17:11:10.847501  1988 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:10.847506  1988 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:10.847508  1988 layer_factory.hpp:77] Creating layer loss
I0429 17:11:10.847514  1988 net.cpp:84] Creating Layer loss
I0429 17:11:10.847517  1988 net.cpp:406] loss <- feat
I0429 17:11:10.847522  1988 net.cpp:406] loss <- feat_p
I0429 17:11:10.847525  1988 net.cpp:406] loss <- sim
I0429 17:11:10.847529  1988 net.cpp:380] loss -> loss
I0429 17:11:10.847538  1988 net.cpp:122] Setting up loss
I0429 17:11:10.847543  1988 net.cpp:129] Top shape: (1)
I0429 17:11:10.847545  1988 net.cpp:132]     with loss weight 1
I0429 17:11:10.847553  1988 net.cpp:137] Memory required for data: 16784404
I0429 17:11:10.847556  1988 net.cpp:198] loss needs backward computation.
I0429 17:11:10.847561  1988 net.cpp:198] feat_p needs backward computation.
I0429 17:11:10.847565  1988 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:10.847573  1988 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:10.847575  1988 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:10.847579  1988 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:10.847582  1988 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:10.847585  1988 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:10.847589  1988 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:10.847592  1988 net.cpp:198] feat needs backward computation.
I0429 17:11:10.847595  1988 net.cpp:198] ip2 needs backward computation.
I0429 17:11:10.847599  1988 net.cpp:198] relu1 needs backward computation.
I0429 17:11:10.847601  1988 net.cpp:198] ip1 needs backward computation.
I0429 17:11:10.847605  1988 net.cpp:198] pool2 needs backward computation.
I0429 17:11:10.847609  1988 net.cpp:198] conv2 needs backward computation.
I0429 17:11:10.847611  1988 net.cpp:198] pool1 needs backward computation.
I0429 17:11:10.847615  1988 net.cpp:198] conv1 needs backward computation.
I0429 17:11:10.847620  1988 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:10.847625  1988 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:10.847626  1988 net.cpp:242] This network produces output loss
I0429 17:11:10.847712  1988 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:11.546320  2016 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:11.546345  2016 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:11.546349  2016 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:11.547757  2016 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:11.547906  2016 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:11.547976  2016 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:11.548974  2016 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:11.549266  2016 net.cpp:84] Creating Layer pair_data
I0429 17:11:11.549283  2016 net.cpp:380] pair_data -> pair_data
I0429 17:11:11.549299  2016 net.cpp:380] pair_data -> sim
I0429 17:11:11.549331  2016 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:11.550134  2016 net.cpp:122] Setting up pair_data
I0429 17:11:11.550148  2016 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:11.550156  2016 net.cpp:129] Top shape: 100 (100)
I0429 17:11:11.550161  2016 net.cpp:137] Memory required for data: 627600
I0429 17:11:11.550168  2016 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:11.550179  2016 net.cpp:84] Creating Layer slice_pair
I0429 17:11:11.550185  2016 net.cpp:406] slice_pair <- pair_data
I0429 17:11:11.550194  2016 net.cpp:380] slice_pair -> data
I0429 17:11:11.550211  2016 net.cpp:380] slice_pair -> data_p
I0429 17:11:11.550225  2016 net.cpp:122] Setting up slice_pair
I0429 17:11:11.550235  2016 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:11.550240  2016 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:11.550246  2016 net.cpp:137] Memory required for data: 1254800
I0429 17:11:11.550251  2016 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:11.550266  2016 net.cpp:84] Creating Layer conv1
I0429 17:11:11.550271  2016 net.cpp:406] conv1 <- data
I0429 17:11:11.550281  2016 net.cpp:380] conv1 -> conv1
I0429 17:11:11.550328  2016 net.cpp:122] Setting up conv1
I0429 17:11:11.550338  2016 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:11.550343  2016 net.cpp:137] Memory required for data: 5862800
I0429 17:11:11.550354  2016 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:11.550366  2016 net.cpp:84] Creating Layer pool1
I0429 17:11:11.550371  2016 net.cpp:406] pool1 <- conv1
I0429 17:11:11.550379  2016 net.cpp:380] pool1 -> pool1
I0429 17:11:11.550391  2016 net.cpp:122] Setting up pool1
I0429 17:11:11.550398  2016 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:11.550405  2016 net.cpp:137] Memory required for data: 7014800
I0429 17:11:11.550408  2016 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:11.550423  2016 net.cpp:84] Creating Layer conv2
I0429 17:11:11.550429  2016 net.cpp:406] conv2 <- pool1
I0429 17:11:11.550438  2016 net.cpp:380] conv2 -> conv2
I0429 17:11:11.550724  2016 net.cpp:122] Setting up conv2
I0429 17:11:11.550734  2016 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:11.550740  2016 net.cpp:137] Memory required for data: 8294800
I0429 17:11:11.550750  2016 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:11.550758  2016 net.cpp:84] Creating Layer pool2
I0429 17:11:11.550763  2016 net.cpp:406] pool2 <- conv2
I0429 17:11:11.550772  2016 net.cpp:380] pool2 -> pool2
I0429 17:11:11.550783  2016 net.cpp:122] Setting up pool2
I0429 17:11:11.550791  2016 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:11.550796  2016 net.cpp:137] Memory required for data: 8614800
I0429 17:11:11.550801  2016 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:11.550810  2016 net.cpp:84] Creating Layer ip1
I0429 17:11:11.550815  2016 net.cpp:406] ip1 <- pool2
I0429 17:11:11.550824  2016 net.cpp:380] ip1 -> ip1
I0429 17:11:11.554908  2016 net.cpp:122] Setting up ip1
I0429 17:11:11.554920  2016 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:11.554924  2016 net.cpp:137] Memory required for data: 8814800
I0429 17:11:11.554935  2016 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:11.554944  2016 net.cpp:84] Creating Layer relu1
I0429 17:11:11.554949  2016 net.cpp:406] relu1 <- ip1
I0429 17:11:11.554957  2016 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:11.554966  2016 net.cpp:122] Setting up relu1
I0429 17:11:11.554972  2016 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:11.554976  2016 net.cpp:137] Memory required for data: 9014800
I0429 17:11:11.554981  2016 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:11.554991  2016 net.cpp:84] Creating Layer ip2
I0429 17:11:11.554996  2016 net.cpp:406] ip2 <- ip1
I0429 17:11:11.555006  2016 net.cpp:380] ip2 -> ip2
I0429 17:11:11.555073  2016 net.cpp:122] Setting up ip2
I0429 17:11:11.555079  2016 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:11.555083  2016 net.cpp:137] Memory required for data: 9018800
I0429 17:11:11.555090  2016 layer_factory.hpp:77] Creating layer feat
I0429 17:11:11.555097  2016 net.cpp:84] Creating Layer feat
I0429 17:11:11.555102  2016 net.cpp:406] feat <- ip2
I0429 17:11:11.555111  2016 net.cpp:380] feat -> feat
I0429 17:11:11.555125  2016 net.cpp:122] Setting up feat
I0429 17:11:11.555131  2016 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:11.555136  2016 net.cpp:137] Memory required for data: 9019600
I0429 17:11:11.555145  2016 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:11.555158  2016 net.cpp:84] Creating Layer conv1_p
I0429 17:11:11.555163  2016 net.cpp:406] conv1_p <- data_p
I0429 17:11:11.555176  2016 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:11.555208  2016 net.cpp:122] Setting up conv1_p
I0429 17:11:11.555217  2016 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:11.555222  2016 net.cpp:137] Memory required for data: 13627600
I0429 17:11:11.555228  2016 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:11.555235  2016 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:11.555240  2016 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:11.555248  2016 net.cpp:84] Creating Layer pool1_p
I0429 17:11:11.555253  2016 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:11.555258  2016 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:11.555269  2016 net.cpp:122] Setting up pool1_p
I0429 17:11:11.555276  2016 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:11.555281  2016 net.cpp:137] Memory required for data: 14779600
I0429 17:11:11.555285  2016 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:11.555299  2016 net.cpp:84] Creating Layer conv2_p
I0429 17:11:11.555305  2016 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:11.555312  2016 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:11.555579  2016 net.cpp:122] Setting up conv2_p
I0429 17:11:11.555588  2016 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:11.555593  2016 net.cpp:137] Memory required for data: 16059600
I0429 17:11:11.555598  2016 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:11.555603  2016 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:11.555608  2016 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:11.555615  2016 net.cpp:84] Creating Layer pool2_p
I0429 17:11:11.555620  2016 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:11.555629  2016 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:11.555639  2016 net.cpp:122] Setting up pool2_p
I0429 17:11:11.555644  2016 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:11.555649  2016 net.cpp:137] Memory required for data: 16379600
I0429 17:11:11.555654  2016 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:11.555661  2016 net.cpp:84] Creating Layer ip1_p
I0429 17:11:11.555665  2016 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:11.555675  2016 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:11.559604  2016 net.cpp:122] Setting up ip1_p
I0429 17:11:11.559614  2016 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:11.559618  2016 net.cpp:137] Memory required for data: 16579600
I0429 17:11:11.559624  2016 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:11.559629  2016 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:11.559635  2016 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:11.559641  2016 net.cpp:84] Creating Layer relu1_p
I0429 17:11:11.559646  2016 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:11.559654  2016 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:11.559662  2016 net.cpp:122] Setting up relu1_p
I0429 17:11:11.559669  2016 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:11.559672  2016 net.cpp:137] Memory required for data: 16779600
I0429 17:11:11.559676  2016 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:11.559687  2016 net.cpp:84] Creating Layer ip2_p
I0429 17:11:11.559691  2016 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:11.559700  2016 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:11.559767  2016 net.cpp:122] Setting up ip2_p
I0429 17:11:11.559773  2016 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:11.559777  2016 net.cpp:137] Memory required for data: 16783600
I0429 17:11:11.559784  2016 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:11.559790  2016 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:11.559795  2016 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:11.559804  2016 net.cpp:84] Creating Layer feat_p
I0429 17:11:11.559808  2016 net.cpp:406] feat_p <- ip2_p
I0429 17:11:11.559815  2016 net.cpp:380] feat_p -> feat_p
I0429 17:11:11.559834  2016 net.cpp:122] Setting up feat_p
I0429 17:11:11.559841  2016 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:11.559845  2016 net.cpp:137] Memory required for data: 16784400
I0429 17:11:11.559850  2016 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:11.559856  2016 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:11.559860  2016 layer_factory.hpp:77] Creating layer loss
I0429 17:11:11.559870  2016 net.cpp:84] Creating Layer loss
I0429 17:11:11.559875  2016 net.cpp:406] loss <- feat
I0429 17:11:11.559880  2016 net.cpp:406] loss <- feat_p
I0429 17:11:11.559886  2016 net.cpp:406] loss <- sim
I0429 17:11:11.559893  2016 net.cpp:380] loss -> loss
I0429 17:11:11.559906  2016 net.cpp:122] Setting up loss
I0429 17:11:11.559912  2016 net.cpp:129] Top shape: (1)
I0429 17:11:11.559916  2016 net.cpp:132]     with loss weight 1
I0429 17:11:11.559927  2016 net.cpp:137] Memory required for data: 16784404
I0429 17:11:11.559932  2016 net.cpp:198] loss needs backward computation.
I0429 17:11:11.559938  2016 net.cpp:198] feat_p needs backward computation.
I0429 17:11:11.559943  2016 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:11.559948  2016 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:11.559953  2016 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:11.559958  2016 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:11.559963  2016 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:11.559967  2016 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:11.559973  2016 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:11.559978  2016 net.cpp:198] feat needs backward computation.
I0429 17:11:11.559983  2016 net.cpp:198] ip2 needs backward computation.
I0429 17:11:11.559988  2016 net.cpp:198] relu1 needs backward computation.
I0429 17:11:11.559993  2016 net.cpp:198] ip1 needs backward computation.
I0429 17:11:11.559998  2016 net.cpp:198] pool2 needs backward computation.
I0429 17:11:11.560003  2016 net.cpp:198] conv2 needs backward computation.
I0429 17:11:11.560008  2016 net.cpp:198] pool1 needs backward computation.
I0429 17:11:11.560012  2016 net.cpp:198] conv1 needs backward computation.
I0429 17:11:11.560020  2016 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:11.560026  2016 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:11.560030  2016 net.cpp:242] This network produces output loss
I0429 17:11:11.560166  2016 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.

 Performance counter stats for 'python test_model.py examples/images/cat_gray.jpg' (5 runs):

     2,029,805,118      cycles:u                                                      ( +-  8.77% )
     2,418,424,148      instructions:u            #    1.19  insns per cycle          ( +-  0.31% )

       0.799604797 seconds time elapsed                                          ( +-  8.46% )

WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:12.253257  2050 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:12.253281  2050 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:12.253284  2050 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:12.254686  2050 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:12.254842  2050 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:12.254906  2050 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:12.255997  2050 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:12.256350  2050 net.cpp:84] Creating Layer pair_data
I0429 17:11:12.256373  2050 net.cpp:380] pair_data -> pair_data
I0429 17:11:12.256394  2050 net.cpp:380] pair_data -> sim
I0429 17:11:12.256433  2050 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:12.257375  2050 net.cpp:122] Setting up pair_data
I0429 17:11:12.257396  2050 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:12.257405  2050 net.cpp:129] Top shape: 100 (100)
I0429 17:11:12.257411  2050 net.cpp:137] Memory required for data: 627600
I0429 17:11:12.257418  2050 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:12.257432  2050 net.cpp:84] Creating Layer slice_pair
I0429 17:11:12.257439  2050 net.cpp:406] slice_pair <- pair_data
I0429 17:11:12.257450  2050 net.cpp:380] slice_pair -> data
I0429 17:11:12.257463  2050 net.cpp:380] slice_pair -> data_p
I0429 17:11:12.257479  2050 net.cpp:122] Setting up slice_pair
I0429 17:11:12.257489  2050 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:12.257498  2050 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:12.257503  2050 net.cpp:137] Memory required for data: 1254800
I0429 17:11:12.257509  2050 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:12.257526  2050 net.cpp:84] Creating Layer conv1
I0429 17:11:12.257534  2050 net.cpp:406] conv1 <- data
I0429 17:11:12.257544  2050 net.cpp:380] conv1 -> conv1
I0429 17:11:12.257603  2050 net.cpp:122] Setting up conv1
I0429 17:11:12.257616  2050 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:12.257622  2050 net.cpp:137] Memory required for data: 5862800
I0429 17:11:12.257634  2050 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:12.257647  2050 net.cpp:84] Creating Layer pool1
I0429 17:11:12.257654  2050 net.cpp:406] pool1 <- conv1
I0429 17:11:12.257663  2050 net.cpp:380] pool1 -> pool1
I0429 17:11:12.257678  2050 net.cpp:122] Setting up pool1
I0429 17:11:12.257686  2050 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:12.257692  2050 net.cpp:137] Memory required for data: 7014800
I0429 17:11:12.257699  2050 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:12.257717  2050 net.cpp:84] Creating Layer conv2
I0429 17:11:12.257724  2050 net.cpp:406] conv2 <- pool1
I0429 17:11:12.257735  2050 net.cpp:380] conv2 -> conv2
I0429 17:11:12.258116  2050 net.cpp:122] Setting up conv2
I0429 17:11:12.258131  2050 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:12.258137  2050 net.cpp:137] Memory required for data: 8294800
I0429 17:11:12.258152  2050 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:12.258163  2050 net.cpp:84] Creating Layer pool2
I0429 17:11:12.258169  2050 net.cpp:406] pool2 <- conv2
I0429 17:11:12.258180  2050 net.cpp:380] pool2 -> pool2
I0429 17:11:12.258195  2050 net.cpp:122] Setting up pool2
I0429 17:11:12.258204  2050 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:12.258209  2050 net.cpp:137] Memory required for data: 8614800
I0429 17:11:12.258215  2050 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:12.258227  2050 net.cpp:84] Creating Layer ip1
I0429 17:11:12.258236  2050 net.cpp:406] ip1 <- pool2
I0429 17:11:12.258249  2050 net.cpp:380] ip1 -> ip1
I0429 17:11:12.263353  2050 net.cpp:122] Setting up ip1
I0429 17:11:12.263370  2050 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:12.263384  2050 net.cpp:137] Memory required for data: 8814800
I0429 17:11:12.263403  2050 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:12.263413  2050 net.cpp:84] Creating Layer relu1
I0429 17:11:12.263422  2050 net.cpp:406] relu1 <- ip1
I0429 17:11:12.263432  2050 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:12.263443  2050 net.cpp:122] Setting up relu1
I0429 17:11:12.263451  2050 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:12.263458  2050 net.cpp:137] Memory required for data: 9014800
I0429 17:11:12.263463  2050 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:12.263475  2050 net.cpp:84] Creating Layer ip2
I0429 17:11:12.263481  2050 net.cpp:406] ip2 <- ip1
I0429 17:11:12.263494  2050 net.cpp:380] ip2 -> ip2
I0429 17:11:12.263576  2050 net.cpp:122] Setting up ip2
I0429 17:11:12.263586  2050 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:12.263592  2050 net.cpp:137] Memory required for data: 9018800
I0429 17:11:12.263600  2050 layer_factory.hpp:77] Creating layer feat
I0429 17:11:12.263610  2050 net.cpp:84] Creating Layer feat
I0429 17:11:12.263617  2050 net.cpp:406] feat <- ip2
I0429 17:11:12.263628  2050 net.cpp:380] feat -> feat
I0429 17:11:12.263646  2050 net.cpp:122] Setting up feat
I0429 17:11:12.263654  2050 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:12.263660  2050 net.cpp:137] Memory required for data: 9019600
I0429 17:11:12.263672  2050 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:12.263689  2050 net.cpp:84] Creating Layer conv1_p
I0429 17:11:12.263695  2050 net.cpp:406] conv1_p <- data_p
I0429 17:11:12.263705  2050 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:12.263746  2050 net.cpp:122] Setting up conv1_p
I0429 17:11:12.263758  2050 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:12.263764  2050 net.cpp:137] Memory required for data: 13627600
I0429 17:11:12.263772  2050 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:12.263782  2050 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:12.263788  2050 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:12.263797  2050 net.cpp:84] Creating Layer pool1_p
I0429 17:11:12.263804  2050 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:12.263813  2050 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:12.263828  2050 net.cpp:122] Setting up pool1_p
I0429 17:11:12.263835  2050 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:12.263841  2050 net.cpp:137] Memory required for data: 14779600
I0429 17:11:12.263847  2050 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:12.263864  2050 net.cpp:84] Creating Layer conv2_p
I0429 17:11:12.263870  2050 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:12.263880  2050 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:12.264236  2050 net.cpp:122] Setting up conv2_p
I0429 17:11:12.264250  2050 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:12.264256  2050 net.cpp:137] Memory required for data: 16059600
I0429 17:11:12.264263  2050 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:12.264271  2050 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:12.264288  2050 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:12.264299  2050 net.cpp:84] Creating Layer pool2_p
I0429 17:11:12.264305  2050 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:12.264315  2050 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:12.264328  2050 net.cpp:122] Setting up pool2_p
I0429 17:11:12.264336  2050 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:12.264341  2050 net.cpp:137] Memory required for data: 16379600
I0429 17:11:12.264346  2050 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:12.264356  2050 net.cpp:84] Creating Layer ip1_p
I0429 17:11:12.264363  2050 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:12.264372  2050 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:12.269121  2050 net.cpp:122] Setting up ip1_p
I0429 17:11:12.269134  2050 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:12.269145  2050 net.cpp:137] Memory required for data: 16579600
I0429 17:11:12.269151  2050 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:12.269158  2050 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:12.269165  2050 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:12.269176  2050 net.cpp:84] Creating Layer relu1_p
I0429 17:11:12.269181  2050 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:12.269189  2050 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:12.269201  2050 net.cpp:122] Setting up relu1_p
I0429 17:11:12.269208  2050 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:12.269213  2050 net.cpp:137] Memory required for data: 16779600
I0429 17:11:12.269218  2050 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:12.269232  2050 net.cpp:84] Creating Layer ip2_p
I0429 17:11:12.269238  2050 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:12.269246  2050 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:12.269325  2050 net.cpp:122] Setting up ip2_p
I0429 17:11:12.269335  2050 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:12.269340  2050 net.cpp:137] Memory required for data: 16783600
I0429 17:11:12.269349  2050 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:12.269356  2050 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:12.269362  2050 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:12.269373  2050 net.cpp:84] Creating Layer feat_p
I0429 17:11:12.269379  2050 net.cpp:406] feat_p <- ip2_p
I0429 17:11:12.269387  2050 net.cpp:380] feat_p -> feat_p
I0429 17:11:12.269403  2050 net.cpp:122] Setting up feat_p
I0429 17:11:12.269412  2050 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:12.269417  2050 net.cpp:137] Memory required for data: 16784400
I0429 17:11:12.269423  2050 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:12.269430  2050 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:12.269435  2050 layer_factory.hpp:77] Creating layer loss
I0429 17:11:12.269448  2050 net.cpp:84] Creating Layer loss
I0429 17:11:12.269454  2050 net.cpp:406] loss <- feat
I0429 17:11:12.269460  2050 net.cpp:406] loss <- feat_p
I0429 17:11:12.269467  2050 net.cpp:406] loss <- sim
I0429 17:11:12.269476  2050 net.cpp:380] loss -> loss
I0429 17:11:12.269492  2050 net.cpp:122] Setting up loss
I0429 17:11:12.269500  2050 net.cpp:129] Top shape: (1)
I0429 17:11:12.269505  2050 net.cpp:132]     with loss weight 1
I0429 17:11:12.269517  2050 net.cpp:137] Memory required for data: 16784404
I0429 17:11:12.269523  2050 net.cpp:198] loss needs backward computation.
I0429 17:11:12.269531  2050 net.cpp:198] feat_p needs backward computation.
I0429 17:11:12.269537  2050 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:12.269543  2050 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:12.269549  2050 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:12.269556  2050 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:12.269562  2050 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:12.269567  2050 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:12.269573  2050 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:12.269580  2050 net.cpp:198] feat needs backward computation.
I0429 17:11:12.269587  2050 net.cpp:198] ip2 needs backward computation.
I0429 17:11:12.269593  2050 net.cpp:198] relu1 needs backward computation.
I0429 17:11:12.269598  2050 net.cpp:198] ip1 needs backward computation.
I0429 17:11:12.269605  2050 net.cpp:198] pool2 needs backward computation.
I0429 17:11:12.269611  2050 net.cpp:198] conv2 needs backward computation.
I0429 17:11:12.269618  2050 net.cpp:198] pool1 needs backward computation.
I0429 17:11:12.269623  2050 net.cpp:198] conv1 needs backward computation.
I0429 17:11:12.269634  2050 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:12.269641  2050 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:12.269652  2050 net.cpp:242] This network produces output loss
I0429 17:11:12.269790  2050 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:12.973937  2084 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:12.973961  2084 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:12.973964  2084 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:12.975396  2084 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:12.975549  2084 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:12.975620  2084 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:12.976568  2084 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:12.976850  2084 net.cpp:84] Creating Layer pair_data
I0429 17:11:12.976866  2084 net.cpp:380] pair_data -> pair_data
I0429 17:11:12.976882  2084 net.cpp:380] pair_data -> sim
I0429 17:11:12.976915  2084 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:12.977710  2084 net.cpp:122] Setting up pair_data
I0429 17:11:12.977723  2084 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:12.977731  2084 net.cpp:129] Top shape: 100 (100)
I0429 17:11:12.977735  2084 net.cpp:137] Memory required for data: 627600
I0429 17:11:12.977741  2084 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:12.977752  2084 net.cpp:84] Creating Layer slice_pair
I0429 17:11:12.977758  2084 net.cpp:406] slice_pair <- pair_data
I0429 17:11:12.977766  2084 net.cpp:380] slice_pair -> data
I0429 17:11:12.977777  2084 net.cpp:380] slice_pair -> data_p
I0429 17:11:12.977790  2084 net.cpp:122] Setting up slice_pair
I0429 17:11:12.977798  2084 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:12.977804  2084 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:12.977809  2084 net.cpp:137] Memory required for data: 1254800
I0429 17:11:12.977814  2084 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:12.977828  2084 net.cpp:84] Creating Layer conv1
I0429 17:11:12.977833  2084 net.cpp:406] conv1 <- data
I0429 17:11:12.977841  2084 net.cpp:380] conv1 -> conv1
I0429 17:11:12.977886  2084 net.cpp:122] Setting up conv1
I0429 17:11:12.977896  2084 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:12.977900  2084 net.cpp:137] Memory required for data: 5862800
I0429 17:11:12.977912  2084 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:12.977922  2084 net.cpp:84] Creating Layer pool1
I0429 17:11:12.977928  2084 net.cpp:406] pool1 <- conv1
I0429 17:11:12.977936  2084 net.cpp:380] pool1 -> pool1
I0429 17:11:12.977947  2084 net.cpp:122] Setting up pool1
I0429 17:11:12.977954  2084 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:12.977959  2084 net.cpp:137] Memory required for data: 7014800
I0429 17:11:12.977972  2084 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:12.977987  2084 net.cpp:84] Creating Layer conv2
I0429 17:11:12.977993  2084 net.cpp:406] conv2 <- pool1
I0429 17:11:12.978001  2084 net.cpp:380] conv2 -> conv2
I0429 17:11:12.978286  2084 net.cpp:122] Setting up conv2
I0429 17:11:12.978296  2084 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:12.978301  2084 net.cpp:137] Memory required for data: 8294800
I0429 17:11:12.978310  2084 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:12.978319  2084 net.cpp:84] Creating Layer pool2
I0429 17:11:12.978324  2084 net.cpp:406] pool2 <- conv2
I0429 17:11:12.978332  2084 net.cpp:380] pool2 -> pool2
I0429 17:11:12.978343  2084 net.cpp:122] Setting up pool2
I0429 17:11:12.978349  2084 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:12.978354  2084 net.cpp:137] Memory required for data: 8614800
I0429 17:11:12.978358  2084 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:12.978369  2084 net.cpp:84] Creating Layer ip1
I0429 17:11:12.978374  2084 net.cpp:406] ip1 <- pool2
I0429 17:11:12.978382  2084 net.cpp:380] ip1 -> ip1
I0429 17:11:12.982627  2084 net.cpp:122] Setting up ip1
I0429 17:11:12.982642  2084 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:12.982647  2084 net.cpp:137] Memory required for data: 8814800
I0429 17:11:12.982658  2084 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:12.982667  2084 net.cpp:84] Creating Layer relu1
I0429 17:11:12.982672  2084 net.cpp:406] relu1 <- ip1
I0429 17:11:12.982681  2084 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:12.982692  2084 net.cpp:122] Setting up relu1
I0429 17:11:12.982697  2084 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:12.982702  2084 net.cpp:137] Memory required for data: 9014800
I0429 17:11:12.982707  2084 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:12.982715  2084 net.cpp:84] Creating Layer ip2
I0429 17:11:12.982720  2084 net.cpp:406] ip2 <- ip1
I0429 17:11:12.982730  2084 net.cpp:380] ip2 -> ip2
I0429 17:11:12.982800  2084 net.cpp:122] Setting up ip2
I0429 17:11:12.982807  2084 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:12.982813  2084 net.cpp:137] Memory required for data: 9018800
I0429 17:11:12.982821  2084 layer_factory.hpp:77] Creating layer feat
I0429 17:11:12.982830  2084 net.cpp:84] Creating Layer feat
I0429 17:11:12.982834  2084 net.cpp:406] feat <- ip2
I0429 17:11:12.982843  2084 net.cpp:380] feat -> feat
I0429 17:11:12.982858  2084 net.cpp:122] Setting up feat
I0429 17:11:12.982864  2084 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:12.982868  2084 net.cpp:137] Memory required for data: 9019600
I0429 17:11:12.982878  2084 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:12.982890  2084 net.cpp:84] Creating Layer conv1_p
I0429 17:11:12.982895  2084 net.cpp:406] conv1_p <- data_p
I0429 17:11:12.982904  2084 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:12.982936  2084 net.cpp:122] Setting up conv1_p
I0429 17:11:12.982946  2084 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:12.982951  2084 net.cpp:137] Memory required for data: 13627600
I0429 17:11:12.982957  2084 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:12.982964  2084 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:12.982969  2084 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:12.982976  2084 net.cpp:84] Creating Layer pool1_p
I0429 17:11:12.982981  2084 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:12.982987  2084 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:12.983000  2084 net.cpp:122] Setting up pool1_p
I0429 17:11:12.983006  2084 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:12.983011  2084 net.cpp:137] Memory required for data: 14779600
I0429 17:11:12.983016  2084 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:12.983027  2084 net.cpp:84] Creating Layer conv2_p
I0429 17:11:12.983033  2084 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:12.983042  2084 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:12.983361  2084 net.cpp:122] Setting up conv2_p
I0429 17:11:12.983372  2084 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:12.983377  2084 net.cpp:137] Memory required for data: 16059600
I0429 17:11:12.983383  2084 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:12.983389  2084 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:12.983395  2084 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:12.983402  2084 net.cpp:84] Creating Layer pool2_p
I0429 17:11:12.983407  2084 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:12.983417  2084 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:12.983428  2084 net.cpp:122] Setting up pool2_p
I0429 17:11:12.983435  2084 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:12.983440  2084 net.cpp:137] Memory required for data: 16379600
I0429 17:11:12.983444  2084 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:12.983453  2084 net.cpp:84] Creating Layer ip1_p
I0429 17:11:12.983458  2084 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:12.983467  2084 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:12.987449  2084 net.cpp:122] Setting up ip1_p
I0429 17:11:12.987460  2084 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:12.987464  2084 net.cpp:137] Memory required for data: 16579600
I0429 17:11:12.987469  2084 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:12.987475  2084 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:12.987480  2084 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:12.987488  2084 net.cpp:84] Creating Layer relu1_p
I0429 17:11:12.987491  2084 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:12.987499  2084 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:12.987507  2084 net.cpp:122] Setting up relu1_p
I0429 17:11:12.987514  2084 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:12.987519  2084 net.cpp:137] Memory required for data: 16779600
I0429 17:11:12.987522  2084 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:12.987534  2084 net.cpp:84] Creating Layer ip2_p
I0429 17:11:12.987538  2084 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:12.987545  2084 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:12.987609  2084 net.cpp:122] Setting up ip2_p
I0429 17:11:12.987617  2084 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:12.987622  2084 net.cpp:137] Memory required for data: 16783600
I0429 17:11:12.987628  2084 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:12.987634  2084 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:12.987639  2084 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:12.987649  2084 net.cpp:84] Creating Layer feat_p
I0429 17:11:12.987654  2084 net.cpp:406] feat_p <- ip2_p
I0429 17:11:12.987661  2084 net.cpp:380] feat_p -> feat_p
I0429 17:11:12.987674  2084 net.cpp:122] Setting up feat_p
I0429 17:11:12.987680  2084 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:12.987684  2084 net.cpp:137] Memory required for data: 16784400
I0429 17:11:12.987689  2084 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:12.987695  2084 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:12.987699  2084 layer_factory.hpp:77] Creating layer loss
I0429 17:11:12.987709  2084 net.cpp:84] Creating Layer loss
I0429 17:11:12.987713  2084 net.cpp:406] loss <- feat
I0429 17:11:12.987718  2084 net.cpp:406] loss <- feat_p
I0429 17:11:12.987725  2084 net.cpp:406] loss <- sim
I0429 17:11:12.987730  2084 net.cpp:380] loss -> loss
I0429 17:11:12.987745  2084 net.cpp:122] Setting up loss
I0429 17:11:12.987751  2084 net.cpp:129] Top shape: (1)
I0429 17:11:12.987754  2084 net.cpp:132]     with loss weight 1
I0429 17:11:12.987764  2084 net.cpp:137] Memory required for data: 16784404
I0429 17:11:12.987769  2084 net.cpp:198] loss needs backward computation.
I0429 17:11:12.987776  2084 net.cpp:198] feat_p needs backward computation.
I0429 17:11:12.987781  2084 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:12.987790  2084 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:12.987794  2084 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:12.987799  2084 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:12.987804  2084 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:12.987809  2084 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:12.987813  2084 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:12.987819  2084 net.cpp:198] feat needs backward computation.
I0429 17:11:12.987823  2084 net.cpp:198] ip2 needs backward computation.
I0429 17:11:12.987828  2084 net.cpp:198] relu1 needs backward computation.
I0429 17:11:12.987833  2084 net.cpp:198] ip1 needs backward computation.
I0429 17:11:12.987838  2084 net.cpp:198] pool2 needs backward computation.
I0429 17:11:12.987843  2084 net.cpp:198] conv2 needs backward computation.
I0429 17:11:12.987848  2084 net.cpp:198] pool1 needs backward computation.
I0429 17:11:12.987851  2084 net.cpp:198] conv1 needs backward computation.
I0429 17:11:12.987859  2084 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:12.987866  2084 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:12.987870  2084 net.cpp:242] This network produces output loss
I0429 17:11:12.987985  2084 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:14.031827  2116 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:14.031853  2116 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:14.031859  2116 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:14.034118  2116 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:14.034359  2116 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:14.034456  2116 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:14.035629  2116 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:14.035970  2116 net.cpp:84] Creating Layer pair_data
I0429 17:11:14.035989  2116 net.cpp:380] pair_data -> pair_data
I0429 17:11:14.036007  2116 net.cpp:380] pair_data -> sim
I0429 17:11:14.036034  2116 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:14.036967  2116 net.cpp:122] Setting up pair_data
I0429 17:11:14.036993  2116 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:14.037003  2116 net.cpp:129] Top shape: 100 (100)
I0429 17:11:14.037008  2116 net.cpp:137] Memory required for data: 627600
I0429 17:11:14.037014  2116 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:14.037026  2116 net.cpp:84] Creating Layer slice_pair
I0429 17:11:14.037034  2116 net.cpp:406] slice_pair <- pair_data
I0429 17:11:14.037051  2116 net.cpp:380] slice_pair -> data
I0429 17:11:14.037065  2116 net.cpp:380] slice_pair -> data_p
I0429 17:11:14.037081  2116 net.cpp:122] Setting up slice_pair
I0429 17:11:14.037091  2116 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:14.037098  2116 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:14.037104  2116 net.cpp:137] Memory required for data: 1254800
I0429 17:11:14.037111  2116 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:14.037130  2116 net.cpp:84] Creating Layer conv1
I0429 17:11:14.037137  2116 net.cpp:406] conv1 <- data
I0429 17:11:14.037147  2116 net.cpp:380] conv1 -> conv1
I0429 17:11:14.037196  2116 net.cpp:122] Setting up conv1
I0429 17:11:14.037207  2116 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:14.037214  2116 net.cpp:137] Memory required for data: 5862800
I0429 17:11:14.037226  2116 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:14.037240  2116 net.cpp:84] Creating Layer pool1
I0429 17:11:14.037245  2116 net.cpp:406] pool1 <- conv1
I0429 17:11:14.037255  2116 net.cpp:380] pool1 -> pool1
I0429 17:11:14.037269  2116 net.cpp:122] Setting up pool1
I0429 17:11:14.037277  2116 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:14.037283  2116 net.cpp:137] Memory required for data: 7014800
I0429 17:11:14.037289  2116 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:14.037305  2116 net.cpp:84] Creating Layer conv2
I0429 17:11:14.037312  2116 net.cpp:406] conv2 <- pool1
I0429 17:11:14.037322  2116 net.cpp:380] conv2 -> conv2
I0429 17:11:14.037647  2116 net.cpp:122] Setting up conv2
I0429 17:11:14.037657  2116 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:14.037663  2116 net.cpp:137] Memory required for data: 8294800
I0429 17:11:14.037674  2116 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:14.037684  2116 net.cpp:84] Creating Layer pool2
I0429 17:11:14.037691  2116 net.cpp:406] pool2 <- conv2
I0429 17:11:14.037701  2116 net.cpp:380] pool2 -> pool2
I0429 17:11:14.037714  2116 net.cpp:122] Setting up pool2
I0429 17:11:14.037722  2116 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:14.037727  2116 net.cpp:137] Memory required for data: 8614800
I0429 17:11:14.037732  2116 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:14.037744  2116 net.cpp:84] Creating Layer ip1
I0429 17:11:14.037750  2116 net.cpp:406] ip1 <- pool2
I0429 17:11:14.037760  2116 net.cpp:380] ip1 -> ip1
I0429 17:11:14.042637  2116 net.cpp:122] Setting up ip1
I0429 17:11:14.042654  2116 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:14.042659  2116 net.cpp:137] Memory required for data: 8814800
I0429 17:11:14.042676  2116 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:14.042686  2116 net.cpp:84] Creating Layer relu1
I0429 17:11:14.042693  2116 net.cpp:406] relu1 <- ip1
I0429 17:11:14.042704  2116 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:14.042714  2116 net.cpp:122] Setting up relu1
I0429 17:11:14.042721  2116 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:14.042727  2116 net.cpp:137] Memory required for data: 9014800
I0429 17:11:14.042733  2116 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:14.042743  2116 net.cpp:84] Creating Layer ip2
I0429 17:11:14.042749  2116 net.cpp:406] ip2 <- ip1
I0429 17:11:14.042762  2116 net.cpp:380] ip2 -> ip2
I0429 17:11:14.042839  2116 net.cpp:122] Setting up ip2
I0429 17:11:14.042846  2116 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:14.042851  2116 net.cpp:137] Memory required for data: 9018800
I0429 17:11:14.042860  2116 layer_factory.hpp:77] Creating layer feat
I0429 17:11:14.042868  2116 net.cpp:84] Creating Layer feat
I0429 17:11:14.042875  2116 net.cpp:406] feat <- ip2
I0429 17:11:14.042884  2116 net.cpp:380] feat -> feat
I0429 17:11:14.042903  2116 net.cpp:122] Setting up feat
I0429 17:11:14.042910  2116 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:14.042917  2116 net.cpp:137] Memory required for data: 9019600
I0429 17:11:14.042928  2116 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:14.042943  2116 net.cpp:84] Creating Layer conv1_p
I0429 17:11:14.042949  2116 net.cpp:406] conv1_p <- data_p
I0429 17:11:14.042966  2116 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:14.043005  2116 net.cpp:122] Setting up conv1_p
I0429 17:11:14.043017  2116 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:14.043023  2116 net.cpp:137] Memory required for data: 13627600
I0429 17:11:14.043030  2116 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:14.043036  2116 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:14.043043  2116 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:14.043052  2116 net.cpp:84] Creating Layer pool1_p
I0429 17:11:14.043058  2116 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:14.043066  2116 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:14.043079  2116 net.cpp:122] Setting up pool1_p
I0429 17:11:14.043087  2116 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:14.043093  2116 net.cpp:137] Memory required for data: 14779600
I0429 17:11:14.043099  2116 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:14.043115  2116 net.cpp:84] Creating Layer conv2_p
I0429 17:11:14.043123  2116 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:14.043131  2116 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:14.043452  2116 net.cpp:122] Setting up conv2_p
I0429 17:11:14.043463  2116 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:14.043468  2116 net.cpp:137] Memory required for data: 16059600
I0429 17:11:14.043475  2116 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:14.043481  2116 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:14.043488  2116 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:14.043498  2116 net.cpp:84] Creating Layer pool2_p
I0429 17:11:14.043505  2116 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:14.043515  2116 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:14.043527  2116 net.cpp:122] Setting up pool2_p
I0429 17:11:14.043535  2116 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:14.043540  2116 net.cpp:137] Memory required for data: 16379600
I0429 17:11:14.043545  2116 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:14.043556  2116 net.cpp:84] Creating Layer ip1_p
I0429 17:11:14.043561  2116 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:14.043572  2116 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:14.048343  2116 net.cpp:122] Setting up ip1_p
I0429 17:11:14.048355  2116 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:14.048360  2116 net.cpp:137] Memory required for data: 16579600
I0429 17:11:14.048367  2116 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:14.048373  2116 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:14.048379  2116 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:14.048389  2116 net.cpp:84] Creating Layer relu1_p
I0429 17:11:14.048394  2116 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:14.048403  2116 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:14.048420  2116 net.cpp:122] Setting up relu1_p
I0429 17:11:14.048427  2116 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:14.048432  2116 net.cpp:137] Memory required for data: 16779600
I0429 17:11:14.048437  2116 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:14.048449  2116 net.cpp:84] Creating Layer ip2_p
I0429 17:11:14.048455  2116 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:14.048463  2116 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:14.048537  2116 net.cpp:122] Setting up ip2_p
I0429 17:11:14.048545  2116 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:14.048550  2116 net.cpp:137] Memory required for data: 16783600
I0429 17:11:14.048559  2116 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:14.048565  2116 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:14.048571  2116 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:14.048581  2116 net.cpp:84] Creating Layer feat_p
I0429 17:11:14.048588  2116 net.cpp:406] feat_p <- ip2_p
I0429 17:11:14.048600  2116 net.cpp:380] feat_p -> feat_p
I0429 17:11:14.048616  2116 net.cpp:122] Setting up feat_p
I0429 17:11:14.048624  2116 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:14.048629  2116 net.cpp:137] Memory required for data: 16784400
I0429 17:11:14.048633  2116 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:14.048640  2116 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:14.048646  2116 layer_factory.hpp:77] Creating layer loss
I0429 17:11:14.048656  2116 net.cpp:84] Creating Layer loss
I0429 17:11:14.048661  2116 net.cpp:406] loss <- feat
I0429 17:11:14.048668  2116 net.cpp:406] loss <- feat_p
I0429 17:11:14.048676  2116 net.cpp:406] loss <- sim
I0429 17:11:14.048683  2116 net.cpp:380] loss -> loss
I0429 17:11:14.048698  2116 net.cpp:122] Setting up loss
I0429 17:11:14.048705  2116 net.cpp:129] Top shape: (1)
I0429 17:11:14.048710  2116 net.cpp:132]     with loss weight 1
I0429 17:11:14.048722  2116 net.cpp:137] Memory required for data: 16784404
I0429 17:11:14.048727  2116 net.cpp:198] loss needs backward computation.
I0429 17:11:14.048735  2116 net.cpp:198] feat_p needs backward computation.
I0429 17:11:14.048740  2116 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:14.048746  2116 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:14.048751  2116 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:14.048758  2116 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:14.048763  2116 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:14.048768  2116 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:14.048774  2116 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:14.048780  2116 net.cpp:198] feat needs backward computation.
I0429 17:11:14.048785  2116 net.cpp:198] ip2 needs backward computation.
I0429 17:11:14.048790  2116 net.cpp:198] relu1 needs backward computation.
I0429 17:11:14.048796  2116 net.cpp:198] ip1 needs backward computation.
I0429 17:11:14.048802  2116 net.cpp:198] pool2 needs backward computation.
I0429 17:11:14.048807  2116 net.cpp:198] conv2 needs backward computation.
I0429 17:11:14.048812  2116 net.cpp:198] pool1 needs backward computation.
I0429 17:11:14.048818  2116 net.cpp:198] conv1 needs backward computation.
I0429 17:11:14.048827  2116 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:14.048835  2116 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:14.048840  2116 net.cpp:242] This network produces output loss
I0429 17:11:14.048976  2116 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:14.797344  2145 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:14.797370  2145 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:14.797374  2145 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:14.798801  2145 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:14.798951  2145 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:14.799024  2145 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:14.799885  2145 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:14.800097  2145 net.cpp:84] Creating Layer pair_data
I0429 17:11:14.800109  2145 net.cpp:380] pair_data -> pair_data
I0429 17:11:14.800120  2145 net.cpp:380] pair_data -> sim
I0429 17:11:14.800143  2145 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:14.800721  2145 net.cpp:122] Setting up pair_data
I0429 17:11:14.800748  2145 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:14.800753  2145 net.cpp:129] Top shape: 100 (100)
I0429 17:11:14.800756  2145 net.cpp:137] Memory required for data: 627600
I0429 17:11:14.800760  2145 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:14.800768  2145 net.cpp:84] Creating Layer slice_pair
I0429 17:11:14.800772  2145 net.cpp:406] slice_pair <- pair_data
I0429 17:11:14.800777  2145 net.cpp:380] slice_pair -> data
I0429 17:11:14.800784  2145 net.cpp:380] slice_pair -> data_p
I0429 17:11:14.800792  2145 net.cpp:122] Setting up slice_pair
I0429 17:11:14.800797  2145 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:14.800801  2145 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:14.800804  2145 net.cpp:137] Memory required for data: 1254800
I0429 17:11:14.800807  2145 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:14.800817  2145 net.cpp:84] Creating Layer conv1
I0429 17:11:14.800820  2145 net.cpp:406] conv1 <- data
I0429 17:11:14.800827  2145 net.cpp:380] conv1 -> conv1
I0429 17:11:14.800858  2145 net.cpp:122] Setting up conv1
I0429 17:11:14.800863  2145 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:14.800868  2145 net.cpp:137] Memory required for data: 5862800
I0429 17:11:14.800874  2145 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:14.800881  2145 net.cpp:84] Creating Layer pool1
I0429 17:11:14.800886  2145 net.cpp:406] pool1 <- conv1
I0429 17:11:14.800892  2145 net.cpp:380] pool1 -> pool1
I0429 17:11:14.800900  2145 net.cpp:122] Setting up pool1
I0429 17:11:14.800904  2145 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:14.800907  2145 net.cpp:137] Memory required for data: 7014800
I0429 17:11:14.800910  2145 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:14.800921  2145 net.cpp:84] Creating Layer conv2
I0429 17:11:14.800925  2145 net.cpp:406] conv2 <- pool1
I0429 17:11:14.800930  2145 net.cpp:380] conv2 -> conv2
I0429 17:11:14.801110  2145 net.cpp:122] Setting up conv2
I0429 17:11:14.801116  2145 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:14.801120  2145 net.cpp:137] Memory required for data: 8294800
I0429 17:11:14.801126  2145 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:14.801132  2145 net.cpp:84] Creating Layer pool2
I0429 17:11:14.801136  2145 net.cpp:406] pool2 <- conv2
I0429 17:11:14.801141  2145 net.cpp:380] pool2 -> pool2
I0429 17:11:14.801148  2145 net.cpp:122] Setting up pool2
I0429 17:11:14.801153  2145 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:14.801156  2145 net.cpp:137] Memory required for data: 8614800
I0429 17:11:14.801159  2145 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:14.801165  2145 net.cpp:84] Creating Layer ip1
I0429 17:11:14.801168  2145 net.cpp:406] ip1 <- pool2
I0429 17:11:14.801174  2145 net.cpp:380] ip1 -> ip1
I0429 17:11:14.803933  2145 net.cpp:122] Setting up ip1
I0429 17:11:14.803941  2145 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:14.803944  2145 net.cpp:137] Memory required for data: 8814800
I0429 17:11:14.803951  2145 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:14.803957  2145 net.cpp:84] Creating Layer relu1
I0429 17:11:14.803968  2145 net.cpp:406] relu1 <- ip1
I0429 17:11:14.803974  2145 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:14.803984  2145 net.cpp:122] Setting up relu1
I0429 17:11:14.803989  2145 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:14.803992  2145 net.cpp:137] Memory required for data: 9014800
I0429 17:11:14.803995  2145 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:14.804003  2145 net.cpp:84] Creating Layer ip2
I0429 17:11:14.804006  2145 net.cpp:406] ip2 <- ip1
I0429 17:11:14.804011  2145 net.cpp:380] ip2 -> ip2
I0429 17:11:14.804107  2145 net.cpp:122] Setting up ip2
I0429 17:11:14.804116  2145 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:14.804118  2145 net.cpp:137] Memory required for data: 9018800
I0429 17:11:14.804123  2145 layer_factory.hpp:77] Creating layer feat
I0429 17:11:14.804129  2145 net.cpp:84] Creating Layer feat
I0429 17:11:14.804132  2145 net.cpp:406] feat <- ip2
I0429 17:11:14.804139  2145 net.cpp:380] feat -> feat
I0429 17:11:14.804150  2145 net.cpp:122] Setting up feat
I0429 17:11:14.804154  2145 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:14.804157  2145 net.cpp:137] Memory required for data: 9019600
I0429 17:11:14.804163  2145 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:14.804172  2145 net.cpp:84] Creating Layer conv1_p
I0429 17:11:14.804177  2145 net.cpp:406] conv1_p <- data_p
I0429 17:11:14.804181  2145 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:14.804208  2145 net.cpp:122] Setting up conv1_p
I0429 17:11:14.804213  2145 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:14.804216  2145 net.cpp:137] Memory required for data: 13627600
I0429 17:11:14.804219  2145 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:14.804224  2145 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:14.804227  2145 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:14.804234  2145 net.cpp:84] Creating Layer pool1_p
I0429 17:11:14.804236  2145 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:14.804241  2145 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:14.804250  2145 net.cpp:122] Setting up pool1_p
I0429 17:11:14.804255  2145 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:14.804257  2145 net.cpp:137] Memory required for data: 14779600
I0429 17:11:14.804260  2145 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:14.804267  2145 net.cpp:84] Creating Layer conv2_p
I0429 17:11:14.804271  2145 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:14.804276  2145 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:14.804477  2145 net.cpp:122] Setting up conv2_p
I0429 17:11:14.804483  2145 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:14.804486  2145 net.cpp:137] Memory required for data: 16059600
I0429 17:11:14.804491  2145 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:14.804493  2145 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:14.804497  2145 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:14.804502  2145 net.cpp:84] Creating Layer pool2_p
I0429 17:11:14.804505  2145 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:14.804510  2145 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:14.804517  2145 net.cpp:122] Setting up pool2_p
I0429 17:11:14.804522  2145 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:14.804524  2145 net.cpp:137] Memory required for data: 16379600
I0429 17:11:14.804527  2145 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:14.804533  2145 net.cpp:84] Creating Layer ip1_p
I0429 17:11:14.804538  2145 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:14.804541  2145 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:14.807261  2145 net.cpp:122] Setting up ip1_p
I0429 17:11:14.807268  2145 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:14.807271  2145 net.cpp:137] Memory required for data: 16579600
I0429 17:11:14.807276  2145 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:14.807278  2145 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:14.807282  2145 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:14.807291  2145 net.cpp:84] Creating Layer relu1_p
I0429 17:11:14.807294  2145 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:14.807299  2145 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:14.807306  2145 net.cpp:122] Setting up relu1_p
I0429 17:11:14.807310  2145 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:14.807313  2145 net.cpp:137] Memory required for data: 16779600
I0429 17:11:14.807315  2145 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:14.807323  2145 net.cpp:84] Creating Layer ip2_p
I0429 17:11:14.807327  2145 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:14.807332  2145 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:14.807376  2145 net.cpp:122] Setting up ip2_p
I0429 17:11:14.807382  2145 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:14.807385  2145 net.cpp:137] Memory required for data: 16783600
I0429 17:11:14.807390  2145 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:14.807394  2145 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:14.807397  2145 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:14.807404  2145 net.cpp:84] Creating Layer feat_p
I0429 17:11:14.807406  2145 net.cpp:406] feat_p <- ip2_p
I0429 17:11:14.807411  2145 net.cpp:380] feat_p -> feat_p
I0429 17:11:14.807420  2145 net.cpp:122] Setting up feat_p
I0429 17:11:14.807436  2145 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:14.807440  2145 net.cpp:137] Memory required for data: 16784400
I0429 17:11:14.807442  2145 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:14.807446  2145 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:14.807449  2145 layer_factory.hpp:77] Creating layer loss
I0429 17:11:14.807456  2145 net.cpp:84] Creating Layer loss
I0429 17:11:14.807459  2145 net.cpp:406] loss <- feat
I0429 17:11:14.807463  2145 net.cpp:406] loss <- feat_p
I0429 17:11:14.807467  2145 net.cpp:406] loss <- sim
I0429 17:11:14.807472  2145 net.cpp:380] loss -> loss
I0429 17:11:14.807482  2145 net.cpp:122] Setting up loss
I0429 17:11:14.807487  2145 net.cpp:129] Top shape: (1)
I0429 17:11:14.807489  2145 net.cpp:132]     with loss weight 1
I0429 17:11:14.807497  2145 net.cpp:137] Memory required for data: 16784404
I0429 17:11:14.807499  2145 net.cpp:198] loss needs backward computation.
I0429 17:11:14.807503  2145 net.cpp:198] feat_p needs backward computation.
I0429 17:11:14.807507  2145 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:14.807510  2145 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:14.807513  2145 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:14.807516  2145 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:14.807519  2145 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:14.807523  2145 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:14.807526  2145 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:14.807529  2145 net.cpp:198] feat needs backward computation.
I0429 17:11:14.807533  2145 net.cpp:198] ip2 needs backward computation.
I0429 17:11:14.807536  2145 net.cpp:198] relu1 needs backward computation.
I0429 17:11:14.807538  2145 net.cpp:198] ip1 needs backward computation.
I0429 17:11:14.807543  2145 net.cpp:198] pool2 needs backward computation.
I0429 17:11:14.807545  2145 net.cpp:198] conv2 needs backward computation.
I0429 17:11:14.807548  2145 net.cpp:198] pool1 needs backward computation.
I0429 17:11:14.807551  2145 net.cpp:198] conv1 needs backward computation.
I0429 17:11:14.807557  2145 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:14.807561  2145 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:14.807564  2145 net.cpp:242] This network produces output loss
I0429 17:11:14.807642  2145 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:15.866874  2185 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:15.866904  2185 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:15.866909  2185 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:15.869194  2185 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:15.869441  2185 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:15.869542  2185 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:15.870707  2185 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:15.871049  2185 net.cpp:84] Creating Layer pair_data
I0429 17:11:15.871068  2185 net.cpp:380] pair_data -> pair_data
I0429 17:11:15.871088  2185 net.cpp:380] pair_data -> sim
I0429 17:11:15.871120  2185 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:15.872098  2185 net.cpp:122] Setting up pair_data
I0429 17:11:15.872117  2185 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:15.872128  2185 net.cpp:129] Top shape: 100 (100)
I0429 17:11:15.872133  2185 net.cpp:137] Memory required for data: 627600
I0429 17:11:15.872139  2185 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:15.872154  2185 net.cpp:84] Creating Layer slice_pair
I0429 17:11:15.872161  2185 net.cpp:406] slice_pair <- pair_data
I0429 17:11:15.872171  2185 net.cpp:380] slice_pair -> data
I0429 17:11:15.872184  2185 net.cpp:380] slice_pair -> data_p
I0429 17:11:15.872212  2185 net.cpp:122] Setting up slice_pair
I0429 17:11:15.872222  2185 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:15.872231  2185 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:15.872236  2185 net.cpp:137] Memory required for data: 1254800
I0429 17:11:15.872241  2185 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:15.872258  2185 net.cpp:84] Creating Layer conv1
I0429 17:11:15.872264  2185 net.cpp:406] conv1 <- data
I0429 17:11:15.872274  2185 net.cpp:380] conv1 -> conv1
I0429 17:11:15.872323  2185 net.cpp:122] Setting up conv1
I0429 17:11:15.872334  2185 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:15.872339  2185 net.cpp:137] Memory required for data: 5862800
I0429 17:11:15.872352  2185 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:15.872364  2185 net.cpp:84] Creating Layer pool1
I0429 17:11:15.872382  2185 net.cpp:406] pool1 <- conv1
I0429 17:11:15.872390  2185 net.cpp:380] pool1 -> pool1
I0429 17:11:15.872406  2185 net.cpp:122] Setting up pool1
I0429 17:11:15.872412  2185 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:15.872418  2185 net.cpp:137] Memory required for data: 7014800
I0429 17:11:15.872423  2185 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:15.872442  2185 net.cpp:84] Creating Layer conv2
I0429 17:11:15.872448  2185 net.cpp:406] conv2 <- pool1
I0429 17:11:15.872458  2185 net.cpp:380] conv2 -> conv2
I0429 17:11:15.872794  2185 net.cpp:122] Setting up conv2
I0429 17:11:15.872812  2185 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:15.872818  2185 net.cpp:137] Memory required for data: 8294800
I0429 17:11:15.872830  2185 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:15.872840  2185 net.cpp:84] Creating Layer pool2
I0429 17:11:15.872846  2185 net.cpp:406] pool2 <- conv2
I0429 17:11:15.872856  2185 net.cpp:380] pool2 -> pool2
I0429 17:11:15.872869  2185 net.cpp:122] Setting up pool2
I0429 17:11:15.872876  2185 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:15.872881  2185 net.cpp:137] Memory required for data: 8614800
I0429 17:11:15.872886  2185 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:15.872897  2185 net.cpp:84] Creating Layer ip1
I0429 17:11:15.872902  2185 net.cpp:406] ip1 <- pool2
I0429 17:11:15.872913  2185 net.cpp:380] ip1 -> ip1
I0429 17:11:15.877758  2185 net.cpp:122] Setting up ip1
I0429 17:11:15.877774  2185 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:15.877779  2185 net.cpp:137] Memory required for data: 8814800
I0429 17:11:15.877794  2185 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:15.877804  2185 net.cpp:84] Creating Layer relu1
I0429 17:11:15.877810  2185 net.cpp:406] relu1 <- ip1
I0429 17:11:15.877820  2185 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:15.877830  2185 net.cpp:122] Setting up relu1
I0429 17:11:15.877837  2185 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:15.877843  2185 net.cpp:137] Memory required for data: 9014800
I0429 17:11:15.877848  2185 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:15.877859  2185 net.cpp:84] Creating Layer ip2
I0429 17:11:15.877864  2185 net.cpp:406] ip2 <- ip1
I0429 17:11:15.877876  2185 net.cpp:380] ip2 -> ip2
I0429 17:11:15.877954  2185 net.cpp:122] Setting up ip2
I0429 17:11:15.877962  2185 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:15.877967  2185 net.cpp:137] Memory required for data: 9018800
I0429 17:11:15.877976  2185 layer_factory.hpp:77] Creating layer feat
I0429 17:11:15.877985  2185 net.cpp:84] Creating Layer feat
I0429 17:11:15.877990  2185 net.cpp:406] feat <- ip2
I0429 17:11:15.878001  2185 net.cpp:380] feat -> feat
I0429 17:11:15.878017  2185 net.cpp:122] Setting up feat
I0429 17:11:15.878024  2185 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:15.878029  2185 net.cpp:137] Memory required for data: 9019600
I0429 17:11:15.878041  2185 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:15.878056  2185 net.cpp:84] Creating Layer conv1_p
I0429 17:11:15.878062  2185 net.cpp:406] conv1_p <- data_p
I0429 17:11:15.878070  2185 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:15.878108  2185 net.cpp:122] Setting up conv1_p
I0429 17:11:15.878119  2185 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:15.878125  2185 net.cpp:137] Memory required for data: 13627600
I0429 17:11:15.878131  2185 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:15.878139  2185 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:15.878144  2185 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:15.878152  2185 net.cpp:84] Creating Layer pool1_p
I0429 17:11:15.878159  2185 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:15.878165  2185 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:15.878177  2185 net.cpp:122] Setting up pool1_p
I0429 17:11:15.878185  2185 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:15.878190  2185 net.cpp:137] Memory required for data: 14779600
I0429 17:11:15.878196  2185 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:15.878211  2185 net.cpp:84] Creating Layer conv2_p
I0429 17:11:15.878217  2185 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:15.878228  2185 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:15.878573  2185 net.cpp:122] Setting up conv2_p
I0429 17:11:15.878587  2185 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:15.878592  2185 net.cpp:137] Memory required for data: 16059600
I0429 17:11:15.878597  2185 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:15.878604  2185 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:15.878618  2185 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:15.878629  2185 net.cpp:84] Creating Layer pool2_p
I0429 17:11:15.878635  2185 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:15.878645  2185 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:15.878659  2185 net.cpp:122] Setting up pool2_p
I0429 17:11:15.878667  2185 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:15.878672  2185 net.cpp:137] Memory required for data: 16379600
I0429 17:11:15.878677  2185 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:15.878687  2185 net.cpp:84] Creating Layer ip1_p
I0429 17:11:15.878693  2185 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:15.878703  2185 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:15.883491  2185 net.cpp:122] Setting up ip1_p
I0429 17:11:15.883502  2185 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:15.883507  2185 net.cpp:137] Memory required for data: 16579600
I0429 17:11:15.883513  2185 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:15.883519  2185 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:15.883525  2185 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:15.883535  2185 net.cpp:84] Creating Layer relu1_p
I0429 17:11:15.883541  2185 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:15.883548  2185 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:15.883560  2185 net.cpp:122] Setting up relu1_p
I0429 17:11:15.883566  2185 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:15.883571  2185 net.cpp:137] Memory required for data: 16779600
I0429 17:11:15.883576  2185 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:15.883589  2185 net.cpp:84] Creating Layer ip2_p
I0429 17:11:15.883595  2185 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:15.883604  2185 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:15.883682  2185 net.cpp:122] Setting up ip2_p
I0429 17:11:15.883692  2185 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:15.883697  2185 net.cpp:137] Memory required for data: 16783600
I0429 17:11:15.883704  2185 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:15.883711  2185 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:15.883718  2185 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:15.883729  2185 net.cpp:84] Creating Layer feat_p
I0429 17:11:15.883735  2185 net.cpp:406] feat_p <- ip2_p
I0429 17:11:15.883744  2185 net.cpp:380] feat_p -> feat_p
I0429 17:11:15.883759  2185 net.cpp:122] Setting up feat_p
I0429 17:11:15.883767  2185 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:15.883771  2185 net.cpp:137] Memory required for data: 16784400
I0429 17:11:15.883777  2185 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:15.883785  2185 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:15.883790  2185 layer_factory.hpp:77] Creating layer loss
I0429 17:11:15.883800  2185 net.cpp:84] Creating Layer loss
I0429 17:11:15.883806  2185 net.cpp:406] loss <- feat
I0429 17:11:15.883813  2185 net.cpp:406] loss <- feat_p
I0429 17:11:15.883819  2185 net.cpp:406] loss <- sim
I0429 17:11:15.883827  2185 net.cpp:380] loss -> loss
I0429 17:11:15.883843  2185 net.cpp:122] Setting up loss
I0429 17:11:15.883852  2185 net.cpp:129] Top shape: (1)
I0429 17:11:15.883857  2185 net.cpp:132]     with loss weight 1
I0429 17:11:15.883867  2185 net.cpp:137] Memory required for data: 16784404
I0429 17:11:15.883873  2185 net.cpp:198] loss needs backward computation.
I0429 17:11:15.883880  2185 net.cpp:198] feat_p needs backward computation.
I0429 17:11:15.883886  2185 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:15.883893  2185 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:15.883898  2185 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:15.883903  2185 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:15.883909  2185 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:15.883920  2185 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:15.883926  2185 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:15.883934  2185 net.cpp:198] feat needs backward computation.
I0429 17:11:15.883939  2185 net.cpp:198] ip2 needs backward computation.
I0429 17:11:15.883944  2185 net.cpp:198] relu1 needs backward computation.
I0429 17:11:15.883949  2185 net.cpp:198] ip1 needs backward computation.
I0429 17:11:15.883955  2185 net.cpp:198] pool2 needs backward computation.
I0429 17:11:15.883961  2185 net.cpp:198] conv2 needs backward computation.
I0429 17:11:15.883966  2185 net.cpp:198] pool1 needs backward computation.
I0429 17:11:15.883971  2185 net.cpp:198] conv1 needs backward computation.
I0429 17:11:15.883981  2185 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:15.883988  2185 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:15.883993  2185 net.cpp:242] This network produces output loss
I0429 17:11:15.884151  2185 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.

 Performance counter stats for 'python test_model.py examples/images/cat_gray.jpg' (5 runs):

     2,240,652,929      cycles:u                                                      ( +-  9.76% )
     2,420,842,311      instructions:u            #    1.08  insns per cycle          ( +-  0.10% )

       0.865735479 seconds time elapsed                                          ( +- 10.33% )

WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:16.610481  2221 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:16.610507  2221 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:16.610509  2221 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:16.611999  2221 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:16.612186  2221 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:16.612268  2221 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:16.613283  2221 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:16.613584  2221 net.cpp:84] Creating Layer pair_data
I0429 17:11:16.613600  2221 net.cpp:380] pair_data -> pair_data
I0429 17:11:16.613620  2221 net.cpp:380] pair_data -> sim
I0429 17:11:16.613649  2221 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:16.614492  2221 net.cpp:122] Setting up pair_data
I0429 17:11:16.614508  2221 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:16.614516  2221 net.cpp:129] Top shape: 100 (100)
I0429 17:11:16.614521  2221 net.cpp:137] Memory required for data: 627600
I0429 17:11:16.614527  2221 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:16.614539  2221 net.cpp:84] Creating Layer slice_pair
I0429 17:11:16.614545  2221 net.cpp:406] slice_pair <- pair_data
I0429 17:11:16.614562  2221 net.cpp:380] slice_pair -> data
I0429 17:11:16.614574  2221 net.cpp:380] slice_pair -> data_p
I0429 17:11:16.614588  2221 net.cpp:122] Setting up slice_pair
I0429 17:11:16.614598  2221 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:16.614605  2221 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:16.614612  2221 net.cpp:137] Memory required for data: 1254800
I0429 17:11:16.614617  2221 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:16.614634  2221 net.cpp:84] Creating Layer conv1
I0429 17:11:16.614639  2221 net.cpp:406] conv1 <- data
I0429 17:11:16.614650  2221 net.cpp:380] conv1 -> conv1
I0429 17:11:16.614699  2221 net.cpp:122] Setting up conv1
I0429 17:11:16.614709  2221 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:16.614715  2221 net.cpp:137] Memory required for data: 5862800
I0429 17:11:16.614727  2221 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:16.614739  2221 net.cpp:84] Creating Layer pool1
I0429 17:11:16.614745  2221 net.cpp:406] pool1 <- conv1
I0429 17:11:16.614753  2221 net.cpp:380] pool1 -> pool1
I0429 17:11:16.614766  2221 net.cpp:122] Setting up pool1
I0429 17:11:16.614774  2221 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:16.614780  2221 net.cpp:137] Memory required for data: 7014800
I0429 17:11:16.614785  2221 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:16.614800  2221 net.cpp:84] Creating Layer conv2
I0429 17:11:16.614806  2221 net.cpp:406] conv2 <- pool1
I0429 17:11:16.614816  2221 net.cpp:380] conv2 -> conv2
I0429 17:11:16.615123  2221 net.cpp:122] Setting up conv2
I0429 17:11:16.615133  2221 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:16.615139  2221 net.cpp:137] Memory required for data: 8294800
I0429 17:11:16.615149  2221 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:16.615159  2221 net.cpp:84] Creating Layer pool2
I0429 17:11:16.615164  2221 net.cpp:406] pool2 <- conv2
I0429 17:11:16.615175  2221 net.cpp:380] pool2 -> pool2
I0429 17:11:16.615186  2221 net.cpp:122] Setting up pool2
I0429 17:11:16.615193  2221 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:16.615198  2221 net.cpp:137] Memory required for data: 8614800
I0429 17:11:16.615203  2221 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:16.615214  2221 net.cpp:84] Creating Layer ip1
I0429 17:11:16.615221  2221 net.cpp:406] ip1 <- pool2
I0429 17:11:16.615231  2221 net.cpp:380] ip1 -> ip1
I0429 17:11:16.619686  2221 net.cpp:122] Setting up ip1
I0429 17:11:16.619702  2221 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:16.619707  2221 net.cpp:137] Memory required for data: 8814800
I0429 17:11:16.619729  2221 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:16.619738  2221 net.cpp:84] Creating Layer relu1
I0429 17:11:16.619745  2221 net.cpp:406] relu1 <- ip1
I0429 17:11:16.619755  2221 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:16.619765  2221 net.cpp:122] Setting up relu1
I0429 17:11:16.619771  2221 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:16.619776  2221 net.cpp:137] Memory required for data: 9014800
I0429 17:11:16.619781  2221 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:16.619791  2221 net.cpp:84] Creating Layer ip2
I0429 17:11:16.619796  2221 net.cpp:406] ip2 <- ip1
I0429 17:11:16.619807  2221 net.cpp:380] ip2 -> ip2
I0429 17:11:16.619882  2221 net.cpp:122] Setting up ip2
I0429 17:11:16.619890  2221 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:16.619895  2221 net.cpp:137] Memory required for data: 9018800
I0429 17:11:16.619904  2221 layer_factory.hpp:77] Creating layer feat
I0429 17:11:16.619911  2221 net.cpp:84] Creating Layer feat
I0429 17:11:16.619917  2221 net.cpp:406] feat <- ip2
I0429 17:11:16.619927  2221 net.cpp:380] feat -> feat
I0429 17:11:16.619945  2221 net.cpp:122] Setting up feat
I0429 17:11:16.619951  2221 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:16.619956  2221 net.cpp:137] Memory required for data: 9019600
I0429 17:11:16.619966  2221 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:16.619982  2221 net.cpp:84] Creating Layer conv1_p
I0429 17:11:16.619992  2221 net.cpp:406] conv1_p <- data_p
I0429 17:11:16.620002  2221 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:16.620064  2221 net.cpp:122] Setting up conv1_p
I0429 17:11:16.620079  2221 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:16.620085  2221 net.cpp:137] Memory required for data: 13627600
I0429 17:11:16.620091  2221 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:16.620097  2221 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:16.620103  2221 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:16.620111  2221 net.cpp:84] Creating Layer pool1_p
I0429 17:11:16.620117  2221 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:16.620126  2221 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:16.620137  2221 net.cpp:122] Setting up pool1_p
I0429 17:11:16.620146  2221 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:16.620151  2221 net.cpp:137] Memory required for data: 14779600
I0429 17:11:16.620156  2221 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:16.620172  2221 net.cpp:84] Creating Layer conv2_p
I0429 17:11:16.620177  2221 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:16.620187  2221 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:16.620499  2221 net.cpp:122] Setting up conv2_p
I0429 17:11:16.620510  2221 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:16.620515  2221 net.cpp:137] Memory required for data: 16059600
I0429 17:11:16.620522  2221 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:16.620527  2221 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:16.620533  2221 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:16.620540  2221 net.cpp:84] Creating Layer pool2_p
I0429 17:11:16.620546  2221 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:16.620556  2221 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:16.620568  2221 net.cpp:122] Setting up pool2_p
I0429 17:11:16.620576  2221 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:16.620581  2221 net.cpp:137] Memory required for data: 16379600
I0429 17:11:16.620586  2221 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:16.620596  2221 net.cpp:84] Creating Layer ip1_p
I0429 17:11:16.620601  2221 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:16.620611  2221 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:16.625068  2221 net.cpp:122] Setting up ip1_p
I0429 17:11:16.625082  2221 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:16.625087  2221 net.cpp:137] Memory required for data: 16579600
I0429 17:11:16.625092  2221 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:16.625098  2221 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:16.625104  2221 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:16.625113  2221 net.cpp:84] Creating Layer relu1_p
I0429 17:11:16.625118  2221 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:16.625125  2221 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:16.625138  2221 net.cpp:122] Setting up relu1_p
I0429 17:11:16.625144  2221 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:16.625147  2221 net.cpp:137] Memory required for data: 16779600
I0429 17:11:16.625152  2221 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:16.625164  2221 net.cpp:84] Creating Layer ip2_p
I0429 17:11:16.625170  2221 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:16.625177  2221 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:16.625249  2221 net.cpp:122] Setting up ip2_p
I0429 17:11:16.625258  2221 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:16.625262  2221 net.cpp:137] Memory required for data: 16783600
I0429 17:11:16.625270  2221 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:16.625277  2221 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:16.625283  2221 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:16.625291  2221 net.cpp:84] Creating Layer feat_p
I0429 17:11:16.625301  2221 net.cpp:406] feat_p <- ip2_p
I0429 17:11:16.625310  2221 net.cpp:380] feat_p -> feat_p
I0429 17:11:16.625325  2221 net.cpp:122] Setting up feat_p
I0429 17:11:16.625331  2221 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:16.625336  2221 net.cpp:137] Memory required for data: 16784400
I0429 17:11:16.625341  2221 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:16.625349  2221 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:16.625354  2221 layer_factory.hpp:77] Creating layer loss
I0429 17:11:16.625363  2221 net.cpp:84] Creating Layer loss
I0429 17:11:16.625368  2221 net.cpp:406] loss <- feat
I0429 17:11:16.625375  2221 net.cpp:406] loss <- feat_p
I0429 17:11:16.625380  2221 net.cpp:406] loss <- sim
I0429 17:11:16.625388  2221 net.cpp:380] loss -> loss
I0429 17:11:16.625403  2221 net.cpp:122] Setting up loss
I0429 17:11:16.625411  2221 net.cpp:129] Top shape: (1)
I0429 17:11:16.625414  2221 net.cpp:132]     with loss weight 1
I0429 17:11:16.625427  2221 net.cpp:137] Memory required for data: 16784404
I0429 17:11:16.625432  2221 net.cpp:198] loss needs backward computation.
I0429 17:11:16.625438  2221 net.cpp:198] feat_p needs backward computation.
I0429 17:11:16.625443  2221 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:16.625448  2221 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:16.625453  2221 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:16.625459  2221 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:16.625464  2221 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:16.625469  2221 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:16.625474  2221 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:16.625480  2221 net.cpp:198] feat needs backward computation.
I0429 17:11:16.625485  2221 net.cpp:198] ip2 needs backward computation.
I0429 17:11:16.625491  2221 net.cpp:198] relu1 needs backward computation.
I0429 17:11:16.625495  2221 net.cpp:198] ip1 needs backward computation.
I0429 17:11:16.625501  2221 net.cpp:198] pool2 needs backward computation.
I0429 17:11:16.625506  2221 net.cpp:198] conv2 needs backward computation.
I0429 17:11:16.625511  2221 net.cpp:198] pool1 needs backward computation.
I0429 17:11:16.625516  2221 net.cpp:198] conv1 needs backward computation.
I0429 17:11:16.625525  2221 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:16.625532  2221 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:16.625536  2221 net.cpp:242] This network produces output loss
I0429 17:11:16.625659  2221 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:17.343982  2250 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:17.344008  2250 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:17.344012  2250 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:17.345522  2250 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:17.345672  2250 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:17.345742  2250 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:17.346776  2250 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:17.347065  2250 net.cpp:84] Creating Layer pair_data
I0429 17:11:17.347080  2250 net.cpp:380] pair_data -> pair_data
I0429 17:11:17.347097  2250 net.cpp:380] pair_data -> sim
I0429 17:11:17.347126  2250 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:17.347926  2250 net.cpp:122] Setting up pair_data
I0429 17:11:17.347939  2250 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:17.347947  2250 net.cpp:129] Top shape: 100 (100)
I0429 17:11:17.347951  2250 net.cpp:137] Memory required for data: 627600
I0429 17:11:17.347957  2250 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:17.347968  2250 net.cpp:84] Creating Layer slice_pair
I0429 17:11:17.347975  2250 net.cpp:406] slice_pair <- pair_data
I0429 17:11:17.347983  2250 net.cpp:380] slice_pair -> data
I0429 17:11:17.347995  2250 net.cpp:380] slice_pair -> data_p
I0429 17:11:17.348006  2250 net.cpp:122] Setting up slice_pair
I0429 17:11:17.348014  2250 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:17.348021  2250 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:17.348026  2250 net.cpp:137] Memory required for data: 1254800
I0429 17:11:17.348031  2250 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:17.348058  2250 net.cpp:84] Creating Layer conv1
I0429 17:11:17.348067  2250 net.cpp:406] conv1 <- data
I0429 17:11:17.348078  2250 net.cpp:380] conv1 -> conv1
I0429 17:11:17.348125  2250 net.cpp:122] Setting up conv1
I0429 17:11:17.348134  2250 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:17.348140  2250 net.cpp:137] Memory required for data: 5862800
I0429 17:11:17.348150  2250 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:17.348160  2250 net.cpp:84] Creating Layer pool1
I0429 17:11:17.348165  2250 net.cpp:406] pool1 <- conv1
I0429 17:11:17.348173  2250 net.cpp:380] pool1 -> pool1
I0429 17:11:17.348186  2250 net.cpp:122] Setting up pool1
I0429 17:11:17.348193  2250 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:17.348198  2250 net.cpp:137] Memory required for data: 7014800
I0429 17:11:17.348202  2250 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:17.348217  2250 net.cpp:84] Creating Layer conv2
I0429 17:11:17.348222  2250 net.cpp:406] conv2 <- pool1
I0429 17:11:17.348233  2250 net.cpp:380] conv2 -> conv2
I0429 17:11:17.348520  2250 net.cpp:122] Setting up conv2
I0429 17:11:17.348529  2250 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:17.348534  2250 net.cpp:137] Memory required for data: 8294800
I0429 17:11:17.348544  2250 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:17.348553  2250 net.cpp:84] Creating Layer pool2
I0429 17:11:17.348557  2250 net.cpp:406] pool2 <- conv2
I0429 17:11:17.348567  2250 net.cpp:380] pool2 -> pool2
I0429 17:11:17.348577  2250 net.cpp:122] Setting up pool2
I0429 17:11:17.348583  2250 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:17.348588  2250 net.cpp:137] Memory required for data: 8614800
I0429 17:11:17.348592  2250 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:17.348603  2250 net.cpp:84] Creating Layer ip1
I0429 17:11:17.348608  2250 net.cpp:406] ip1 <- pool2
I0429 17:11:17.348616  2250 net.cpp:380] ip1 -> ip1
I0429 17:11:17.352810  2250 net.cpp:122] Setting up ip1
I0429 17:11:17.352823  2250 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:17.352828  2250 net.cpp:137] Memory required for data: 8814800
I0429 17:11:17.352850  2250 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:17.352860  2250 net.cpp:84] Creating Layer relu1
I0429 17:11:17.352866  2250 net.cpp:406] relu1 <- ip1
I0429 17:11:17.352883  2250 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:17.352893  2250 net.cpp:122] Setting up relu1
I0429 17:11:17.352900  2250 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:17.352905  2250 net.cpp:137] Memory required for data: 9014800
I0429 17:11:17.352910  2250 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:17.352918  2250 net.cpp:84] Creating Layer ip2
I0429 17:11:17.352923  2250 net.cpp:406] ip2 <- ip1
I0429 17:11:17.352933  2250 net.cpp:380] ip2 -> ip2
I0429 17:11:17.353004  2250 net.cpp:122] Setting up ip2
I0429 17:11:17.353011  2250 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:17.353015  2250 net.cpp:137] Memory required for data: 9018800
I0429 17:11:17.353024  2250 layer_factory.hpp:77] Creating layer feat
I0429 17:11:17.353031  2250 net.cpp:84] Creating Layer feat
I0429 17:11:17.353036  2250 net.cpp:406] feat <- ip2
I0429 17:11:17.353045  2250 net.cpp:380] feat -> feat
I0429 17:11:17.353061  2250 net.cpp:122] Setting up feat
I0429 17:11:17.353067  2250 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:17.353071  2250 net.cpp:137] Memory required for data: 9019600
I0429 17:11:17.353081  2250 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:17.353094  2250 net.cpp:84] Creating Layer conv1_p
I0429 17:11:17.353099  2250 net.cpp:406] conv1_p <- data_p
I0429 17:11:17.353107  2250 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:17.353140  2250 net.cpp:122] Setting up conv1_p
I0429 17:11:17.353148  2250 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:17.353153  2250 net.cpp:137] Memory required for data: 13627600
I0429 17:11:17.353158  2250 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:17.353164  2250 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:17.353169  2250 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:17.353179  2250 net.cpp:84] Creating Layer pool1_p
I0429 17:11:17.353184  2250 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:17.353191  2250 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:17.353204  2250 net.cpp:122] Setting up pool1_p
I0429 17:11:17.353210  2250 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:17.353214  2250 net.cpp:137] Memory required for data: 14779600
I0429 17:11:17.353219  2250 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:17.353231  2250 net.cpp:84] Creating Layer conv2_p
I0429 17:11:17.353237  2250 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:17.353245  2250 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:17.353548  2250 net.cpp:122] Setting up conv2_p
I0429 17:11:17.353559  2250 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:17.353564  2250 net.cpp:137] Memory required for data: 16059600
I0429 17:11:17.353569  2250 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:17.353574  2250 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:17.353579  2250 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:17.353590  2250 net.cpp:84] Creating Layer pool2_p
I0429 17:11:17.353595  2250 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:17.353605  2250 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:17.353615  2250 net.cpp:122] Setting up pool2_p
I0429 17:11:17.353622  2250 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:17.353626  2250 net.cpp:137] Memory required for data: 16379600
I0429 17:11:17.353631  2250 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:17.353639  2250 net.cpp:84] Creating Layer ip1_p
I0429 17:11:17.353644  2250 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:17.353653  2250 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:17.357818  2250 net.cpp:122] Setting up ip1_p
I0429 17:11:17.357830  2250 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:17.357833  2250 net.cpp:137] Memory required for data: 16579600
I0429 17:11:17.357839  2250 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:17.357846  2250 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:17.357856  2250 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:17.357864  2250 net.cpp:84] Creating Layer relu1_p
I0429 17:11:17.357869  2250 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:17.357877  2250 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:17.357887  2250 net.cpp:122] Setting up relu1_p
I0429 17:11:17.357892  2250 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:17.357897  2250 net.cpp:137] Memory required for data: 16779600
I0429 17:11:17.357900  2250 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:17.357913  2250 net.cpp:84] Creating Layer ip2_p
I0429 17:11:17.357918  2250 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:17.357924  2250 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:17.357995  2250 net.cpp:122] Setting up ip2_p
I0429 17:11:17.358002  2250 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:17.358006  2250 net.cpp:137] Memory required for data: 16783600
I0429 17:11:17.358014  2250 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:17.358021  2250 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:17.358026  2250 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:17.358036  2250 net.cpp:84] Creating Layer feat_p
I0429 17:11:17.358042  2250 net.cpp:406] feat_p <- ip2_p
I0429 17:11:17.358048  2250 net.cpp:380] feat_p -> feat_p
I0429 17:11:17.358062  2250 net.cpp:122] Setting up feat_p
I0429 17:11:17.358069  2250 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:17.358073  2250 net.cpp:137] Memory required for data: 16784400
I0429 17:11:17.358078  2250 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:17.358084  2250 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:17.358088  2250 layer_factory.hpp:77] Creating layer loss
I0429 17:11:17.358098  2250 net.cpp:84] Creating Layer loss
I0429 17:11:17.358103  2250 net.cpp:406] loss <- feat
I0429 17:11:17.358109  2250 net.cpp:406] loss <- feat_p
I0429 17:11:17.358115  2250 net.cpp:406] loss <- sim
I0429 17:11:17.358122  2250 net.cpp:380] loss -> loss
I0429 17:11:17.358136  2250 net.cpp:122] Setting up loss
I0429 17:11:17.358142  2250 net.cpp:129] Top shape: (1)
I0429 17:11:17.358146  2250 net.cpp:132]     with loss weight 1
I0429 17:11:17.358157  2250 net.cpp:137] Memory required for data: 16784404
I0429 17:11:17.358162  2250 net.cpp:198] loss needs backward computation.
I0429 17:11:17.358170  2250 net.cpp:198] feat_p needs backward computation.
I0429 17:11:17.358175  2250 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:17.358180  2250 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:17.358186  2250 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:17.358191  2250 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:17.358196  2250 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:17.358201  2250 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:17.358207  2250 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:17.358213  2250 net.cpp:198] feat needs backward computation.
I0429 17:11:17.358218  2250 net.cpp:198] ip2 needs backward computation.
I0429 17:11:17.358224  2250 net.cpp:198] relu1 needs backward computation.
I0429 17:11:17.358229  2250 net.cpp:198] ip1 needs backward computation.
I0429 17:11:17.358235  2250 net.cpp:198] pool2 needs backward computation.
I0429 17:11:17.358242  2250 net.cpp:198] conv2 needs backward computation.
I0429 17:11:17.358247  2250 net.cpp:198] pool1 needs backward computation.
I0429 17:11:17.358252  2250 net.cpp:198] conv1 needs backward computation.
I0429 17:11:17.358260  2250 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:17.358268  2250 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:17.358273  2250 net.cpp:242] This network produces output loss
I0429 17:11:17.358392  2250 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:18.081634  2278 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:18.081657  2278 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:18.081660  2278 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:18.083078  2278 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:18.083228  2278 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:18.083300  2278 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:18.084328  2278 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:18.084641  2278 net.cpp:84] Creating Layer pair_data
I0429 17:11:18.084657  2278 net.cpp:380] pair_data -> pair_data
I0429 17:11:18.084676  2278 net.cpp:380] pair_data -> sim
I0429 17:11:18.084709  2278 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:18.085541  2278 net.cpp:122] Setting up pair_data
I0429 17:11:18.085562  2278 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:18.085571  2278 net.cpp:129] Top shape: 100 (100)
I0429 17:11:18.085575  2278 net.cpp:137] Memory required for data: 627600
I0429 17:11:18.085582  2278 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:18.085593  2278 net.cpp:84] Creating Layer slice_pair
I0429 17:11:18.085599  2278 net.cpp:406] slice_pair <- pair_data
I0429 17:11:18.085608  2278 net.cpp:380] slice_pair -> data
I0429 17:11:18.085620  2278 net.cpp:380] slice_pair -> data_p
I0429 17:11:18.085633  2278 net.cpp:122] Setting up slice_pair
I0429 17:11:18.085642  2278 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:18.085649  2278 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:18.085654  2278 net.cpp:137] Memory required for data: 1254800
I0429 17:11:18.085659  2278 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:18.085675  2278 net.cpp:84] Creating Layer conv1
I0429 17:11:18.085680  2278 net.cpp:406] conv1 <- data
I0429 17:11:18.085690  2278 net.cpp:380] conv1 -> conv1
I0429 17:11:18.085736  2278 net.cpp:122] Setting up conv1
I0429 17:11:18.085747  2278 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:18.085752  2278 net.cpp:137] Memory required for data: 5862800
I0429 17:11:18.085763  2278 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:18.085774  2278 net.cpp:84] Creating Layer pool1
I0429 17:11:18.085780  2278 net.cpp:406] pool1 <- conv1
I0429 17:11:18.085788  2278 net.cpp:380] pool1 -> pool1
I0429 17:11:18.085800  2278 net.cpp:122] Setting up pool1
I0429 17:11:18.085808  2278 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:18.085813  2278 net.cpp:137] Memory required for data: 7014800
I0429 17:11:18.085819  2278 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:18.085834  2278 net.cpp:84] Creating Layer conv2
I0429 17:11:18.085840  2278 net.cpp:406] conv2 <- pool1
I0429 17:11:18.085850  2278 net.cpp:380] conv2 -> conv2
I0429 17:11:18.086170  2278 net.cpp:122] Setting up conv2
I0429 17:11:18.086181  2278 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:18.086186  2278 net.cpp:137] Memory required for data: 8294800
I0429 17:11:18.086197  2278 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:18.086206  2278 net.cpp:84] Creating Layer pool2
I0429 17:11:18.086212  2278 net.cpp:406] pool2 <- conv2
I0429 17:11:18.086221  2278 net.cpp:380] pool2 -> pool2
I0429 17:11:18.086233  2278 net.cpp:122] Setting up pool2
I0429 17:11:18.086241  2278 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:18.086246  2278 net.cpp:137] Memory required for data: 8614800
I0429 17:11:18.086251  2278 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:18.086261  2278 net.cpp:84] Creating Layer ip1
I0429 17:11:18.086266  2278 net.cpp:406] ip1 <- pool2
I0429 17:11:18.086275  2278 net.cpp:380] ip1 -> ip1
I0429 17:11:18.090591  2278 net.cpp:122] Setting up ip1
I0429 17:11:18.090605  2278 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:18.090610  2278 net.cpp:137] Memory required for data: 8814800
I0429 17:11:18.090625  2278 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:18.090633  2278 net.cpp:84] Creating Layer relu1
I0429 17:11:18.090639  2278 net.cpp:406] relu1 <- ip1
I0429 17:11:18.090648  2278 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:18.090656  2278 net.cpp:122] Setting up relu1
I0429 17:11:18.090663  2278 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:18.090668  2278 net.cpp:137] Memory required for data: 9014800
I0429 17:11:18.090672  2278 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:18.090682  2278 net.cpp:84] Creating Layer ip2
I0429 17:11:18.090687  2278 net.cpp:406] ip2 <- ip1
I0429 17:11:18.090698  2278 net.cpp:380] ip2 -> ip2
I0429 17:11:18.090787  2278 net.cpp:122] Setting up ip2
I0429 17:11:18.090798  2278 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:18.090803  2278 net.cpp:137] Memory required for data: 9018800
I0429 17:11:18.090812  2278 layer_factory.hpp:77] Creating layer feat
I0429 17:11:18.090821  2278 net.cpp:84] Creating Layer feat
I0429 17:11:18.090826  2278 net.cpp:406] feat <- ip2
I0429 17:11:18.090836  2278 net.cpp:380] feat -> feat
I0429 17:11:18.090852  2278 net.cpp:122] Setting up feat
I0429 17:11:18.090857  2278 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:18.090862  2278 net.cpp:137] Memory required for data: 9019600
I0429 17:11:18.090873  2278 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:18.090886  2278 net.cpp:84] Creating Layer conv1_p
I0429 17:11:18.090891  2278 net.cpp:406] conv1_p <- data_p
I0429 17:11:18.090899  2278 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:18.090932  2278 net.cpp:122] Setting up conv1_p
I0429 17:11:18.090942  2278 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:18.090947  2278 net.cpp:137] Memory required for data: 13627600
I0429 17:11:18.090953  2278 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:18.090960  2278 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:18.090965  2278 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:18.090973  2278 net.cpp:84] Creating Layer pool1_p
I0429 17:11:18.090978  2278 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:18.090986  2278 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:18.090996  2278 net.cpp:122] Setting up pool1_p
I0429 17:11:18.091003  2278 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:18.091008  2278 net.cpp:137] Memory required for data: 14779600
I0429 17:11:18.091012  2278 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:18.091027  2278 net.cpp:84] Creating Layer conv2_p
I0429 17:11:18.091032  2278 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:18.091039  2278 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:18.091330  2278 net.cpp:122] Setting up conv2_p
I0429 17:11:18.091339  2278 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:18.091344  2278 net.cpp:137] Memory required for data: 16059600
I0429 17:11:18.091349  2278 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:18.091361  2278 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:18.091367  2278 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:18.091374  2278 net.cpp:84] Creating Layer pool2_p
I0429 17:11:18.091379  2278 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:18.091388  2278 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:18.091399  2278 net.cpp:122] Setting up pool2_p
I0429 17:11:18.091406  2278 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:18.091410  2278 net.cpp:137] Memory required for data: 16379600
I0429 17:11:18.091415  2278 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:18.091423  2278 net.cpp:84] Creating Layer ip1_p
I0429 17:11:18.091428  2278 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:18.091439  2278 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:18.095585  2278 net.cpp:122] Setting up ip1_p
I0429 17:11:18.095597  2278 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:18.095602  2278 net.cpp:137] Memory required for data: 16579600
I0429 17:11:18.095607  2278 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:18.095613  2278 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:18.095619  2278 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:18.095628  2278 net.cpp:84] Creating Layer relu1_p
I0429 17:11:18.095633  2278 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:18.095641  2278 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:18.095649  2278 net.cpp:122] Setting up relu1_p
I0429 17:11:18.095656  2278 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:18.095660  2278 net.cpp:137] Memory required for data: 16779600
I0429 17:11:18.095664  2278 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:18.095676  2278 net.cpp:84] Creating Layer ip2_p
I0429 17:11:18.095681  2278 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:18.095690  2278 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:18.095758  2278 net.cpp:122] Setting up ip2_p
I0429 17:11:18.095767  2278 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:18.095772  2278 net.cpp:137] Memory required for data: 16783600
I0429 17:11:18.095779  2278 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:18.095787  2278 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:18.095791  2278 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:18.095803  2278 net.cpp:84] Creating Layer feat_p
I0429 17:11:18.095808  2278 net.cpp:406] feat_p <- ip2_p
I0429 17:11:18.095815  2278 net.cpp:380] feat_p -> feat_p
I0429 17:11:18.095830  2278 net.cpp:122] Setting up feat_p
I0429 17:11:18.095836  2278 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:18.095841  2278 net.cpp:137] Memory required for data: 16784400
I0429 17:11:18.095846  2278 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:18.095852  2278 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:18.095857  2278 layer_factory.hpp:77] Creating layer loss
I0429 17:11:18.095867  2278 net.cpp:84] Creating Layer loss
I0429 17:11:18.095872  2278 net.cpp:406] loss <- feat
I0429 17:11:18.095878  2278 net.cpp:406] loss <- feat_p
I0429 17:11:18.095885  2278 net.cpp:406] loss <- sim
I0429 17:11:18.095891  2278 net.cpp:380] loss -> loss
I0429 17:11:18.095906  2278 net.cpp:122] Setting up loss
I0429 17:11:18.095912  2278 net.cpp:129] Top shape: (1)
I0429 17:11:18.095916  2278 net.cpp:132]     with loss weight 1
I0429 17:11:18.095927  2278 net.cpp:137] Memory required for data: 16784404
I0429 17:11:18.095932  2278 net.cpp:198] loss needs backward computation.
I0429 17:11:18.095939  2278 net.cpp:198] feat_p needs backward computation.
I0429 17:11:18.095944  2278 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:18.095949  2278 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:18.095954  2278 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:18.095959  2278 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:18.095964  2278 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:18.095974  2278 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:18.095980  2278 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:18.095986  2278 net.cpp:198] feat needs backward computation.
I0429 17:11:18.095991  2278 net.cpp:198] ip2 needs backward computation.
I0429 17:11:18.095996  2278 net.cpp:198] relu1 needs backward computation.
I0429 17:11:18.096001  2278 net.cpp:198] ip1 needs backward computation.
I0429 17:11:18.096007  2278 net.cpp:198] pool2 needs backward computation.
I0429 17:11:18.096012  2278 net.cpp:198] conv2 needs backward computation.
I0429 17:11:18.096017  2278 net.cpp:198] pool1 needs backward computation.
I0429 17:11:18.096022  2278 net.cpp:198] conv1 needs backward computation.
I0429 17:11:18.096030  2278 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:18.096042  2278 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:18.096047  2278 net.cpp:242] This network produces output loss
I0429 17:11:18.096168  2278 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:18.896634  2306 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:18.896661  2306 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:18.896664  2306 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:18.898119  2306 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:18.898272  2306 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:18.898346  2306 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:18.899241  2306 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:18.899428  2306 net.cpp:84] Creating Layer pair_data
I0429 17:11:18.899438  2306 net.cpp:380] pair_data -> pair_data
I0429 17:11:18.899451  2306 net.cpp:380] pair_data -> sim
I0429 17:11:18.899469  2306 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:18.900014  2306 net.cpp:122] Setting up pair_data
I0429 17:11:18.900074  2306 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:18.900080  2306 net.cpp:129] Top shape: 100 (100)
I0429 17:11:18.900082  2306 net.cpp:137] Memory required for data: 627600
I0429 17:11:18.900086  2306 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:18.900094  2306 net.cpp:84] Creating Layer slice_pair
I0429 17:11:18.900099  2306 net.cpp:406] slice_pair <- pair_data
I0429 17:11:18.900104  2306 net.cpp:380] slice_pair -> data
I0429 17:11:18.900111  2306 net.cpp:380] slice_pair -> data_p
I0429 17:11:18.900122  2306 net.cpp:122] Setting up slice_pair
I0429 17:11:18.900127  2306 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:18.900131  2306 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:18.900140  2306 net.cpp:137] Memory required for data: 1254800
I0429 17:11:18.900143  2306 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:18.900153  2306 net.cpp:84] Creating Layer conv1
I0429 17:11:18.900156  2306 net.cpp:406] conv1 <- data
I0429 17:11:18.900161  2306 net.cpp:380] conv1 -> conv1
I0429 17:11:18.900202  2306 net.cpp:122] Setting up conv1
I0429 17:11:18.900209  2306 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:18.900213  2306 net.cpp:137] Memory required for data: 5862800
I0429 17:11:18.900220  2306 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:18.900228  2306 net.cpp:84] Creating Layer pool1
I0429 17:11:18.900231  2306 net.cpp:406] pool1 <- conv1
I0429 17:11:18.900236  2306 net.cpp:380] pool1 -> pool1
I0429 17:11:18.900244  2306 net.cpp:122] Setting up pool1
I0429 17:11:18.900249  2306 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:18.900254  2306 net.cpp:137] Memory required for data: 7014800
I0429 17:11:18.900256  2306 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:18.900265  2306 net.cpp:84] Creating Layer conv2
I0429 17:11:18.900269  2306 net.cpp:406] conv2 <- pool1
I0429 17:11:18.900275  2306 net.cpp:380] conv2 -> conv2
I0429 17:11:18.900462  2306 net.cpp:122] Setting up conv2
I0429 17:11:18.900468  2306 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:18.900472  2306 net.cpp:137] Memory required for data: 8294800
I0429 17:11:18.900478  2306 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:18.900485  2306 net.cpp:84] Creating Layer pool2
I0429 17:11:18.900488  2306 net.cpp:406] pool2 <- conv2
I0429 17:11:18.900494  2306 net.cpp:380] pool2 -> pool2
I0429 17:11:18.900501  2306 net.cpp:122] Setting up pool2
I0429 17:11:18.900506  2306 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:18.900508  2306 net.cpp:137] Memory required for data: 8614800
I0429 17:11:18.900511  2306 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:18.900518  2306 net.cpp:84] Creating Layer ip1
I0429 17:11:18.900521  2306 net.cpp:406] ip1 <- pool2
I0429 17:11:18.900527  2306 net.cpp:380] ip1 -> ip1
I0429 17:11:18.903281  2306 net.cpp:122] Setting up ip1
I0429 17:11:18.903290  2306 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:18.903292  2306 net.cpp:137] Memory required for data: 8814800
I0429 17:11:18.903298  2306 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:18.903304  2306 net.cpp:84] Creating Layer relu1
I0429 17:11:18.903313  2306 net.cpp:406] relu1 <- ip1
I0429 17:11:18.903318  2306 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:18.903324  2306 net.cpp:122] Setting up relu1
I0429 17:11:18.903328  2306 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:18.903332  2306 net.cpp:137] Memory required for data: 9014800
I0429 17:11:18.903336  2306 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:18.903342  2306 net.cpp:84] Creating Layer ip2
I0429 17:11:18.903345  2306 net.cpp:406] ip2 <- ip1
I0429 17:11:18.903350  2306 net.cpp:380] ip2 -> ip2
I0429 17:11:18.903394  2306 net.cpp:122] Setting up ip2
I0429 17:11:18.903399  2306 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:18.903403  2306 net.cpp:137] Memory required for data: 9018800
I0429 17:11:18.903408  2306 layer_factory.hpp:77] Creating layer feat
I0429 17:11:18.903412  2306 net.cpp:84] Creating Layer feat
I0429 17:11:18.903415  2306 net.cpp:406] feat <- ip2
I0429 17:11:18.903421  2306 net.cpp:380] feat -> feat
I0429 17:11:18.903431  2306 net.cpp:122] Setting up feat
I0429 17:11:18.903435  2306 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:18.903437  2306 net.cpp:137] Memory required for data: 9019600
I0429 17:11:18.903445  2306 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:18.903452  2306 net.cpp:84] Creating Layer conv1_p
I0429 17:11:18.903455  2306 net.cpp:406] conv1_p <- data_p
I0429 17:11:18.903461  2306 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:18.903483  2306 net.cpp:122] Setting up conv1_p
I0429 17:11:18.903489  2306 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:18.903492  2306 net.cpp:137] Memory required for data: 13627600
I0429 17:11:18.903501  2306 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:18.903504  2306 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:18.903507  2306 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:18.903512  2306 net.cpp:84] Creating Layer pool1_p
I0429 17:11:18.903517  2306 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:18.903520  2306 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:18.903529  2306 net.cpp:122] Setting up pool1_p
I0429 17:11:18.903534  2306 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:18.903537  2306 net.cpp:137] Memory required for data: 14779600
I0429 17:11:18.903540  2306 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:18.903548  2306 net.cpp:84] Creating Layer conv2_p
I0429 17:11:18.903553  2306 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:18.903558  2306 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:18.903743  2306 net.cpp:122] Setting up conv2_p
I0429 17:11:18.903750  2306 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:18.903754  2306 net.cpp:137] Memory required for data: 16059600
I0429 17:11:18.903759  2306 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:18.903761  2306 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:18.903765  2306 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:18.903770  2306 net.cpp:84] Creating Layer pool2_p
I0429 17:11:18.903774  2306 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:18.903779  2306 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:18.903786  2306 net.cpp:122] Setting up pool2_p
I0429 17:11:18.903790  2306 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:18.903794  2306 net.cpp:137] Memory required for data: 16379600
I0429 17:11:18.903796  2306 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:18.903803  2306 net.cpp:84] Creating Layer ip1_p
I0429 17:11:18.903806  2306 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:18.903811  2306 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:18.906555  2306 net.cpp:122] Setting up ip1_p
I0429 17:11:18.906564  2306 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:18.906568  2306 net.cpp:137] Memory required for data: 16579600
I0429 17:11:18.906571  2306 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:18.906575  2306 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:18.906579  2306 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:18.906584  2306 net.cpp:84] Creating Layer relu1_p
I0429 17:11:18.906587  2306 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:18.906591  2306 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:18.906599  2306 net.cpp:122] Setting up relu1_p
I0429 17:11:18.906602  2306 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:18.906605  2306 net.cpp:137] Memory required for data: 16779600
I0429 17:11:18.906608  2306 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:18.906618  2306 net.cpp:84] Creating Layer ip2_p
I0429 17:11:18.906621  2306 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:18.906626  2306 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:18.906673  2306 net.cpp:122] Setting up ip2_p
I0429 17:11:18.906679  2306 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:18.906682  2306 net.cpp:137] Memory required for data: 16783600
I0429 17:11:18.906687  2306 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:18.906692  2306 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:18.906694  2306 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:18.906700  2306 net.cpp:84] Creating Layer feat_p
I0429 17:11:18.906704  2306 net.cpp:406] feat_p <- ip2_p
I0429 17:11:18.906709  2306 net.cpp:380] feat_p -> feat_p
I0429 17:11:18.906718  2306 net.cpp:122] Setting up feat_p
I0429 17:11:18.906723  2306 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:18.906725  2306 net.cpp:137] Memory required for data: 16784400
I0429 17:11:18.906729  2306 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:18.906736  2306 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:18.906740  2306 layer_factory.hpp:77] Creating layer loss
I0429 17:11:18.906746  2306 net.cpp:84] Creating Layer loss
I0429 17:11:18.906750  2306 net.cpp:406] loss <- feat
I0429 17:11:18.906754  2306 net.cpp:406] loss <- feat_p
I0429 17:11:18.906759  2306 net.cpp:406] loss <- sim
I0429 17:11:18.906762  2306 net.cpp:380] loss -> loss
I0429 17:11:18.906772  2306 net.cpp:122] Setting up loss
I0429 17:11:18.906776  2306 net.cpp:129] Top shape: (1)
I0429 17:11:18.906780  2306 net.cpp:132]     with loss weight 1
I0429 17:11:18.906787  2306 net.cpp:137] Memory required for data: 16784404
I0429 17:11:18.906790  2306 net.cpp:198] loss needs backward computation.
I0429 17:11:18.906795  2306 net.cpp:198] feat_p needs backward computation.
I0429 17:11:18.906798  2306 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:18.906801  2306 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:18.906805  2306 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:18.906808  2306 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:18.906811  2306 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:18.906816  2306 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:18.906818  2306 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:18.906822  2306 net.cpp:198] feat needs backward computation.
I0429 17:11:18.906826  2306 net.cpp:198] ip2 needs backward computation.
I0429 17:11:18.906828  2306 net.cpp:198] relu1 needs backward computation.
I0429 17:11:18.906831  2306 net.cpp:198] ip1 needs backward computation.
I0429 17:11:18.906836  2306 net.cpp:198] pool2 needs backward computation.
I0429 17:11:18.906838  2306 net.cpp:198] conv2 needs backward computation.
I0429 17:11:18.906841  2306 net.cpp:198] pool1 needs backward computation.
I0429 17:11:18.906844  2306 net.cpp:198] conv1 needs backward computation.
I0429 17:11:18.906850  2306 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:18.906855  2306 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:18.906858  2306 net.cpp:242] This network produces output loss
I0429 17:11:18.906949  2306 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0429 17:11:19.604622  2334 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0429 17:11:19.604647  2334 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0429 17:11:19.604650  2334 _caffe.cpp:142] Net('examples/siamese/mnist_siamese_train_test.prototxt', 1, weights='examples/siamese/mnist_siamese_iter_1000.caffemodel')
I0429 17:11:19.606047  2334 net.cpp:294] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0429 17:11:19.606197  2334 net.cpp:51] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0429 17:11:19.606267  2334 layer_factory.hpp:77] Creating layer pair_data
I0429 17:11:19.607329  2334 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb
I0429 17:11:19.607615  2334 net.cpp:84] Creating Layer pair_data
I0429 17:11:19.607631  2334 net.cpp:380] pair_data -> pair_data
I0429 17:11:19.607648  2334 net.cpp:380] pair_data -> sim
I0429 17:11:19.607681  2334 data_layer.cpp:45] output data size: 100,2,28,28
I0429 17:11:19.608497  2334 net.cpp:122] Setting up pair_data
I0429 17:11:19.608525  2334 net.cpp:129] Top shape: 100 2 28 28 (156800)
I0429 17:11:19.608532  2334 net.cpp:129] Top shape: 100 (100)
I0429 17:11:19.608536  2334 net.cpp:137] Memory required for data: 627600
I0429 17:11:19.608541  2334 layer_factory.hpp:77] Creating layer slice_pair
I0429 17:11:19.608552  2334 net.cpp:84] Creating Layer slice_pair
I0429 17:11:19.608558  2334 net.cpp:406] slice_pair <- pair_data
I0429 17:11:19.608566  2334 net.cpp:380] slice_pair -> data
I0429 17:11:19.608577  2334 net.cpp:380] slice_pair -> data_p
I0429 17:11:19.608588  2334 net.cpp:122] Setting up slice_pair
I0429 17:11:19.608595  2334 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:19.608603  2334 net.cpp:129] Top shape: 100 1 28 28 (78400)
I0429 17:11:19.608606  2334 net.cpp:137] Memory required for data: 1254800
I0429 17:11:19.608610  2334 layer_factory.hpp:77] Creating layer conv1
I0429 17:11:19.608624  2334 net.cpp:84] Creating Layer conv1
I0429 17:11:19.608630  2334 net.cpp:406] conv1 <- data
I0429 17:11:19.608639  2334 net.cpp:380] conv1 -> conv1
I0429 17:11:19.608685  2334 net.cpp:122] Setting up conv1
I0429 17:11:19.608695  2334 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:19.608700  2334 net.cpp:137] Memory required for data: 5862800
I0429 17:11:19.608711  2334 layer_factory.hpp:77] Creating layer pool1
I0429 17:11:19.608721  2334 net.cpp:84] Creating Layer pool1
I0429 17:11:19.608726  2334 net.cpp:406] pool1 <- conv1
I0429 17:11:19.608732  2334 net.cpp:380] pool1 -> pool1
I0429 17:11:19.608743  2334 net.cpp:122] Setting up pool1
I0429 17:11:19.608749  2334 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:19.608754  2334 net.cpp:137] Memory required for data: 7014800
I0429 17:11:19.608758  2334 layer_factory.hpp:77] Creating layer conv2
I0429 17:11:19.608773  2334 net.cpp:84] Creating Layer conv2
I0429 17:11:19.608778  2334 net.cpp:406] conv2 <- pool1
I0429 17:11:19.608788  2334 net.cpp:380] conv2 -> conv2
I0429 17:11:19.609064  2334 net.cpp:122] Setting up conv2
I0429 17:11:19.609074  2334 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:19.609078  2334 net.cpp:137] Memory required for data: 8294800
I0429 17:11:19.609087  2334 layer_factory.hpp:77] Creating layer pool2
I0429 17:11:19.609097  2334 net.cpp:84] Creating Layer pool2
I0429 17:11:19.609100  2334 net.cpp:406] pool2 <- conv2
I0429 17:11:19.609108  2334 net.cpp:380] pool2 -> pool2
I0429 17:11:19.609119  2334 net.cpp:122] Setting up pool2
I0429 17:11:19.609125  2334 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:19.609129  2334 net.cpp:137] Memory required for data: 8614800
I0429 17:11:19.609134  2334 layer_factory.hpp:77] Creating layer ip1
I0429 17:11:19.609143  2334 net.cpp:84] Creating Layer ip1
I0429 17:11:19.609148  2334 net.cpp:406] ip1 <- pool2
I0429 17:11:19.609156  2334 net.cpp:380] ip1 -> ip1
I0429 17:11:19.613101  2334 net.cpp:122] Setting up ip1
I0429 17:11:19.613112  2334 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:19.613116  2334 net.cpp:137] Memory required for data: 8814800
I0429 17:11:19.613129  2334 layer_factory.hpp:77] Creating layer relu1
I0429 17:11:19.613138  2334 net.cpp:84] Creating Layer relu1
I0429 17:11:19.613143  2334 net.cpp:406] relu1 <- ip1
I0429 17:11:19.613152  2334 net.cpp:367] relu1 -> ip1 (in-place)
I0429 17:11:19.613160  2334 net.cpp:122] Setting up relu1
I0429 17:11:19.613167  2334 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:19.613170  2334 net.cpp:137] Memory required for data: 9014800
I0429 17:11:19.613175  2334 layer_factory.hpp:77] Creating layer ip2
I0429 17:11:19.613189  2334 net.cpp:84] Creating Layer ip2
I0429 17:11:19.613195  2334 net.cpp:406] ip2 <- ip1
I0429 17:11:19.613204  2334 net.cpp:380] ip2 -> ip2
I0429 17:11:19.613270  2334 net.cpp:122] Setting up ip2
I0429 17:11:19.613276  2334 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:19.613281  2334 net.cpp:137] Memory required for data: 9018800
I0429 17:11:19.613288  2334 layer_factory.hpp:77] Creating layer feat
I0429 17:11:19.613296  2334 net.cpp:84] Creating Layer feat
I0429 17:11:19.613299  2334 net.cpp:406] feat <- ip2
I0429 17:11:19.613309  2334 net.cpp:380] feat -> feat
I0429 17:11:19.613324  2334 net.cpp:122] Setting up feat
I0429 17:11:19.613329  2334 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:19.613334  2334 net.cpp:137] Memory required for data: 9019600
I0429 17:11:19.613343  2334 layer_factory.hpp:77] Creating layer conv1_p
I0429 17:11:19.613355  2334 net.cpp:84] Creating Layer conv1_p
I0429 17:11:19.613360  2334 net.cpp:406] conv1_p <- data_p
I0429 17:11:19.613368  2334 net.cpp:380] conv1_p -> conv1_p
I0429 17:11:19.613399  2334 net.cpp:122] Setting up conv1_p
I0429 17:11:19.613409  2334 net.cpp:129] Top shape: 100 20 24 24 (1152000)
I0429 17:11:19.613412  2334 net.cpp:137] Memory required for data: 13627600
I0429 17:11:19.613417  2334 net.cpp:465] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0429 17:11:19.613423  2334 net.cpp:465] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0429 17:11:19.613430  2334 layer_factory.hpp:77] Creating layer pool1_p
I0429 17:11:19.613436  2334 net.cpp:84] Creating Layer pool1_p
I0429 17:11:19.613440  2334 net.cpp:406] pool1_p <- conv1_p
I0429 17:11:19.613448  2334 net.cpp:380] pool1_p -> pool1_p
I0429 17:11:19.613458  2334 net.cpp:122] Setting up pool1_p
I0429 17:11:19.613466  2334 net.cpp:129] Top shape: 100 20 12 12 (288000)
I0429 17:11:19.613469  2334 net.cpp:137] Memory required for data: 14779600
I0429 17:11:19.613473  2334 layer_factory.hpp:77] Creating layer conv2_p
I0429 17:11:19.613487  2334 net.cpp:84] Creating Layer conv2_p
I0429 17:11:19.613492  2334 net.cpp:406] conv2_p <- pool1_p
I0429 17:11:19.613498  2334 net.cpp:380] conv2_p -> conv2_p
I0429 17:11:19.613788  2334 net.cpp:122] Setting up conv2_p
I0429 17:11:19.613800  2334 net.cpp:129] Top shape: 100 50 8 8 (320000)
I0429 17:11:19.613804  2334 net.cpp:137] Memory required for data: 16059600
I0429 17:11:19.613809  2334 net.cpp:465] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0429 17:11:19.613814  2334 net.cpp:465] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0429 17:11:19.613819  2334 layer_factory.hpp:77] Creating layer pool2_p
I0429 17:11:19.613831  2334 net.cpp:84] Creating Layer pool2_p
I0429 17:11:19.613836  2334 net.cpp:406] pool2_p <- conv2_p
I0429 17:11:19.613843  2334 net.cpp:380] pool2_p -> pool2_p
I0429 17:11:19.613854  2334 net.cpp:122] Setting up pool2_p
I0429 17:11:19.613860  2334 net.cpp:129] Top shape: 100 50 4 4 (80000)
I0429 17:11:19.613864  2334 net.cpp:137] Memory required for data: 16379600
I0429 17:11:19.613868  2334 layer_factory.hpp:77] Creating layer ip1_p
I0429 17:11:19.613878  2334 net.cpp:84] Creating Layer ip1_p
I0429 17:11:19.613881  2334 net.cpp:406] ip1_p <- pool2_p
I0429 17:11:19.613893  2334 net.cpp:380] ip1_p -> ip1_p
I0429 17:11:19.617738  2334 net.cpp:122] Setting up ip1_p
I0429 17:11:19.617748  2334 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:19.617753  2334 net.cpp:137] Memory required for data: 16579600
I0429 17:11:19.617758  2334 net.cpp:465] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0429 17:11:19.617763  2334 net.cpp:465] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0429 17:11:19.617768  2334 layer_factory.hpp:77] Creating layer relu1_p
I0429 17:11:19.617775  2334 net.cpp:84] Creating Layer relu1_p
I0429 17:11:19.617780  2334 net.cpp:406] relu1_p <- ip1_p
I0429 17:11:19.617785  2334 net.cpp:367] relu1_p -> ip1_p (in-place)
I0429 17:11:19.617795  2334 net.cpp:122] Setting up relu1_p
I0429 17:11:19.617800  2334 net.cpp:129] Top shape: 100 500 (50000)
I0429 17:11:19.617808  2334 net.cpp:137] Memory required for data: 16779600
I0429 17:11:19.617812  2334 layer_factory.hpp:77] Creating layer ip2_p
I0429 17:11:19.617823  2334 net.cpp:84] Creating Layer ip2_p
I0429 17:11:19.617827  2334 net.cpp:406] ip2_p <- ip1_p
I0429 17:11:19.617835  2334 net.cpp:380] ip2_p -> ip2_p
I0429 17:11:19.617897  2334 net.cpp:122] Setting up ip2_p
I0429 17:11:19.617903  2334 net.cpp:129] Top shape: 100 10 (1000)
I0429 17:11:19.617907  2334 net.cpp:137] Memory required for data: 16783600
I0429 17:11:19.617913  2334 net.cpp:465] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0429 17:11:19.617919  2334 net.cpp:465] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0429 17:11:19.617923  2334 layer_factory.hpp:77] Creating layer feat_p
I0429 17:11:19.617933  2334 net.cpp:84] Creating Layer feat_p
I0429 17:11:19.617938  2334 net.cpp:406] feat_p <- ip2_p
I0429 17:11:19.617944  2334 net.cpp:380] feat_p -> feat_p
I0429 17:11:19.617957  2334 net.cpp:122] Setting up feat_p
I0429 17:11:19.617962  2334 net.cpp:129] Top shape: 100 2 (200)
I0429 17:11:19.617966  2334 net.cpp:137] Memory required for data: 16784400
I0429 17:11:19.617970  2334 net.cpp:465] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0429 17:11:19.617976  2334 net.cpp:465] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0429 17:11:19.617980  2334 layer_factory.hpp:77] Creating layer loss
I0429 17:11:19.617988  2334 net.cpp:84] Creating Layer loss
I0429 17:11:19.617993  2334 net.cpp:406] loss <- feat
I0429 17:11:19.617998  2334 net.cpp:406] loss <- feat_p
I0429 17:11:19.618003  2334 net.cpp:406] loss <- sim
I0429 17:11:19.618010  2334 net.cpp:380] loss -> loss
I0429 17:11:19.618026  2334 net.cpp:122] Setting up loss
I0429 17:11:19.618032  2334 net.cpp:129] Top shape: (1)
I0429 17:11:19.618036  2334 net.cpp:132]     with loss weight 1
I0429 17:11:19.618046  2334 net.cpp:137] Memory required for data: 16784404
I0429 17:11:19.618049  2334 net.cpp:198] loss needs backward computation.
I0429 17:11:19.618055  2334 net.cpp:198] feat_p needs backward computation.
I0429 17:11:19.618060  2334 net.cpp:198] ip2_p needs backward computation.
I0429 17:11:19.618064  2334 net.cpp:198] relu1_p needs backward computation.
I0429 17:11:19.618068  2334 net.cpp:198] ip1_p needs backward computation.
I0429 17:11:19.618073  2334 net.cpp:198] pool2_p needs backward computation.
I0429 17:11:19.618077  2334 net.cpp:198] conv2_p needs backward computation.
I0429 17:11:19.618083  2334 net.cpp:198] pool1_p needs backward computation.
I0429 17:11:19.618086  2334 net.cpp:198] conv1_p needs backward computation.
I0429 17:11:19.618091  2334 net.cpp:198] feat needs backward computation.
I0429 17:11:19.618096  2334 net.cpp:198] ip2 needs backward computation.
I0429 17:11:19.618100  2334 net.cpp:198] relu1 needs backward computation.
I0429 17:11:19.618104  2334 net.cpp:198] ip1 needs backward computation.
I0429 17:11:19.618109  2334 net.cpp:198] pool2 needs backward computation.
I0429 17:11:19.618113  2334 net.cpp:198] conv2 needs backward computation.
I0429 17:11:19.618119  2334 net.cpp:198] pool1 needs backward computation.
I0429 17:11:19.618122  2334 net.cpp:198] conv1 needs backward computation.
I0429 17:11:19.618129  2334 net.cpp:200] slice_pair does not need backward computation.
I0429 17:11:19.618135  2334 net.cpp:200] pair_data does not need backward computation.
I0429 17:11:19.618139  2334 net.cpp:242] This network produces output loss
I0429 17:11:19.618245  2334 net.cpp:255] Network initialization done.
Traceback (most recent call last):
  File "test_model.py", line 26, in <module>
    out = net.forward_all(data=np.asarray([img.transpose(2,0,1)]))
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 202, in _Net_forward_all
    outs = self.forward(blobs=blobs, **batch)
  File "/home/data0/shikhar/caffe/caffe/python/caffe/pycaffe.py", line 123, in _Net_forward
    raise Exception('Input blob arguments do not match net inputs.')
Exception: Input blob arguments do not match net inputs.

 Performance counter stats for 'python test_model.py examples/images/cat_gray.jpg' (5 runs):

     1,856,892,760      cycles:u                                                      ( +-  3.91% )
     2,414,877,609      instructions:u            #    1.30  insns per cycle          ( +-  0.47% )

       0.741948387 seconds time elapsed                                          ( +-  1.97% )

