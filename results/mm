I0510 14:02:30.162138 131950 caffe.cpp:211] Use CPU.
I0510 14:02:30.162391 131950 solver.cpp:44] Initializing solver from parameters: 
base_lr: 0.01
display: 10
max_iter: 50
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 500
snapshot_prefix: "examples/mnist/lenet"
solver_mode: CPU
net: "examples/mnist/lenet_train_test.prototxt"
train_state {
  level: 0
  stage: ""
}
I0510 14:02:30.162487 131950 solver.cpp:87] Creating training net from net file: examples/mnist/lenet_train_test.prototxt
I0510 14:02:30.162731 131950 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer mnist
I0510 14:02:30.162750 131950 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0510 14:02:30.162869 131950 net.cpp:51] Initializing net from parameters: 
name: "LeNet"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "mnist"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/mnist/mnist_train_lmdb"
    batch_size: 64
    backend: LMDB
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "gaussian"
      std: 0.0001
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "ip2"
  bottom: "label"
  top: "loss"
}
I0510 14:02:30.162940 131950 layer_factory.hpp:77] Creating layer mnist
I0510 14:02:30.163044 131950 db_lmdb.cpp:35] Opened lmdb examples/mnist/mnist_train_lmdb
I0510 14:02:30.163076 131950 net.cpp:84] Creating Layer mnist
I0510 14:02:30.163089 131950 net.cpp:380] mnist -> data
I0510 14:02:30.163112 131950 net.cpp:380] mnist -> label
I0510 14:02:30.163149 131950 data_layer.cpp:45] output data size: 64,1,28,28
I0510 14:02:30.163692 131950 net.cpp:122] Setting up mnist
I0510 14:02:30.163707 131950 net.cpp:129] Top shape: 64 1 28 28 (50176)
I0510 14:02:30.163714 131950 net.cpp:129] Top shape: 64 (64)
I0510 14:02:30.163719 131950 net.cpp:137] Memory required for data: 200960
I0510 14:02:30.163727 131950 layer_factory.hpp:77] Creating layer conv1
I0510 14:02:30.163745 131950 net.cpp:84] Creating Layer conv1
I0510 14:02:30.163753 131950 net.cpp:406] conv1 <- data
I0510 14:02:30.163766 131950 net.cpp:380] conv1 -> conv1
I0510 14:02:30.163785 131950 base_conv_layer.cpp:121] Group is 1channel is1number of output is 20
I0510 14:02:30.163828 131950 net.cpp:122] Setting up conv1
I0510 14:02:30.163837 131950 net.cpp:129] Top shape: 64 20 24 24 (737280)
I0510 14:02:30.163843 131950 net.cpp:137] Memory required for data: 3150080
I0510 14:02:30.163871 131950 layer_factory.hpp:77] Creating layer pool1
I0510 14:02:30.163889 131950 net.cpp:84] Creating Layer pool1
I0510 14:02:30.163895 131950 net.cpp:406] pool1 <- conv1
I0510 14:02:30.163903 131950 net.cpp:380] pool1 -> pool1
I0510 14:02:30.163920 131950 net.cpp:122] Setting up pool1
I0510 14:02:30.163931 131950 net.cpp:129] Top shape: 64 20 12 12 (184320)
I0510 14:02:30.163936 131950 net.cpp:137] Memory required for data: 3887360
I0510 14:02:30.163941 131950 layer_factory.hpp:77] Creating layer conv2
I0510 14:02:30.163952 131950 net.cpp:84] Creating Layer conv2
I0510 14:02:30.163959 131950 net.cpp:406] conv2 <- pool1
I0510 14:02:30.163965 131950 net.cpp:380] conv2 -> conv2
I0510 14:02:30.163980 131950 base_conv_layer.cpp:121] Group is 1channel is20number of output is 50
I0510 14:02:30.164460 131950 net.cpp:122] Setting up conv2
I0510 14:02:30.164474 131950 net.cpp:129] Top shape: 64 50 8 8 (204800)
I0510 14:02:30.164479 131950 net.cpp:137] Memory required for data: 4706560
I0510 14:02:30.164490 131950 layer_factory.hpp:77] Creating layer pool2
I0510 14:02:30.164499 131950 net.cpp:84] Creating Layer pool2
I0510 14:02:30.164505 131950 net.cpp:406] pool2 <- conv2
I0510 14:02:30.164511 131950 net.cpp:380] pool2 -> pool2
I0510 14:02:30.164521 131950 net.cpp:122] Setting up pool2
I0510 14:02:30.164528 131950 net.cpp:129] Top shape: 64 50 4 4 (51200)
I0510 14:02:30.164535 131950 net.cpp:137] Memory required for data: 4911360
I0510 14:02:30.164539 131950 layer_factory.hpp:77] Creating layer ip1
I0510 14:02:30.164548 131950 net.cpp:84] Creating Layer ip1
I0510 14:02:30.164553 131950 net.cpp:406] ip1 <- pool2
I0510 14:02:30.164562 131950 net.cpp:380] ip1 -> ip1
I0510 14:02:30.171203 131950 net.cpp:122] Setting up ip1
I0510 14:02:30.171213 131950 net.cpp:129] Top shape: 64 500 (32000)
I0510 14:02:30.171218 131950 net.cpp:137] Memory required for data: 5039360
I0510 14:02:30.171231 131950 layer_factory.hpp:77] Creating layer relu1
I0510 14:02:30.171237 131950 net.cpp:84] Creating Layer relu1
I0510 14:02:30.171242 131950 net.cpp:406] relu1 <- ip1
I0510 14:02:30.171249 131950 net.cpp:367] relu1 -> ip1 (in-place)
I0510 14:02:30.171259 131950 net.cpp:122] Setting up relu1
I0510 14:02:30.171265 131950 net.cpp:129] Top shape: 64 500 (32000)
I0510 14:02:30.171269 131950 net.cpp:137] Memory required for data: 5167360
I0510 14:02:30.171274 131950 layer_factory.hpp:77] Creating layer ip2
I0510 14:02:30.171283 131950 net.cpp:84] Creating Layer ip2
I0510 14:02:30.171286 131950 net.cpp:406] ip2 <- ip1
I0510 14:02:30.171294 131950 net.cpp:380] ip2 -> ip2
I0510 14:02:30.171396 131950 net.cpp:122] Setting up ip2
I0510 14:02:30.171404 131950 net.cpp:129] Top shape: 64 10 (640)
I0510 14:02:30.171409 131950 net.cpp:137] Memory required for data: 5169920
I0510 14:02:30.171417 131950 layer_factory.hpp:77] Creating layer loss
I0510 14:02:30.171425 131950 net.cpp:84] Creating Layer loss
I0510 14:02:30.171432 131950 net.cpp:406] loss <- ip2
I0510 14:02:30.171437 131950 net.cpp:406] loss <- label
I0510 14:02:30.171444 131950 net.cpp:380] loss -> loss
I0510 14:02:30.171458 131950 layer_factory.hpp:77] Creating layer loss
I0510 14:02:30.171475 131950 net.cpp:122] Setting up loss
I0510 14:02:30.171483 131950 net.cpp:129] Top shape: (1)
I0510 14:02:30.171486 131950 net.cpp:132]     with loss weight 1
I0510 14:02:30.171501 131950 net.cpp:137] Memory required for data: 5169924
I0510 14:02:30.171507 131950 net.cpp:198] loss needs backward computation.
I0510 14:02:30.171515 131950 net.cpp:198] ip2 needs backward computation.
I0510 14:02:30.171520 131950 net.cpp:198] relu1 needs backward computation.
I0510 14:02:30.171525 131950 net.cpp:198] ip1 needs backward computation.
I0510 14:02:30.171530 131950 net.cpp:198] pool2 needs backward computation.
I0510 14:02:30.171535 131950 net.cpp:198] conv2 needs backward computation.
I0510 14:02:30.171540 131950 net.cpp:198] pool1 needs backward computation.
I0510 14:02:30.171545 131950 net.cpp:198] conv1 needs backward computation.
I0510 14:02:30.171552 131950 net.cpp:200] mnist does not need backward computation.
I0510 14:02:30.171562 131950 net.cpp:242] This network produces output loss
I0510 14:02:30.171583 131950 net.cpp:255] Network initialization done.
I0510 14:02:30.171628 131950 solver.cpp:56] Solver scaffolding done.
I0510 14:02:30.171661 131950 caffe.cpp:248] Starting Optimization
I0510 14:02:30.171666 131950 solver.cpp:272] Solving LeNet
I0510 14:02:30.171670 131950 solver.cpp:273] Learning Rate Policy: inv
I0510 14:02:30.278053 131950 solver.cpp:218] Iteration 0 (-9.24747e-38 iter/s, 0.106s/10 iters), loss = 2.30258
I0510 14:02:30.278072 131950 solver.cpp:237]     Train net output #0: loss = 2.30258 (* 1 = 2.30258 loss)
I0510 14:02:30.278080 131950 sgd_solver.cpp:105] Iteration 0, lr = 0.01
I0510 14:02:31.075836 131950 solver.cpp:218] Iteration 10 (12.5471 iter/s, 0.797s/10 iters), loss = 2.3017
I0510 14:02:31.075857 131950 solver.cpp:237]     Train net output #0: loss = 2.3017 (* 1 = 2.3017 loss)
I0510 14:02:31.075862 131950 sgd_solver.cpp:105] Iteration 10, lr = 0.00999251
I0510 14:02:31.872498 131950 solver.cpp:218] Iteration 20 (12.5628 iter/s, 0.796s/10 iters), loss = 2.30386
I0510 14:02:31.872514 131950 solver.cpp:237]     Train net output #0: loss = 2.30386 (* 1 = 2.30386 loss)
I0510 14:02:31.872519 131950 sgd_solver.cpp:105] Iteration 20, lr = 0.00998503
I0510 14:02:32.669279 131950 solver.cpp:218] Iteration 30 (12.5628 iter/s, 0.796s/10 iters), loss = 2.30115
I0510 14:02:32.669298 131950 solver.cpp:237]     Train net output #0: loss = 2.30115 (* 1 = 2.30115 loss)
I0510 14:02:32.669302 131950 sgd_solver.cpp:105] Iteration 30, lr = 0.00997756
I0510 14:02:33.465430 131950 solver.cpp:218] Iteration 40 (12.5628 iter/s, 0.796s/10 iters), loss = 2.30064
I0510 14:02:33.465448 131950 solver.cpp:237]     Train net output #0: loss = 2.30064 (* 1 = 2.30064 loss)
I0510 14:02:33.465452 131950 sgd_solver.cpp:105] Iteration 40, lr = 0.0099701
I0510 14:02:34.184563 131950 solver.cpp:447] Snapshotting to binary proto file examples/mnist/lenet_iter_50.caffemodel
I0510 14:02:34.190791 131950 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/mnist/lenet_iter_50.solverstate
I0510 14:02:34.226605 131950 solver.cpp:310] Iteration 50, loss = 2.29446
I0510 14:02:34.226622 131950 solver.cpp:315] Optimization Done.
I0510 14:02:34.226626 131950 caffe.cpp:259] Optimization Done.
