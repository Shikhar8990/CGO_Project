#           time             counts unit events
     0.100122520            103,906      r02c0:u                  
     0.200318968            119,827      r02c0:u                  
     0.300447135             11,760      r02c0:u                  
     0.400574055             31,009      r02c0:u                  
     0.500714407             10,217      r02c0:u                  
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0510 10:10:26.700459 129525 _caffe.cpp:139] DEPRECATION WARNING - deprecated use of Python interface
W0510 10:10:26.700484 129525 _caffe.cpp:140] Use this instead (with the named "weights" parameter):
W0510 10:10:26.700485 129525 _caffe.cpp:142] Net('examples/cifar10/cifar10_quick.prototxt', 1, weights='examples/cifar10/cifar10_quick_iter_5000.caffemodel.h5')
I0510 10:10:26.701794 129525 net.cpp:51] Initializing net from parameters: 
name: "CIFAR10_quick_test"
state {
  phase: TEST
  level: 0
}
layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 32
      dim: 32
    }
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "pool1"
  top: "pool1"
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 32
    pad: 2
    kernel_size: 5
    stride: 1
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "pool2"
  top: "conv3"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  convolution_param {
    num_output: 64
    pad: 2
    kernel_size: 5
    stride: 1
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool3"
  top: "ip1"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 64
  }
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    lr_mult: 1
  }
  param {
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
  }
}
layer {
  name: "prob"
  type: "Softmax"
  bottom: "ip2"
  top: "prob"
}
I0510 10:10:26.701838 129525 layer_factory.hpp:77] Creating layer data
I0510 10:10:26.701850 129525 net.cpp:84] Creating Layer data
I0510 10:10:26.701858 129525 net.cpp:380] data -> data
I0510 10:10:26.701871 129525 net.cpp:122] Setting up data
I0510 10:10:26.701877 129525 net.cpp:129] Top shape: 1 3 32 32 (3072)
I0510 10:10:26.701880 129525 net.cpp:137] Memory required for data: 12288
I0510 10:10:26.701884 129525 layer_factory.hpp:77] Creating layer conv1
I0510 10:10:26.701890 129525 net.cpp:84] Creating Layer conv1
I0510 10:10:26.701894 129525 net.cpp:406] conv1 <- data
I0510 10:10:26.701898 129525 net.cpp:380] conv1 -> conv1
I0510 10:10:26.701908 129525 base_conv_layer.cpp:121] Group is 1channel is3number of output is 32
I0510 10:10:26.701925 129525 net.cpp:122] Setting up conv1
I0510 10:10:26.701930 129525 net.cpp:129] Top shape: 1 32 32 32 (32768)
I0510 10:10:26.701932 129525 net.cpp:137] Memory required for data: 143360
I0510 10:10:26.701941 129525 layer_factory.hpp:77] Creating layer pool1
I0510 10:10:26.701946 129525 net.cpp:84] Creating Layer pool1
I0510 10:10:26.701947 129525 net.cpp:406] pool1 <- conv1
I0510 10:10:26.701952 129525 net.cpp:380] pool1 -> pool1
I0510 10:10:26.701959 129525 net.cpp:122] Setting up pool1
I0510 10:10:26.701970 129525 net.cpp:129] Top shape: 1 32 16 16 (8192)
I0510 10:10:26.701972 129525 net.cpp:137] Memory required for data: 176128
I0510 10:10:26.701975 129525 layer_factory.hpp:77] Creating layer relu1
I0510 10:10:26.701979 129525 net.cpp:84] Creating Layer relu1
I0510 10:10:26.701982 129525 net.cpp:406] relu1 <- pool1
I0510 10:10:26.701985 129525 net.cpp:367] relu1 -> pool1 (in-place)
I0510 10:10:26.701990 129525 net.cpp:122] Setting up relu1
I0510 10:10:26.701993 129525 net.cpp:129] Top shape: 1 32 16 16 (8192)
I0510 10:10:26.701997 129525 net.cpp:137] Memory required for data: 208896
I0510 10:10:26.701998 129525 layer_factory.hpp:77] Creating layer conv2
I0510 10:10:26.702003 129525 net.cpp:84] Creating Layer conv2
I0510 10:10:26.702006 129525 net.cpp:406] conv2 <- pool1
I0510 10:10:26.702010 129525 net.cpp:380] conv2 -> conv2
I0510 10:10:26.702016 129525 base_conv_layer.cpp:121] Group is 1channel is32number of output is 32
I0510 10:10:26.702070 129525 net.cpp:122] Setting up conv2
I0510 10:10:26.702075 129525 net.cpp:129] Top shape: 1 32 16 16 (8192)
I0510 10:10:26.702077 129525 net.cpp:137] Memory required for data: 241664
I0510 10:10:26.702083 129525 layer_factory.hpp:77] Creating layer relu2
I0510 10:10:26.702087 129525 net.cpp:84] Creating Layer relu2
I0510 10:10:26.702090 129525 net.cpp:406] relu2 <- conv2
I0510 10:10:26.702093 129525 net.cpp:367] relu2 -> conv2 (in-place)
I0510 10:10:26.702097 129525 net.cpp:122] Setting up relu2
I0510 10:10:26.702101 129525 net.cpp:129] Top shape: 1 32 16 16 (8192)
I0510 10:10:26.702103 129525 net.cpp:137] Memory required for data: 274432
I0510 10:10:26.702105 129525 layer_factory.hpp:77] Creating layer pool2
I0510 10:10:26.702109 129525 net.cpp:84] Creating Layer pool2
I0510 10:10:26.702111 129525 net.cpp:406] pool2 <- conv2
I0510 10:10:26.702116 129525 net.cpp:380] pool2 -> pool2
I0510 10:10:26.702121 129525 net.cpp:122] Setting up pool2
I0510 10:10:26.702124 129525 net.cpp:129] Top shape: 1 32 8 8 (2048)
I0510 10:10:26.702127 129525 net.cpp:137] Memory required for data: 282624
I0510 10:10:26.702129 129525 layer_factory.hpp:77] Creating layer conv3
I0510 10:10:26.702134 129525 net.cpp:84] Creating Layer conv3
I0510 10:10:26.702136 129525 net.cpp:406] conv3 <- pool2
I0510 10:10:26.702142 129525 net.cpp:380] conv3 -> conv3
I0510 10:10:26.702147 129525 base_conv_layer.cpp:121] Group is 1channel is32number of output is 64
I0510 10:10:26.702232 129525 net.cpp:122] Setting up conv3
I0510 10:10:26.702239 129525 net.cpp:129] Top shape: 1 64 8 8 (4096)
I0510 10:10:26.702240 129525 net.cpp:137] Memory required for data: 299008
I0510 10:10:26.702246 129525 layer_factory.hpp:77] Creating layer relu3
I0510 10:10:26.702250 129525 net.cpp:84] Creating Layer relu3
I0510 10:10:26.702253 129525 net.cpp:406] relu3 <- conv3
I0510 10:10:26.702256 129525 net.cpp:367] relu3 -> conv3 (in-place)
I0510 10:10:26.702260 129525 net.cpp:122] Setting up relu3
I0510 10:10:26.702265 129525 net.cpp:129] Top shape: 1 64 8 8 (4096)
I0510 10:10:26.702266 129525 net.cpp:137] Memory required for data: 315392
I0510 10:10:26.702268 129525 layer_factory.hpp:77] Creating layer pool3
I0510 10:10:26.702272 129525 net.cpp:84] Creating Layer pool3
I0510 10:10:26.702275 129525 net.cpp:406] pool3 <- conv3
I0510 10:10:26.702277 129525 net.cpp:380] pool3 -> pool3
I0510 10:10:26.702282 129525 net.cpp:122] Setting up pool3
I0510 10:10:26.702286 129525 net.cpp:129] Top shape: 1 64 4 4 (1024)
I0510 10:10:26.702288 129525 net.cpp:137] Memory required for data: 319488
I0510 10:10:26.702291 129525 layer_factory.hpp:77] Creating layer ip1
I0510 10:10:26.702297 129525 net.cpp:84] Creating Layer ip1
I0510 10:10:26.702299 129525 net.cpp:406] ip1 <- pool3
I0510 10:10:26.702303 129525 net.cpp:380] ip1 -> ip1
I0510 10:10:26.702409 129525 net.cpp:122] Setting up ip1
I0510 10:10:26.702415 129525 net.cpp:129] Top shape: 1 64 (64)
I0510 10:10:26.702417 129525 net.cpp:137] Memory required for data: 319744
I0510 10:10:26.702422 129525 layer_factory.hpp:77] Creating layer ip2
I0510 10:10:26.702425 129525 net.cpp:84] Creating Layer ip2
I0510 10:10:26.702432 129525 net.cpp:406] ip2 <- ip1
I0510 10:10:26.702436 129525 net.cpp:380] ip2 -> ip2
I0510 10:10:26.702446 129525 net.cpp:122] Setting up ip2
I0510 10:10:26.702450 129525 net.cpp:129] Top shape: 1 10 (10)
I0510 10:10:26.702452 129525 net.cpp:137] Memory required for data: 319784
I0510 10:10:26.702457 129525 layer_factory.hpp:77] Creating layer prob
I0510 10:10:26.702461 129525 net.cpp:84] Creating Layer prob
I0510 10:10:26.702464 129525 net.cpp:406] prob <- ip2
I0510 10:10:26.702467 129525 net.cpp:380] prob -> prob
I0510 10:10:26.702473 129525 net.cpp:122] Setting up prob
I0510 10:10:26.702477 129525 net.cpp:129] Top shape: 1 10 (10)
I0510 10:10:26.702479 129525 net.cpp:137] Memory required for data: 319824
I0510 10:10:26.702482 129525 net.cpp:200] prob does not need backward computation.
I0510 10:10:26.702486 129525 net.cpp:200] ip2 does not need backward computation.
I0510 10:10:26.702487 129525 net.cpp:200] ip1 does not need backward computation.
I0510 10:10:26.702491 129525 net.cpp:200] pool3 does not need backward computation.
I0510 10:10:26.702492 129525 net.cpp:200] relu3 does not need backward computation.
I0510 10:10:26.702495 129525 net.cpp:200] conv3 does not need backward computation.
I0510 10:10:26.702497 129525 net.cpp:200] pool2 does not need backward computation.
I0510 10:10:26.702500 129525 net.cpp:200] relu2 does not need backward computation.
I0510 10:10:26.702503 129525 net.cpp:200] conv2 does not need backward computation.
I0510 10:10:26.702505 129525 net.cpp:200] relu1 does not need backward computation.
I0510 10:10:26.702507 129525 net.cpp:200] pool1 does not need backward computation.
I0510 10:10:26.702510 129525 net.cpp:200] conv1 does not need backward computation.
I0510 10:10:26.702513 129525 net.cpp:200] data does not need backward computation.
I0510 10:10:26.702515 129525 net.cpp:242] This network produces output prob
I0510 10:10:26.702522 129525 net.cpp:255] Network initialization done.
I0510 10:10:26.703279 129525 net.cpp:798] Ignoring source layer cifar
I0510 10:10:26.703351 129525 hdf5.cpp:32] Datatype class: H5T_FLOAT
F0510 10:10:26.703361 129525 hdf5.cpp:79] Check failed: blob_dims == blob->shape() Cannot load blob from hdf5; shape mismatch. Source shape is 32 1 5 5 (800) target shape is 32 3 5 5 (2400)
*** Check failure stack trace: ***
     0.600842084             51,579      r02c0:u                  
     0.696799770                  0      r02c0:u                  
python: Aborted
